{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML4FG_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evelimes/HackerRank/blob/master/ML4FG_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAoozId5UPNd"
      },
      "source": [
        "1. Train the classifier for each disease independently\n",
        "2. We store the weights fo the embedding layer\n",
        "3. Create a new classifier - does not accept custtom embedding layer\n",
        "\n",
        "4. Pass input through cusotm embedding -  depending on dimensions\n",
        "5. Get uniform size embedding for all data\n",
        "6. Use the embeddings as input to train a multi-class classification model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsrc_Dg3MFbM"
      },
      "source": [
        "## Link to data dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJWfL9L1MEP9"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAkTe7yf8w7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25186e14-bc6e-4efd-d4b6-207e2bb165bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "datadir=\"/content/drive/MyDrive/ML4fungen final project/\"\n",
        "!ls \"$datadir\" | tail"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "chd-seq.csv\n",
            "gene-risk-labels.csv\n",
            "ML4FG Final Report.gdoc\n",
            "ML4FG_project.ipynb\n",
            "ML4fungen final report TO DO List.gdoc\n",
            "rna-seq_counts_SCH.csv\n",
            "sch_auc.png\n",
            "sch_gene_preds.csv\n",
            "SCHgenes-risk.csv\n",
            "sch-seq.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o--3CCqRcuhW"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yyqiqgzgETx"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy import interp \n",
        "from itertools import cycle\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt2y55mUG6DG"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJNfeLccwmY"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1BEDi6JI9mh"
      },
      "source": [
        "class CustomEmbedding(nn.Module):\n",
        "    def __init__(self, inputs, hidden):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(inputs, hidden)\n",
        "        self.l2 = nn.Linear(hidden, hidden)\n",
        "        self.bn = nn.BatchNorm1d(hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout  = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden, outputs):\n",
        "        super().__init__()\n",
        "        self.emb =  embedding_layer\n",
        "        self.bn1 = nn.BatchNorm1d(hidden)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden)\n",
        "        self.l1 = nn.Linear(hidden, hidden)\n",
        "        self.l2 = nn.Linear(hidden, outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout  = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        # x = self.dropout(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.bn1(x)\n",
        "        # x = self.l1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        # x = torch.tanh(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.l2(x)\n",
        "        return x"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54yNw7u8c1q8"
      },
      "source": [
        "# train funciton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLwVqaDMlga"
      },
      "source": [
        "def train_one_epoch(model, inputs, labels, loss_func, optimizer, device, acc_func=None):\n",
        "   \n",
        "    optimizer.zero_grad()\n",
        "    # inputs = inputs.cuda()\n",
        "    # labels = labels.cuda()\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    loss = loss_func(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if acc_func:\n",
        "        acc = acc_func(outputs,labels)\n",
        "        accuracy =  acc.data.cpu()\n",
        "    \n",
        "\n",
        "    epoch_loss = loss.data.cpu()\n",
        "\n",
        "    if acc_func:\n",
        "        return epoch_loss, accuracy\n",
        "    \n",
        "    return epoch_loss"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAvGWFChc4oU"
      },
      "source": [
        "# eval function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klrH6CGvx3g2"
      },
      "source": [
        "def calc_accuracy(outputs, labels):\n",
        "    return torch.mean((torch.argmax(outputs, dim=1) == labels).float())"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OojdtoTaKbD"
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, inputs, labels, loss_func, device, acc_func=None, pr=True):\n",
        "    model.eval()\n",
        "    tp=0\n",
        "    fp=0\n",
        "    tot_p=0\n",
        "    # inputs = inputs.cuda()\n",
        "    # labels = labels.cuda()\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    loss = loss_func(outputs, labels)\n",
        "    epoch_loss = loss.data.cpu()\n",
        "    if acc_func:\n",
        "        accuracy = acc_func(outputs,labels)\n",
        "    \n",
        "    if pr:\n",
        "        tp+= torch.sum(torch.logical_and(torch.argmax(outputs, dim=1), labels))\n",
        "        fp+= torch.sum(torch.logical_and(torch.argmax(outputs, dim=1), torch.logical_not(labels)))\n",
        "        tot_p+=torch.sum(labels)\n",
        "\n",
        "    if pr:\n",
        "        recall = tp/tot_p\n",
        "        precision = tp/(tp+fp)\n",
        "        f1 = 2*precision*recall/(precision+recall)\n",
        "        # print(\"Recall = \", recall, tp, fp, tot_p)\n",
        "        # print(\"Precision = \", precision)\n",
        "        # print(\"F1 score = \", f1)\n",
        "\n",
        "    if acc_func:\n",
        "        return epoch_loss, accuracy, precision, recall, f1\n",
        "    \n",
        "    return epoch_loss"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr3iAeKvd0SO"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZLpvheJedrY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "44334673-2349-48cb-c047-cf51e0bb1b35"
      },
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "path_to_assigment_dir = Path(\"/content/drive/My Drive/ML4fungen final project\")\n",
        "autdata = pd.read_csv(path_to_assigment_dir /\"aut-seq.csv\", index_col=0)\n",
        "autdata = autdata.transpose()\n",
        "\n",
        "chddata = pd.read_csv(path_to_assigment_dir /\"chd-seq.csv\", index_col=0)\n",
        "chddata = chddata.transpose()\n",
        "\n",
        "schdata = pd.read_csv(path_to_assigment_dir /\"sch-seq.csv\", index_col=0)\n",
        "schdata = schdata.transpose()\n",
        "schdata.iloc[:6,:8]"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Gene</th>\n",
              "      <th>A1BG</th>\n",
              "      <th>A1CF</th>\n",
              "      <th>A2M</th>\n",
              "      <th>A2ML1</th>\n",
              "      <th>A3GALT2</th>\n",
              "      <th>A4GALT</th>\n",
              "      <th>A4GNT</th>\n",
              "      <th>AAAS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0815</th>\n",
              "      <td>-0.305129</td>\n",
              "      <td>-0.322929</td>\n",
              "      <td>1.156678</td>\n",
              "      <td>-0.262462</td>\n",
              "      <td>-0.319919</td>\n",
              "      <td>-0.312851</td>\n",
              "      <td>-0.322929</td>\n",
              "      <td>-0.239034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0817</th>\n",
              "      <td>-0.288902</td>\n",
              "      <td>-0.301473</td>\n",
              "      <td>0.241984</td>\n",
              "      <td>-0.277341</td>\n",
              "      <td>-0.300013</td>\n",
              "      <td>-0.294851</td>\n",
              "      <td>-0.301922</td>\n",
              "      <td>-0.251190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0819</th>\n",
              "      <td>-0.318663</td>\n",
              "      <td>-0.342822</td>\n",
              "      <td>0.865400</td>\n",
              "      <td>-0.294746</td>\n",
              "      <td>-0.339908</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.342822</td>\n",
              "      <td>-0.252375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0823</th>\n",
              "      <td>-0.305430</td>\n",
              "      <td>-0.324059</td>\n",
              "      <td>0.670341</td>\n",
              "      <td>-0.277794</td>\n",
              "      <td>-0.323138</td>\n",
              "      <td>-0.319351</td>\n",
              "      <td>-0.324059</td>\n",
              "      <td>-0.263361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0827</th>\n",
              "      <td>-0.317849</td>\n",
              "      <td>-0.337633</td>\n",
              "      <td>0.533127</td>\n",
              "      <td>-0.303401</td>\n",
              "      <td>-0.336461</td>\n",
              "      <td>-0.331906</td>\n",
              "      <td>-0.337372</td>\n",
              "      <td>-0.258236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CA1_CTL_0828</th>\n",
              "      <td>-0.309713</td>\n",
              "      <td>-0.340803</td>\n",
              "      <td>0.709763</td>\n",
              "      <td>-0.289034</td>\n",
              "      <td>-0.339536</td>\n",
              "      <td>-0.330533</td>\n",
              "      <td>-0.341084</td>\n",
              "      <td>-0.246831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Gene              A1BG      A1CF       A2M  ...    A4GALT     A4GNT      AAAS\n",
              "CA1_CTL_0815 -0.305129 -0.322929  1.156678  ... -0.312851 -0.322929 -0.239034\n",
              "CA1_CTL_0817 -0.288902 -0.301473  0.241984  ... -0.294851 -0.301922 -0.251190\n",
              "CA1_CTL_0819 -0.318663 -0.342822  0.865400  ... -0.332867 -0.342822 -0.252375\n",
              "CA1_CTL_0823 -0.305430 -0.324059  0.670341  ... -0.319351 -0.324059 -0.263361\n",
              "CA1_CTL_0827 -0.317849 -0.337633  0.533127  ... -0.331906 -0.337372 -0.258236\n",
              "CA1_CTL_0828 -0.309713 -0.340803  0.709763  ... -0.330533 -0.341084 -0.246831\n",
              "\n",
              "[6 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SazD5LsNhiEt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ed117add-c965-468b-92b6-6053790fb8c7"
      },
      "source": [
        "risk = pd.read_csv(path_to_assigment_dir / \"gene-risk-labels.csv\",  index_col = 0)\n",
        "risk.iloc[:6,:8]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aut</th>\n",
              "      <th>chd</th>\n",
              "      <th>sch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AASDH</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABCA1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABCA2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABCB11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABCC9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ABCD3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        aut  chd  sch\n",
              "AASDH     0    0    1\n",
              "ABCA1     0    0    1\n",
              "ABCA2     0    0    1\n",
              "ABCB11    0    0    1\n",
              "ABCC9     1    0    0\n",
              "ABCD3     1    0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAmDnt9UnGVR"
      },
      "source": [
        "*Generate common genes for every disease*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFsJ9y7hkHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aae5d67-0144-4bf0-a3b4-06bc297819f3"
      },
      "source": [
        "#aut filter\n",
        "common_genes = [g for g in risk.index]\n",
        "common_genes = list(set(common_genes).intersection(autdata.columns.tolist() ))\n",
        "common_genes = sorted(common_genes) # required for reproducibility, set order is undefined\n",
        "aut_exp = autdata[common_genes]\n",
        "aut_exp.shape"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 787)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJbW2mMjxMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e415a373-1c46-4c41-e628-a75e039afcd3"
      },
      "source": [
        "aut_risk = risk.loc[common_genes, :]\n",
        "aut_risk = aut_risk[\"aut\"]\n",
        "aut_risk.head()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AASDH    0\n",
              "ABCA1    0\n",
              "ABCA2    0\n",
              "ABCC9    1\n",
              "ABCD3    1\n",
              "Name: aut, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz2R7uwRjhw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ec4d4d-9349-47d0-8cde-02f55f55abe6"
      },
      "source": [
        "#chd filter \n",
        "common_genes2 = [g for g in risk.index]\n",
        "common_genes2 = list(set(common_genes2).intersection(chddata.columns.tolist() ))\n",
        "common_genes2 = sorted(common_genes2) # required for reproducibility, set order is undefined\n",
        "chd_exp = chddata[common_genes2]\n",
        "chd_exp.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSB2qpn2jzAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837d392c-9d80-476d-da01-7786b216c23c"
      },
      "source": [
        "chd_risk = risk.loc[common_genes2,:]\n",
        "chd_risk = chd_risk[\"chd\"]\n",
        "chd_risk.shape"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpUTqV66jjei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0502737c-de07-48fb-d7a6-8ee62bf41989"
      },
      "source": [
        "#sch filter\n",
        "common_genes3 = [g for g in risk.index]\n",
        "common_genes3 = list(set(common_genes3).intersection(schdata.columns.tolist()))\n",
        "common_genes3 = sorted(common_genes3) # required for reproducibility, set order is undefined\n",
        "sch_exp = schdata[common_genes3]\n",
        "sch_exp.shape"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78, 882)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yth-hfcVhmcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec5bf02-6310-4442-f009-96ffab8748b2"
      },
      "source": [
        "sch_risk = risk.loc[common_genes3,:]\n",
        "sch_risk = sch_risk[\"sch\"]\n",
        "sch_risk.shape"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(882,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reX36sw5c7Ac"
      },
      "source": [
        "# Autism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DibuDc4NoUNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "d0b0ea1a-b069-4377-de5a-ec5c8c8dea07"
      },
      "source": [
        "aut_exp_n = np.array(aut_exp).T\n",
        "aut_risk_n = np.array(aut_risk)\n",
        "np.mean(aut_risk_n)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AASDH</th>\n",
              "      <th>ABCA1</th>\n",
              "      <th>ABCA2</th>\n",
              "      <th>ABCC9</th>\n",
              "      <th>ABCD3</th>\n",
              "      <th>ABR</th>\n",
              "      <th>ACAN</th>\n",
              "      <th>ACTB</th>\n",
              "      <th>ACTR3</th>\n",
              "      <th>ACVR2B</th>\n",
              "      <th>ADAM32</th>\n",
              "      <th>ADAMTS10</th>\n",
              "      <th>ADAMTS18</th>\n",
              "      <th>ADAMTS20</th>\n",
              "      <th>ADAMTS6</th>\n",
              "      <th>ADCY1</th>\n",
              "      <th>ADD1</th>\n",
              "      <th>ADD2</th>\n",
              "      <th>ADNP</th>\n",
              "      <th>ADRA1A</th>\n",
              "      <th>ADSL</th>\n",
              "      <th>AGAP3</th>\n",
              "      <th>AIG1</th>\n",
              "      <th>AIRE</th>\n",
              "      <th>AK4</th>\n",
              "      <th>AKAP13</th>\n",
              "      <th>ALG9</th>\n",
              "      <th>ALS2</th>\n",
              "      <th>AMPD2</th>\n",
              "      <th>AMPH</th>\n",
              "      <th>ANK2</th>\n",
              "      <th>ANKDD1A</th>\n",
              "      <th>ANKRD11</th>\n",
              "      <th>ANKRD50</th>\n",
              "      <th>ANKS1B</th>\n",
              "      <th>ANKS6</th>\n",
              "      <th>AP1B1</th>\n",
              "      <th>AP2B1</th>\n",
              "      <th>AP5Z1</th>\n",
              "      <th>ARHGAP25</th>\n",
              "      <th>...</th>\n",
              "      <th>WDR19</th>\n",
              "      <th>WDR34</th>\n",
              "      <th>WDR35</th>\n",
              "      <th>WDR48</th>\n",
              "      <th>WDR60</th>\n",
              "      <th>WDR66</th>\n",
              "      <th>WIPF3</th>\n",
              "      <th>WNK3</th>\n",
              "      <th>WWC2</th>\n",
              "      <th>WWOX</th>\n",
              "      <th>XKR6</th>\n",
              "      <th>XPC</th>\n",
              "      <th>XRRA1</th>\n",
              "      <th>ZACN</th>\n",
              "      <th>ZBTB14</th>\n",
              "      <th>ZBTB39</th>\n",
              "      <th>ZC3H13</th>\n",
              "      <th>ZEB2</th>\n",
              "      <th>ZFHX2</th>\n",
              "      <th>ZFP64</th>\n",
              "      <th>ZFPM2</th>\n",
              "      <th>ZIC3</th>\n",
              "      <th>ZNF165</th>\n",
              "      <th>ZNF197</th>\n",
              "      <th>ZNF213</th>\n",
              "      <th>ZNF236</th>\n",
              "      <th>ZNF407</th>\n",
              "      <th>ZNF423</th>\n",
              "      <th>ZNF516</th>\n",
              "      <th>ZNF528</th>\n",
              "      <th>ZNF623</th>\n",
              "      <th>ZNF644</th>\n",
              "      <th>ZNF721</th>\n",
              "      <th>ZNF749</th>\n",
              "      <th>ZNF804A</th>\n",
              "      <th>ZNF831</th>\n",
              "      <th>ZNFX1</th>\n",
              "      <th>ZSCAN2</th>\n",
              "      <th>ZSWIM7</th>\n",
              "      <th>ZW10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hRgl2a_7</th>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>4.715301</td>\n",
              "      <td>1.877835</td>\n",
              "      <td>2.508383</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>2.823657</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>1.562561</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>2.193109</td>\n",
              "      <td>0.616739</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>3.454205</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>2.193109</td>\n",
              "      <td>1.877835</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>0.616739</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>1.247287</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>2.193109</td>\n",
              "      <td>1.247287</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>0.616739</td>\n",
              "      <td>0.301465</td>\n",
              "      <td>-0.329083</td>\n",
              "      <td>1.247287</td>\n",
              "      <td>0.932013</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.013809</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.644357</td>\n",
              "      <td>-0.013809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hProgBP_7</th>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>4.819379</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>2.892496</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>1.736367</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>1.736367</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>2.121743</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>2.507120</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>...</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>2.121743</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>1.350990</td>\n",
              "      <td>1.350990</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>0.580237</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>1.736367</td>\n",
              "      <td>0.965614</td>\n",
              "      <td>0.194861</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.575892</td>\n",
              "      <td>-0.190516</td>\n",
              "      <td>0.580237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hOMTN_7</th>\n",
              "      <td>0.991196</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>1.317239</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>4.903711</td>\n",
              "      <td>0.991196</td>\n",
              "      <td>1.969325</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>1.969325</td>\n",
              "      <td>0.991196</td>\n",
              "      <td>1.317239</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>1.969325</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>0.013068</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>2.947453</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>1.969325</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>3.273496</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>1.317239</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>1.643282</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.665153</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>3.273496</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>0.013067</td>\n",
              "      <td>0.013068</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>1.317239</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>0.013067</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>1.643282</td>\n",
              "      <td>0.339110</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>1.643282</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.639018</td>\n",
              "      <td>-0.312976</td>\n",
              "      <td>1.643282</td>\n",
              "      <td>-0.639018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hRgl2a_9</th>\n",
              "      <td>2.292747</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>1.868302</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>4.839421</td>\n",
              "      <td>1.443857</td>\n",
              "      <td>2.292747</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>1.868302</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>1.443857</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>1.443857</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>2.717193</td>\n",
              "      <td>1.443857</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>3.141639</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>...</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>1.868302</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>1.019411</td>\n",
              "      <td>1.443857</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>3.141639</td>\n",
              "      <td>0.594965</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.678371</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>0.170520</td>\n",
              "      <td>-0.253926</td>\n",
              "      <td>-0.253926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hNProg_9</th>\n",
              "      <td>1.255239</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>1.734068</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>5.085872</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>2.691726</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>2.212897</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>4.128213</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>2.212897</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>3.649385</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>1.734068</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>2.212897</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>...</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>2.212897</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>3.170556</td>\n",
              "      <td>2.212897</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>1.255239</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>2.691726</td>\n",
              "      <td>0.297581</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>0.776410</td>\n",
              "      <td>-0.660077</td>\n",
              "      <td>-0.181248</td>\n",
              "      <td>-0.181248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NPCs_6_GW16</th>\n",
              "      <td>0.597357</td>\n",
              "      <td>1.154650</td>\n",
              "      <td>0.225828</td>\n",
              "      <td>2.455000</td>\n",
              "      <td>1.526179</td>\n",
              "      <td>1.154650</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>2.826529</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>1.711943</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>1.711943</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>0.783121</td>\n",
              "      <td>0.597357</td>\n",
              "      <td>1.154650</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>0.783121</td>\n",
              "      <td>0.783121</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>0.597357</td>\n",
              "      <td>0.783121</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>1.526179</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>1.154650</td>\n",
              "      <td>0.597357</td>\n",
              "      <td>0.225828</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>1.340414</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>...</td>\n",
              "      <td>0.597357</td>\n",
              "      <td>1.526179</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>0.968886</td>\n",
              "      <td>0.597357</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>1.897708</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>0.040064</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>0.968886</td>\n",
              "      <td>2.826529</td>\n",
              "      <td>0.040064</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.517229</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>0.225828</td>\n",
              "      <td>0.040064</td>\n",
              "      <td>0.225828</td>\n",
              "      <td>-0.145700</td>\n",
              "      <td>1.340414</td>\n",
              "      <td>0.411593</td>\n",
              "      <td>-0.702994</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>-0.888758</td>\n",
              "      <td>-0.331465</td>\n",
              "      <td>0.225828</td>\n",
              "      <td>1.897708</td>\n",
              "      <td>-0.702994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NPCs_8_GW16</th>\n",
              "      <td>0.266267</td>\n",
              "      <td>0.632407</td>\n",
              "      <td>-0.282944</td>\n",
              "      <td>2.096969</td>\n",
              "      <td>1.730828</td>\n",
              "      <td>1.364688</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>2.096969</td>\n",
              "      <td>1.364688</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>-0.832154</td>\n",
              "      <td>-0.282944</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>1.913899</td>\n",
              "      <td>1.364688</td>\n",
              "      <td>1.730828</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>1.364688</td>\n",
              "      <td>1.547758</td>\n",
              "      <td>0.998548</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>-0.649084</td>\n",
              "      <td>0.815478</td>\n",
              "      <td>0.815478</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>0.449337</td>\n",
              "      <td>-0.282944</td>\n",
              "      <td>1.730828</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>0.632407</td>\n",
              "      <td>1.547758</td>\n",
              "      <td>0.083197</td>\n",
              "      <td>0.083197</td>\n",
              "      <td>1.364688</td>\n",
              "      <td>0.632407</td>\n",
              "      <td>0.449337</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815478</td>\n",
              "      <td>1.181618</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>1.181618</td>\n",
              "      <td>0.815478</td>\n",
              "      <td>-0.832154</td>\n",
              "      <td>1.547758</td>\n",
              "      <td>0.815478</td>\n",
              "      <td>-0.832154</td>\n",
              "      <td>-0.649084</td>\n",
              "      <td>-0.649084</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>-0.649084</td>\n",
              "      <td>1.730828</td>\n",
              "      <td>1.913899</td>\n",
              "      <td>0.449337</td>\n",
              "      <td>0.449337</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>-0.649084</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>-0.282944</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>-0.282944</td>\n",
              "      <td>0.449337</td>\n",
              "      <td>0.266267</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>-0.832154</td>\n",
              "      <td>0.998548</td>\n",
              "      <td>0.083197</td>\n",
              "      <td>-0.832154</td>\n",
              "      <td>0.083197</td>\n",
              "      <td>-1.015224</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>0.632407</td>\n",
              "      <td>1.181618</td>\n",
              "      <td>-0.466014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NPCs_7_GW16</th>\n",
              "      <td>0.581812</td>\n",
              "      <td>1.378971</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>1.511831</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>1.777551</td>\n",
              "      <td>1.511831</td>\n",
              "      <td>0.714672</td>\n",
              "      <td>-1.012506</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>-1.012506</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>-0.879646</td>\n",
              "      <td>0.847532</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>0.980392</td>\n",
              "      <td>1.777551</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>1.113251</td>\n",
              "      <td>1.113251</td>\n",
              "      <td>0.714672</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>1.378971</td>\n",
              "      <td>0.581812</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>0.448952</td>\n",
              "      <td>0.448952</td>\n",
              "      <td>1.777551</td>\n",
              "      <td>-0.215347</td>\n",
              "      <td>1.378971</td>\n",
              "      <td>0.980392</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>1.378971</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>-1.012506</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183233</td>\n",
              "      <td>1.378971</td>\n",
              "      <td>0.847532</td>\n",
              "      <td>0.714672</td>\n",
              "      <td>1.246111</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>1.511831</td>\n",
              "      <td>0.581812</td>\n",
              "      <td>-0.481066</td>\n",
              "      <td>-0.215347</td>\n",
              "      <td>-0.481066</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>-0.082487</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>-0.082487</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>1.777551</td>\n",
              "      <td>-0.613926</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>-0.746786</td>\n",
              "      <td>-0.613926</td>\n",
              "      <td>-0.348207</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>-0.348207</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>-0.082487</td>\n",
              "      <td>0.714672</td>\n",
              "      <td>0.448952</td>\n",
              "      <td>0.581812</td>\n",
              "      <td>-0.082487</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>0.714672</td>\n",
              "      <td>-1.012506</td>\n",
              "      <td>0.316092</td>\n",
              "      <td>-1.145366</td>\n",
              "      <td>0.050373</td>\n",
              "      <td>1.246111</td>\n",
              "      <td>1.644691</td>\n",
              "      <td>0.316092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NPCs_9_GW16</th>\n",
              "      <td>0.325422</td>\n",
              "      <td>0.452639</td>\n",
              "      <td>-0.310664</td>\n",
              "      <td>2.106463</td>\n",
              "      <td>1.597594</td>\n",
              "      <td>1.979246</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>2.488115</td>\n",
              "      <td>1.597594</td>\n",
              "      <td>0.707074</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.437881</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.310664</td>\n",
              "      <td>0.452639</td>\n",
              "      <td>1.979246</td>\n",
              "      <td>0.961508</td>\n",
              "      <td>1.215943</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>0.961508</td>\n",
              "      <td>0.579857</td>\n",
              "      <td>0.834291</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.437881</td>\n",
              "      <td>0.834291</td>\n",
              "      <td>0.961508</td>\n",
              "      <td>0.198205</td>\n",
              "      <td>-0.310664</td>\n",
              "      <td>0.070988</td>\n",
              "      <td>2.233681</td>\n",
              "      <td>-0.437881</td>\n",
              "      <td>0.961508</td>\n",
              "      <td>0.579857</td>\n",
              "      <td>0.452639</td>\n",
              "      <td>-0.056230</td>\n",
              "      <td>0.452639</td>\n",
              "      <td>1.088726</td>\n",
              "      <td>-0.056230</td>\n",
              "      <td>-0.565098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.325422</td>\n",
              "      <td>1.343160</td>\n",
              "      <td>0.070988</td>\n",
              "      <td>1.088726</td>\n",
              "      <td>0.325422</td>\n",
              "      <td>-0.565098</td>\n",
              "      <td>1.215943</td>\n",
              "      <td>0.579857</td>\n",
              "      <td>-0.692316</td>\n",
              "      <td>0.198205</td>\n",
              "      <td>-0.819533</td>\n",
              "      <td>0.198205</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.565098</td>\n",
              "      <td>1.597594</td>\n",
              "      <td>2.233681</td>\n",
              "      <td>-0.565098</td>\n",
              "      <td>0.707074</td>\n",
              "      <td>-0.692316</td>\n",
              "      <td>0.325422</td>\n",
              "      <td>-0.565098</td>\n",
              "      <td>0.070988</td>\n",
              "      <td>-0.056230</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.437881</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.310664</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>1.215943</td>\n",
              "      <td>-0.056230</td>\n",
              "      <td>-0.819533</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>-0.946750</td>\n",
              "      <td>-0.183447</td>\n",
              "      <td>0.198205</td>\n",
              "      <td>1.343160</td>\n",
              "      <td>-0.310664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OPCs_2_GW16</th>\n",
              "      <td>-0.403946</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>2.353695</td>\n",
              "      <td>0.699110</td>\n",
              "      <td>1.802167</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>2.353695</td>\n",
              "      <td>1.802167</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>1.802167</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>2.077931</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>1.802167</td>\n",
              "      <td>0.699110</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>-0.128182</td>\n",
              "      <td>1.802167</td>\n",
              "      <td>-0.128182</td>\n",
              "      <td>2.077931</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>1.526403</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.128182</td>\n",
              "      <td>0.699110</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>0.699110</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>1.526403</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>0.147582</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>1.526403</td>\n",
              "      <td>2.077931</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>1.250639</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>-0.403946</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>1.250639</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>-0.955475</td>\n",
              "      <td>-0.679710</td>\n",
              "      <td>0.423346</td>\n",
              "      <td>1.526403</td>\n",
              "      <td>-0.403946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95 rows × 787 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                AASDH     ABCA1     ABCA2  ...    ZSCAN2    ZSWIM7      ZW10\n",
              "hRgl2a_7     0.301465 -0.329083 -0.644357  ... -0.644357 -0.644357 -0.013809\n",
              "hProgBP_7    0.580237 -0.190516  0.580237  ... -0.575892 -0.190516  0.580237\n",
              "hOMTN_7      0.991196 -0.312976 -0.312976  ... -0.312976  1.643282 -0.639018\n",
              "hRgl2a_9     2.292747  0.170520 -0.678371  ...  0.170520 -0.253926 -0.253926\n",
              "hNProg_9     1.255239  1.255239 -0.181248  ... -0.660077 -0.181248 -0.181248\n",
              "...               ...       ...       ...  ...       ...       ...       ...\n",
              "NPCs_6_GW16  0.597357  1.154650  0.225828  ...  0.225828  1.897708 -0.702994\n",
              "NPCs_8_GW16  0.266267  0.632407 -0.282944  ...  0.632407  1.181618 -0.466014\n",
              "NPCs_7_GW16  0.581812  1.378971  0.050373  ...  1.246111  1.644691  0.316092\n",
              "NPCs_9_GW16  0.325422  0.452639 -0.310664  ...  0.198205  1.343160 -0.310664\n",
              "OPCs_2_GW16 -0.403946  0.974874  0.423346  ...  0.423346  1.526403 -0.403946\n",
              "\n",
              "[95 rows x 787 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQC4aWWLo_hA"
      },
      "source": [
        "inputs_train, inputs_val, labels_train, labels_val = model_selection.train_test_split(aut_exp_n, aut_risk_n, test_size=0.2,\n",
        "                                                        random_state=0)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7gyqil06TVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77641927-8bc8-46dc-8e4c-f58cf9732ff0"
      },
      "source": [
        "inputs_train = torch.tensor(inputs_train, dtype=torch.float32, requires_grad=True)\n",
        "inputs_val = torch.tensor(inputs_val, dtype=torch.float32, requires_grad=True)\n",
        "labels_train = torch.tensor(labels_train, dtype=torch.long)\n",
        "labels_val = torch.tensor(labels_val, dtype=torch.long)\n",
        "[inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, \n",
        " inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, \n",
        " labels_val.shape, labels_val.dtype]"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([629, 95]),\n",
              " torch.float32,\n",
              " True,\n",
              " torch.Size([158, 95]),\n",
              " torch.float32,\n",
              " torch.Size([629]),\n",
              " torch.int64,\n",
              " torch.Size([158]),\n",
              " torch.int64]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyhmlRrb28ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4e980f-90e3-403d-b093-ba8848a5c3d5"
      },
      "source": [
        "inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, labels_val.shape, labels_val.dtype"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([629, 95]),\n",
              " torch.float32,\n",
              " True,\n",
              " torch.Size([158, 95]),\n",
              " torch.float32,\n",
              " torch.Size([629]),\n",
              " torch.int64,\n",
              " torch.Size([158]),\n",
              " torch.int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYhqFcVdA8J"
      },
      "source": [
        "## Init model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d343Nq6JcQED"
      },
      "source": [
        "aut_embed = CustomEmbedding(95, 60)\n",
        "aut_cla = SimpleNN(aut_embed, 60, 2)\n",
        "loss_func = nn.CrossEntropyLoss(torch.tensor([0.7,  0.3]))\n",
        "optimizer = torch.optim.Adam(aut_cla.parameters())\n",
        "model = aut_cla\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UAC0csMVLLh"
      },
      "source": [
        "from copy import  copy"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBKuQ0rxc9bv"
      },
      "source": [
        "## train for autism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7nDqr8tXv3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f399a40b-51f5-4c64-b456-f9571817960b"
      },
      "source": [
        "max_val_accs = []\n",
        "aut_embed_state_dicts = []\n",
        "for i in range(20):\n",
        "    aut_embed = CustomEmbedding(95, 60)\n",
        "    aut_cla = SimpleNN(aut_embed, 60, 2)\n",
        "    loss_func = nn.CrossEntropyLoss(torch.tensor([0.3,  0.7]))\n",
        "    optimizer = torch.optim.Adam(aut_cla.parameters())\n",
        "    model = aut_cla\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    num_epochs = 100000\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    print_freq = 1000\n",
        "    patience = 2000\n",
        "    patience_2 = patience\n",
        "    max_val_acc = 0\n",
        "    aut_embed_state_dict = None\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        if patience==0:\n",
        "            print(\"Training complete\")\n",
        "            break\n",
        "        model.train()\n",
        "        train_loss, train_acc = train_one_epoch(model, inputs_train, labels_train, loss_func, optimizer, device, acc_func=calc_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # evaluate on the test dataset\n",
        "        model.eval()\n",
        "        val_loss, accuracy, precision, recall, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "        patience -= 1\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            patience =  patience_2\n",
        "            aut_embed_state_dict = aut_embed.state_dict().copy()\n",
        "            print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}, {accuracy}\")\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        if epoch%print_freq==0:    \n",
        "            print(f\"Epoch =  {epoch} \\t Train loss = {train_loss:.5f} \\t  Train Acc = {train_acc:.5f} \\t\",\n",
        "            f\"Val loss = {val_loss:.5f} \\t Acc = {precision:.5f}{recall:.5f}{val_acc:.5f} \\t Patience = {patience}\")\n",
        "    max_val_accs.append(max_val_acc)\n",
        "    aut_embed_state_dicts.append(aut_embed_state_dict)\n",
        "\n",
        "print(\"Max val losses________________\")\n",
        "print(max_val_accs)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Max val Acc = 0.42384102940559387, 0.44936707615852356\n",
            "Epoch =  0 \t Train loss = 0.75125 \t  Train Acc = 0.55803 \t Val loss = 0.69056 \t Acc = 0.329900.592590.42384 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.43356648087501526, 0.4873417615890503\n",
            "Epoch 3 \t Max val Acc = 0.43661975860595703, 0.49367088079452515\n",
            "Epoch 4 \t Max val Acc = 0.4740740656852722, 0.550632894039154\n",
            "Epoch 5 \t Max val Acc = 0.5116279125213623, 0.6012658476829529\n",
            "Epoch 6 \t Max val Acc = 0.5538461804389954, 0.6329113841056824\n",
            "Epoch 7 \t Max val Acc = 0.5625, 0.6455696225166321\n",
            "Epoch 11 \t Max val Acc = 0.585365891456604, 0.6772152185440063\n",
            "Epoch 12 \t Max val Acc = 0.5932203531265259, 0.6962025165557861\n",
            "Epoch 37 \t Max val Acc = 0.5982906222343445, 0.702531635761261\n",
            "Epoch =  1000 \t Train loss = 0.08269 \t  Train Acc = 0.96025 \t Val loss = 3.05865 \t Acc = 0.482760.259260.33735 \t Patience = 1037\n",
            "Epoch =  2000 \t Train loss = 0.02986 \t  Train Acc = 0.98569 \t Val loss = 4.17522 \t Acc = 0.464290.240740.31707 \t Patience = 37\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.301369845867157, 0.6772152185440063\n",
            "Epoch =  0 \t Train loss = 0.76379 \t  Train Acc = 0.54213 \t Val loss = 0.69167 \t Acc = 0.578950.203700.30137 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.3820224702358246, 0.6518987417221069\n",
            "Epoch 2 \t Max val Acc = 0.3913043737411499, 0.6455696225166321\n",
            "Epoch 12 \t Max val Acc = 0.40776699781417847, 0.6139240264892578\n",
            "Epoch 13 \t Max val Acc = 0.4313725531101227, 0.6329113841056824\n",
            "Epoch 14 \t Max val Acc = 0.4571428596973419, 0.6392405033111572\n",
            "Epoch 18 \t Max val Acc = 0.4672897458076477, 0.6392405033111572\n",
            "Epoch 21 \t Max val Acc = 0.4716981053352356, 0.6455696225166321\n",
            "Epoch 25 \t Max val Acc = 0.47706422209739685, 0.6392405033111572\n",
            "Epoch 26 \t Max val Acc = 0.5, 0.6455696225166321\n",
            "Epoch 27 \t Max val Acc = 0.5178571343421936, 0.6582278609275818\n",
            "Epoch 31 \t Max val Acc = 0.5263158082962036, 0.6582278609275818\n",
            "Epoch 41 \t Max val Acc = 0.5344827771186829, 0.6582278609275818\n",
            "Epoch 43 \t Max val Acc = 0.5391303896903992, 0.6645569801330566\n",
            "Epoch 44 \t Max val Acc = 0.5565217137336731, 0.6772152185440063\n",
            "Epoch =  1000 \t Train loss = 0.06496 \t  Train Acc = 0.98251 \t Val loss = 3.22978 \t Acc = 0.520000.240740.32911 \t Patience = 1044\n",
            "Epoch =  2000 \t Train loss = 0.05406 \t  Train Acc = 0.97933 \t Val loss = 3.87023 \t Acc = 0.571430.370370.44944 \t Patience = 44\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.34074077010154724, 0.43670886754989624\n",
            "Epoch =  0 \t Train loss = 0.75528 \t  Train Acc = 0.51033 \t Val loss = 0.69815 \t Acc = 0.283950.425930.34074 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.3636363744735718, 0.5126582384109497\n",
            "Epoch 3 \t Max val Acc = 0.4166666269302368, 0.5569620132446289\n",
            "Epoch 21 \t Max val Acc = 0.4356435537338257, 0.6392405033111572\n",
            "Epoch 27 \t Max val Acc = 0.44897958636283875, 0.6582278609275818\n",
            "Epoch 38 \t Max val Acc = 0.46000000834465027, 0.6582278609275818\n",
            "Epoch 39 \t Max val Acc = 0.4705882668495178, 0.6582278609275818\n",
            "Epoch 46 \t Max val Acc = 0.4716981053352356, 0.6455696225166321\n",
            "Epoch 63 \t Max val Acc = 0.48543688654899597, 0.6645569801330566\n",
            "Epoch 64 \t Max val Acc = 0.4999999403953552, 0.6708860993385315\n",
            "Epoch 66 \t Max val Acc = 0.5333333611488342, 0.6898733973503113\n",
            "Epoch 67 \t Max val Acc = 0.5471697449684143, 0.6962025165557861\n",
            "Epoch =  1000 \t Train loss = 0.05950 \t  Train Acc = 0.98092 \t Val loss = 2.45736 \t Acc = 0.521740.222220.31169 \t Patience = 1067\n",
            "Epoch =  2000 \t Train loss = 0.03729 \t  Train Acc = 0.98887 \t Val loss = 2.89001 \t Acc = 0.608700.259260.36364 \t Patience = 67\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.49162009358406067, 0.42405062913894653\n",
            "Epoch =  0 \t Train loss = 0.80966 \t  Train Acc = 0.43720 \t Val loss = 0.69599 \t Acc = 0.352000.814810.49162 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.5051546096801758, 0.39240506291389465\n",
            "Epoch 32 \t Max val Acc = 0.5166667103767395, 0.6329113841056824\n",
            "Epoch 33 \t Max val Acc = 0.5528455376625061, 0.6518987417221069\n",
            "Epoch 34 \t Max val Acc = 0.5573770403862, 0.6582278609275818\n",
            "Epoch 35 \t Max val Acc = 0.5759999752044678, 0.6645569801330566\n",
            "Epoch =  1000 \t Train loss = 0.06306 \t  Train Acc = 0.97456 \t Val loss = 3.41042 \t Acc = 0.437500.259260.32558 \t Patience = 1035\n",
            "Epoch =  2000 \t Train loss = 0.03700 \t  Train Acc = 0.99046 \t Val loss = 4.38452 \t Acc = 0.444440.222220.29630 \t Patience = 35\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.3404255211353302, 0.4113923907279968\n",
            "Epoch =  0 \t Train loss = 0.76302 \t  Train Acc = 0.51987 \t Val loss = 0.69726 \t Acc = 0.275860.444440.34043 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.36764705181121826, 0.4556961953639984\n",
            "Epoch 2 \t Max val Acc = 0.38016530871391296, 0.5253164768218994\n",
            "Epoch 3 \t Max val Acc = 0.4173913300037384, 0.5759493708610535\n",
            "Epoch 17 \t Max val Acc = 0.4285714328289032, 0.6962025165557861\n",
            "Epoch 18 \t Max val Acc = 0.44186046719551086, 0.6962025165557861\n",
            "Epoch 20 \t Max val Acc = 0.449438214302063, 0.6898733973503113\n",
            "Epoch 47 \t Max val Acc = 0.4536082446575165, 0.6645569801330566\n",
            "Epoch 48 \t Max val Acc = 0.46464645862579346, 0.6645569801330566\n",
            "Epoch 49 \t Max val Acc = 0.48000001907348633, 0.6708860993385315\n",
            "Epoch 54 \t Max val Acc = 0.48076921701431274, 0.6582278609275818\n",
            "Epoch 56 \t Max val Acc = 0.48543688654899597, 0.6645569801330566\n",
            "Epoch 71 \t Max val Acc = 0.5148515105247498, 0.6898733973503113\n",
            "Epoch 98 \t Max val Acc = 0.5346534252166748, 0.702531635761261\n",
            "Epoch =  1000 \t Train loss = 0.06798 \t  Train Acc = 0.96820 \t Val loss = 2.85743 \t Acc = 0.484850.296300.36782 \t Patience = 1098\n",
            "Epoch =  2000 \t Train loss = 0.03151 \t  Train Acc = 0.98569 \t Val loss = 3.78574 \t Acc = 0.486490.333330.39560 \t Patience = 98\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.28282827138900757, 0.550632894039154\n",
            "Epoch =  0 \t Train loss = 0.77809 \t  Train Acc = 0.50238 \t Val loss = 0.69286 \t Acc = 0.311110.259260.28283 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.3137255012989044, 0.5569620132446289\n",
            "Epoch 2 \t Max val Acc = 0.36190474033355713, 0.5759493708610535\n",
            "Epoch 3 \t Max val Acc = 0.38532111048698425, 0.5759493708610535\n",
            "Epoch 4 \t Max val Acc = 0.4107142686843872, 0.5822784900665283\n",
            "Epoch 5 \t Max val Acc = 0.4137931168079376, 0.5696202516555786\n",
            "Epoch 9 \t Max val Acc = 0.4247787892818451, 0.5886076092720032\n",
            "Epoch 11 \t Max val Acc = 0.4482758343219757, 0.594936728477478\n",
            "Epoch 14 \t Max val Acc = 0.4576271176338196, 0.594936728477478\n",
            "Epoch 15 \t Max val Acc = 0.47457626461982727, 0.607594907283783\n",
            "Epoch 16 \t Max val Acc = 0.5254237651824951, 0.6455696225166321\n",
            "Epoch 17 \t Max val Acc = 0.5289255976676941, 0.6392405033111572\n",
            "Epoch 18 \t Max val Acc = 0.5409835577011108, 0.6455696225166321\n",
            "Epoch 54 \t Max val Acc = 0.5645161271095276, 0.6582278609275818\n",
            "Epoch 55 \t Max val Acc = 0.5714285373687744, 0.6582278609275818\n",
            "Epoch =  1000 \t Train loss = 0.06789 \t  Train Acc = 0.97615 \t Val loss = 3.35389 \t Acc = 0.500000.203700.28947 \t Patience = 1055\n",
            "Epoch =  2000 \t Train loss = 0.03944 \t  Train Acc = 0.98410 \t Val loss = 4.30335 \t Acc = 0.541670.240740.33333 \t Patience = 55\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.78653 \t  Train Acc = 0.52623 \t Val loss = 0.71666 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 2 \t Max val Acc = 0.036363635212183, 0.6645569801330566\n",
            "Epoch 3 \t Max val Acc = 0.10526315867900848, 0.6772152185440063\n",
            "Epoch 4 \t Max val Acc = 0.16949151456356049, 0.6898733973503113\n",
            "Epoch 5 \t Max val Acc = 0.2500000298023224, 0.6962025165557861\n",
            "Epoch 6 \t Max val Acc = 0.27272728085517883, 0.6962025165557861\n",
            "Epoch 7 \t Max val Acc = 0.28985506296157837, 0.6898733973503113\n",
            "Epoch 8 \t Max val Acc = 0.32876715064048767, 0.6898733973503113\n",
            "Epoch 10 \t Max val Acc = 0.34210526943206787, 0.6835442781448364\n",
            "Epoch 12 \t Max val Acc = 0.37037038803100586, 0.6772152185440063\n",
            "Epoch 13 \t Max val Acc = 0.39024391770362854, 0.6835442781448364\n",
            "Epoch 14 \t Max val Acc = 0.42352941632270813, 0.6898733973503113\n",
            "Epoch 15 \t Max val Acc = 0.4269663095474243, 0.6772152185440063\n",
            "Epoch 16 \t Max val Acc = 0.4680851101875305, 0.6835442781448364\n",
            "Epoch 19 \t Max val Acc = 0.47422680258750916, 0.6772152185440063\n",
            "Epoch 42 \t Max val Acc = 0.5, 0.6582278609275818\n",
            "Epoch 43 \t Max val Acc = 0.5046728849411011, 0.6645569801330566\n",
            "Epoch 44 \t Max val Acc = 0.5137615203857422, 0.6645569801330566\n",
            "Epoch 54 \t Max val Acc = 0.5225224494934082, 0.6645569801330566\n",
            "Epoch 58 \t Max val Acc = 0.5233644843101501, 0.6772152185440063\n",
            "Epoch =  1000 \t Train loss = 0.06837 \t  Train Acc = 0.97297 \t Val loss = 2.49669 \t Acc = 0.481480.240740.32099 \t Patience = 1058\n",
            "Epoch =  2000 \t Train loss = 0.05065 \t  Train Acc = 0.98410 \t Val loss = 3.61946 \t Acc = 0.565220.240740.33766 \t Patience = 58\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.0357142873108387, 0.6582278609275818\n",
            "Epoch =  0 \t Train loss = 0.80059 \t  Train Acc = 0.52146 \t Val loss = 0.69480 \t Acc = 0.500000.018520.03571 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.07017543911933899, 0.6645569801330566\n",
            "Epoch 2 \t Max val Acc = 0.2539682388305664, 0.702531635761261\n",
            "Epoch 4 \t Max val Acc = 0.2702702581882477, 0.6582278609275818\n",
            "Epoch 5 \t Max val Acc = 0.2716049253940582, 0.6265822649002075\n",
            "Epoch 7 \t Max val Acc = 0.3255814015865326, 0.6329113841056824\n",
            "Epoch 9 \t Max val Acc = 0.3448275923728943, 0.6392405033111572\n",
            "Epoch 10 \t Max val Acc = 0.4000000059604645, 0.6582278609275818\n",
            "Epoch 11 \t Max val Acc = 0.4130434989929199, 0.6582278609275818\n",
            "Epoch 13 \t Max val Acc = 0.42553189396858215, 0.6582278609275818\n",
            "Epoch 15 \t Max val Acc = 0.4536082446575165, 0.6645569801330566\n",
            "Epoch 16 \t Max val Acc = 0.4848484694957733, 0.6772152185440063\n",
            "Epoch 18 \t Max val Acc = 0.48543688654899597, 0.6645569801330566\n",
            "Epoch 21 \t Max val Acc = 0.509433925151825, 0.6708860993385315\n",
            "Epoch 22 \t Max val Acc = 0.5283018350601196, 0.6835442781448364\n",
            "Epoch 23 \t Max val Acc = 0.5420560836791992, 0.6898733973503113\n",
            "Epoch 24 \t Max val Acc = 0.5660377144813538, 0.7088607549667358\n",
            "Epoch 26 \t Max val Acc = 0.5688073635101318, 0.702531635761261\n",
            "Epoch 29 \t Max val Acc = 0.5840708017349243, 0.702531635761261\n",
            "Epoch 30 \t Max val Acc = 0.596491277217865, 0.7088607549667358\n",
            "Epoch =  1000 \t Train loss = 0.09716 \t  Train Acc = 0.97615 \t Val loss = 2.52036 \t Acc = 0.593750.351850.44186 \t Patience = 1030\n",
            "Epoch =  2000 \t Train loss = 0.05767 \t  Train Acc = 0.98410 \t Val loss = 3.44058 \t Acc = 0.593750.351850.44186 \t Patience = 30\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.3255814015865326, 0.6329113841056824\n",
            "Epoch =  0 \t Train loss = 0.79725 \t  Train Acc = 0.45946 \t Val loss = 0.69001 \t Acc = 0.437500.259260.32558 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.3499999940395355, 0.6708860993385315\n",
            "Epoch 2 \t Max val Acc = 0.35555553436279297, 0.6329113841056824\n",
            "Epoch 3 \t Max val Acc = 0.35999998450279236, 0.594936728477478\n",
            "Epoch 5 \t Max val Acc = 0.365217387676239, 0.5379746556282043\n",
            "Epoch 10 \t Max val Acc = 0.37168142199516296, 0.550632894039154\n",
            "Epoch 11 \t Max val Acc = 0.4000000059604645, 0.5632911324501038\n",
            "Epoch 12 \t Max val Acc = 0.42105263471603394, 0.5822784900665283\n",
            "Epoch 13 \t Max val Acc = 0.4247787892818451, 0.5886076092720032\n",
            "Epoch 14 \t Max val Acc = 0.4285714626312256, 0.594936728477478\n",
            "Epoch 15 \t Max val Acc = 0.4363636374473572, 0.607594907283783\n",
            "Epoch 16 \t Max val Acc = 0.4909090995788574, 0.6455696225166321\n",
            "Epoch 17 \t Max val Acc = 0.4954128563404083, 0.6518987417221069\n",
            "Epoch 21 \t Max val Acc = 0.5137615203857422, 0.6645569801330566\n",
            "Epoch 22 \t Max val Acc = 0.5272727012634277, 0.6708860993385315\n",
            "Epoch 34 \t Max val Acc = 0.5499999523162842, 0.6582278609275818\n",
            "Epoch 37 \t Max val Acc = 0.5528455376625061, 0.6518987417221069\n",
            "Epoch 38 \t Max val Acc = 0.559999942779541, 0.6518987417221069\n",
            "Epoch 39 \t Max val Acc = 0.5714285373687744, 0.6582278609275818\n",
            "Epoch 55 \t Max val Acc = 0.5714285969734192, 0.6772152185440063\n",
            "Epoch 57 \t Max val Acc = 0.5737705230712891, 0.6708860993385315\n",
            "Epoch 58 \t Max val Acc = 0.5806452035903931, 0.6708860993385315\n",
            "Epoch 59 \t Max val Acc = 0.585365891456604, 0.6772152185440063\n",
            "Epoch 61 \t Max val Acc = 0.5901639461517334, 0.6835442781448364\n",
            "Epoch =  1000 \t Train loss = 0.04611 \t  Train Acc = 0.97774 \t Val loss = 2.95199 \t Acc = 0.560000.259260.35443 \t Patience = 1061\n",
            "Epoch =  2000 \t Train loss = 0.04488 \t  Train Acc = 0.98887 \t Val loss = 3.63610 \t Acc = 0.521740.222220.31169 \t Patience = 61\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.5094339847564697, 0.3417721390724182\n",
            "Epoch =  0 \t Train loss = 0.79091 \t  Train Acc = 0.46105 \t Val loss = 0.67980 \t Acc = 0.341771.000000.50943 \t Patience = 2000\n",
            "Epoch 7 \t Max val Acc = 0.5263158082962036, 0.4303797483444214\n",
            "Epoch 8 \t Max val Acc = 0.530386745929718, 0.46202531456947327\n",
            "Epoch 12 \t Max val Acc = 0.5308642387390137, 0.5189873576164246\n",
            "Epoch 13 \t Max val Acc = 0.5477706789970398, 0.550632894039154\n",
            "Epoch 14 \t Max val Acc = 0.5503355860710144, 0.5759493708610535\n",
            "Epoch 31 \t Max val Acc = 0.5666666626930237, 0.6708860993385315\n",
            "Epoch 33 \t Max val Acc = 0.5950413346290588, 0.6898733973503113\n",
            "Epoch 34 \t Max val Acc = 0.5967742204666138, 0.6835442781448364\n",
            "Epoch =  1000 \t Train loss = 0.05588 \t  Train Acc = 0.97297 \t Val loss = 2.82093 \t Acc = 0.533330.148150.23188 \t Patience = 1034\n",
            "Epoch =  2000 \t Train loss = 0.05350 \t  Train Acc = 0.97615 \t Val loss = 3.76812 \t Acc = 0.450000.166670.24324 \t Patience = 34\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.5118483304977417, 0.34810125827789307\n",
            "Epoch =  0 \t Train loss = 0.77686 \t  Train Acc = 0.45151 \t Val loss = 0.68023 \t Acc = 0.343951.000000.51185 \t Patience = 2000\n",
            "Epoch 3 \t Max val Acc = 0.5142857432365417, 0.3544303774833679\n",
            "Epoch 4 \t Max val Acc = 0.5192307829856873, 0.3670886158943176\n",
            "Epoch 11 \t Max val Acc = 0.5202312469482422, 0.474683552980423\n",
            "Epoch 13 \t Max val Acc = 0.5276073813438416, 0.5126582384109497\n",
            "Epoch 14 \t Max val Acc = 0.5308642387390137, 0.5189873576164246\n",
            "Epoch 15 \t Max val Acc = 0.5375000238418579, 0.5316455960273743\n",
            "Epoch 16 \t Max val Acc = 0.5695364475250244, 0.5886076092720032\n",
            "Epoch 18 \t Max val Acc = 0.5774648189544678, 0.6202531456947327\n",
            "Epoch 20 \t Max val Acc = 0.5846154093742371, 0.6582278609275818\n",
            "Epoch 23 \t Max val Acc = 0.5873016715049744, 0.6708860993385315\n",
            "Epoch 24 \t Max val Acc = 0.5920000076293945, 0.6772152185440063\n",
            "Epoch =  1000 \t Train loss = 0.07529 \t  Train Acc = 0.98092 \t Val loss = 2.44828 \t Acc = 0.463410.351850.40000 \t Patience = 1024\n",
            "Epoch =  2000 \t Train loss = 0.05249 \t  Train Acc = 0.97774 \t Val loss = 3.56750 \t Acc = 0.487180.351850.40860 \t Patience = 24\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.06779661029577255, 0.6518987417221069\n",
            "Epoch =  0 \t Train loss = 0.81316 \t  Train Acc = 0.51351 \t Val loss = 0.70272 \t Acc = 0.400000.037040.06780 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.06896551698446274, 0.6582278609275818\n",
            "Epoch 3 \t Max val Acc = 0.13114753365516663, 0.6645569801330566\n",
            "Epoch 4 \t Max val Acc = 0.158730149269104, 0.6645569801330566\n",
            "Epoch 5 \t Max val Acc = 0.17910447716712952, 0.6518987417221069\n",
            "Epoch 6 \t Max val Acc = 0.22857141494750977, 0.6582278609275818\n",
            "Epoch 7 \t Max val Acc = 0.27397260069847107, 0.6645569801330566\n",
            "Epoch 8 \t Max val Acc = 0.29729729890823364, 0.6708860993385315\n",
            "Epoch 9 \t Max val Acc = 0.3116883337497711, 0.6645569801330566\n",
            "Epoch 12 \t Max val Acc = 0.3291139006614685, 0.6645569801330566\n",
            "Epoch 14 \t Max val Acc = 0.375, 0.6835442781448364\n",
            "Epoch 15 \t Max val Acc = 0.37974685430526733, 0.6898733973503113\n",
            "Epoch 17 \t Max val Acc = 0.4146341383457184, 0.6962025165557861\n",
            "Epoch 21 \t Max val Acc = 0.42352941632270813, 0.6898733973503113\n",
            "Epoch 24 \t Max val Acc = 0.4318181574344635, 0.6835442781448364\n",
            "Epoch 33 \t Max val Acc = 0.4444444179534912, 0.6835442781448364\n",
            "Epoch 44 \t Max val Acc = 0.4536082446575165, 0.6645569801330566\n",
            "Epoch 58 \t Max val Acc = 0.4554455578327179, 0.6518987417221069\n",
            "Epoch 59 \t Max val Acc = 0.4705882668495178, 0.6582278609275818\n",
            "Epoch 77 \t Max val Acc = 0.48076921701431274, 0.6582278609275818\n",
            "Epoch 78 \t Max val Acc = 0.48543688654899597, 0.6645569801330566\n",
            "Epoch =  1000 \t Train loss = 0.04001 \t  Train Acc = 0.99205 \t Val loss = 2.81734 \t Acc = 0.380950.296300.33333 \t Patience = 1078\n",
            "Epoch 1223 \t Max val Acc = 0.485981285572052, 0.6518987417221069\n",
            "Epoch 1392 \t Max val Acc = 0.4901961088180542, 0.6708860993385315\n",
            "Epoch =  2000 \t Train loss = 0.06468 \t  Train Acc = 0.98092 \t Val loss = 3.50484 \t Acc = 0.435900.314810.36559 \t Patience = 1392\n",
            "Epoch 2644 \t Max val Acc = 0.49056604504585266, 0.6582278609275818\n",
            "Epoch 2861 \t Max val Acc = 0.49504950642585754, 0.6772152185440063\n",
            "Epoch 2862 \t Max val Acc = 0.5048543214797974, 0.6772152185440063\n",
            "Epoch =  3000 \t Train loss = 0.06713 \t  Train Acc = 0.97774 \t Val loss = 3.89420 \t Acc = 0.461540.444440.45283 \t Patience = 1862\n",
            "Epoch 3241 \t Max val Acc = 0.5137615203857422, 0.6645569801330566\n",
            "Epoch 3678 \t Max val Acc = 0.5185185074806213, 0.6708860993385315\n",
            "Epoch 3871 \t Max val Acc = 0.5283018350601196, 0.6835442781448364\n",
            "Epoch =  4000 \t Train loss = 0.03478 \t  Train Acc = 0.99364 \t Val loss = 4.35630 \t Acc = 0.461540.333330.38710 \t Patience = 1871\n",
            "Epoch =  5000 \t Train loss = 0.05027 \t  Train Acc = 0.98410 \t Val loss = 4.25841 \t Acc = 0.420000.388890.40385 \t Patience = 871\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.1315789520740509, 0.5822784900665283\n",
            "Epoch =  0 \t Train loss = 0.74619 \t  Train Acc = 0.55644 \t Val loss = 0.70986 \t Acc = 0.227270.092590.13158 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.1492537260055542, 0.6392405033111572\n",
            "Epoch 15 \t Max val Acc = 0.1538461595773697, 0.6518987417221069\n",
            "Epoch 17 \t Max val Acc = 0.20588235557079315, 0.6582278609275818\n",
            "Epoch 18 \t Max val Acc = 0.22857141494750977, 0.6582278609275818\n",
            "Epoch 19 \t Max val Acc = 0.2535211443901062, 0.6645569801330566\n",
            "Epoch 21 \t Max val Acc = 0.277777761220932, 0.6708860993385315\n",
            "Epoch 22 \t Max val Acc = 0.301369845867157, 0.6772152185440063\n",
            "Epoch 24 \t Max val Acc = 0.3636363744735718, 0.6898733973503113\n",
            "Epoch 26 \t Max val Acc = 0.395061731338501, 0.6898733973503113\n",
            "Epoch 27 \t Max val Acc = 0.40909093618392944, 0.6708860993385315\n",
            "Epoch 28 \t Max val Acc = 0.4269663095474243, 0.6772152185440063\n",
            "Epoch 29 \t Max val Acc = 0.4395604431629181, 0.6772152185440063\n",
            "Epoch 40 \t Max val Acc = 0.4421052634716034, 0.6645569801330566\n",
            "Epoch 41 \t Max val Acc = 0.4583333432674408, 0.6708860993385315\n",
            "Epoch 42 \t Max val Acc = 0.46315789222717285, 0.6772152185440063\n",
            "Epoch 43 \t Max val Acc = 0.4842105507850647, 0.6898733973503113\n",
            "Epoch 46 \t Max val Acc = 0.5208333134651184, 0.7088607549667358\n",
            "Epoch =  1000 \t Train loss = 0.06845 \t  Train Acc = 0.96979 \t Val loss = 2.88354 \t Acc = 0.452380.351850.39583 \t Patience = 1046\n",
            "Epoch =  2000 \t Train loss = 0.07291 \t  Train Acc = 0.97933 \t Val loss = 3.79174 \t Acc = 0.431820.351850.38776 \t Patience = 46\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.78073 \t  Train Acc = 0.52305 \t Val loss = 0.71159 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 3 \t Max val Acc = 0.07017543911933899, 0.6645569801330566\n",
            "Epoch 4 \t Max val Acc = 0.1355932205915451, 0.6772152185440063\n",
            "Epoch 5 \t Max val Acc = 0.1666666716337204, 0.6835442781448364\n",
            "Epoch 6 \t Max val Acc = 0.19672131538391113, 0.6898733973503113\n",
            "Epoch 8 \t Max val Acc = 0.23880599439144135, 0.6772152185440063\n",
            "Epoch 9 \t Max val Acc = 0.30985915660858154, 0.6898733973503113\n",
            "Epoch 11 \t Max val Acc = 0.320000022649765, 0.6772152185440063\n",
            "Epoch 12 \t Max val Acc = 0.4000000059604645, 0.6962025165557861\n",
            "Epoch 13 \t Max val Acc = 0.4285714328289032, 0.6962025165557861\n",
            "Epoch 16 \t Max val Acc = 0.4597700834274292, 0.702531635761261\n",
            "Epoch 17 \t Max val Acc = 0.47191011905670166, 0.702531635761261\n",
            "Epoch 18 \t Max val Acc = 0.48351648449897766, 0.702531635761261\n",
            "Epoch 19 \t Max val Acc = 0.5510203838348389, 0.7215189933776855\n",
            "Epoch 33 \t Max val Acc = 0.5546218156814575, 0.6645569801330566\n",
            "Epoch 34 \t Max val Acc = 0.5666666626930237, 0.6708860993385315\n",
            "Epoch 42 \t Max val Acc = 0.5759999752044678, 0.6645569801330566\n",
            "Epoch 50 \t Max val Acc = 0.5785123705863953, 0.6772152185440063\n",
            "Epoch 51 \t Max val Acc = 0.5882352590560913, 0.6898733973503113\n",
            "Epoch 52 \t Max val Acc = 0.5932203531265259, 0.6962025165557861\n",
            "Epoch 53 \t Max val Acc = 0.5950413346290588, 0.6898733973503113\n",
            "Epoch =  1000 \t Train loss = 0.05538 \t  Train Acc = 0.97774 \t Val loss = 3.66113 \t Acc = 0.391300.166670.23377 \t Patience = 1053\n",
            "Epoch =  2000 \t Train loss = 0.03369 \t  Train Acc = 0.98728 \t Val loss = 4.80553 \t Acc = 0.400000.148150.21622 \t Patience = 53\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.3030302822589874, 0.5632911324501038\n",
            "Epoch =  0 \t Train loss = 0.74432 \t  Train Acc = 0.55008 \t Val loss = 0.69029 \t Acc = 0.333330.277780.30303 \t Patience = 2000\n",
            "Epoch 4 \t Max val Acc = 0.3076923191547394, 0.5443037748336792\n",
            "Epoch 6 \t Max val Acc = 0.3300970792770386, 0.5632911324501038\n",
            "Epoch 7 \t Max val Acc = 0.3333333432674408, 0.5696202516555786\n",
            "Epoch 8 \t Max val Acc = 0.3529411554336548, 0.5822784900665283\n",
            "Epoch 10 \t Max val Acc = 0.3689320385456085, 0.5886076092720032\n",
            "Epoch 11 \t Max val Acc = 0.3883495032787323, 0.6012658476829529\n",
            "Epoch 12 \t Max val Acc = 0.39603960514068604, 0.6139240264892578\n",
            "Epoch 13 \t Max val Acc = 0.4040403962135315, 0.6265822649002075\n",
            "Epoch 14 \t Max val Acc = 0.40816327929496765, 0.6329113841056824\n",
            "Epoch 16 \t Max val Acc = 0.42424243688583374, 0.6392405033111572\n",
            "Epoch 17 \t Max val Acc = 0.4285714626312256, 0.6455696225166321\n",
            "Epoch 18 \t Max val Acc = 0.4329896867275238, 0.6518987417221069\n",
            "Epoch 20 \t Max val Acc = 0.4444444477558136, 0.6518987417221069\n",
            "Epoch 21 \t Max val Acc = 0.4554455578327179, 0.6518987417221069\n",
            "Epoch 22 \t Max val Acc = 0.4660194516181946, 0.6518987417221069\n",
            "Epoch 31 \t Max val Acc = 0.49504950642585754, 0.6772152185440063\n",
            "Epoch 33 \t Max val Acc = 0.5098039507865906, 0.6835442781448364\n",
            "Epoch 34 \t Max val Acc = 0.529411792755127, 0.6962025165557861\n",
            "Epoch 48 \t Max val Acc = 0.5405405759811401, 0.6772152185440063\n",
            "Epoch 49 \t Max val Acc = 0.5454545617103577, 0.6835442781448364\n",
            "Epoch 50 \t Max val Acc = 0.5555555820465088, 0.6962025165557861\n",
            "Epoch =  1000 \t Train loss = 0.06363 \t  Train Acc = 0.98251 \t Val loss = 2.63325 \t Acc = 0.444440.296300.35556 \t Patience = 1050\n",
            "Epoch =  2000 \t Train loss = 0.02637 \t  Train Acc = 0.98728 \t Val loss = 3.36150 \t Acc = 0.447370.314810.36957 \t Patience = 50\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.5026177763938904, 0.3987341821193695\n",
            "Epoch =  0 \t Train loss = 0.78569 \t  Train Acc = 0.42925 \t Val loss = 0.69860 \t Acc = 0.350360.888890.50262 \t Patience = 2000\n",
            "Epoch 33 \t Max val Acc = 0.5196850299835205, 0.6139240264892578\n",
            "Epoch 34 \t Max val Acc = 0.546875, 0.6329113841056824\n",
            "Epoch 35 \t Max val Acc = 0.5511810779571533, 0.6392405033111572\n",
            "Epoch 36 \t Max val Acc = 0.555555522441864, 0.6455696225166321\n",
            "Epoch 50 \t Max val Acc = 0.5641025900840759, 0.6772152185440063\n",
            "Epoch 51 \t Max val Acc = 0.573913037776947, 0.6898733973503113\n",
            "Epoch =  1000 \t Train loss = 0.06080 \t  Train Acc = 0.97615 \t Val loss = 2.62282 \t Acc = 0.500000.333330.40000 \t Patience = 1051\n",
            "Epoch =  2000 \t Train loss = 0.04032 \t  Train Acc = 0.98887 \t Val loss = 3.59479 \t Acc = 0.533330.296300.38095 \t Patience = 51\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.5086705088615417, 0.46202531456947327\n",
            "Epoch =  0 \t Train loss = 0.77007 \t  Train Acc = 0.50079 \t Val loss = 0.67233 \t Acc = 0.369750.814810.50867 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.5185185074806213, 0.5063291192054749\n",
            "Epoch 2 \t Max val Acc = 0.5263158082962036, 0.5443037748336792\n",
            "Epoch 4 \t Max val Acc = 0.5581395030021667, 0.6392405033111572\n",
            "Epoch =  1000 \t Train loss = 0.06604 \t  Train Acc = 0.97933 \t Val loss = 2.63393 \t Acc = 0.484850.296300.36782 \t Patience = 1004\n",
            "Epoch =  2000 \t Train loss = 0.06924 \t  Train Acc = 0.97615 \t Val loss = 3.37950 \t Acc = 0.571430.296300.39024 \t Patience = 4\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.82714 \t  Train Acc = 0.46741 \t Val loss = 0.70302 \t Acc = 0.000000.00000nan \t Patience = 1999\n",
            "Epoch 1 \t Max val Acc = 0.09677419811487198, 0.6455696225166321\n",
            "Epoch 2 \t Max val Acc = 0.2133333384990692, 0.6265822649002075\n",
            "Epoch 3 \t Max val Acc = 0.2916666567325592, 0.5696202516555786\n",
            "Epoch 4 \t Max val Acc = 0.34285715222358704, 0.5632911324501038\n",
            "Epoch 9 \t Max val Acc = 0.3486238718032837, 0.550632894039154\n",
            "Epoch 11 \t Max val Acc = 0.37168142199516296, 0.550632894039154\n",
            "Epoch 12 \t Max val Acc = 0.38532111048698425, 0.5759493708610535\n",
            "Epoch 13 \t Max val Acc = 0.4000000059604645, 0.5822784900665283\n",
            "Epoch 15 \t Max val Acc = 0.40366971492767334, 0.5886076092720032\n",
            "Epoch 16 \t Max val Acc = 0.4324324131011963, 0.6012658476829529\n",
            "Epoch 18 \t Max val Acc = 0.4403669834136963, 0.6139240264892578\n",
            "Epoch 19 \t Max val Acc = 0.4545454680919647, 0.6202531456947327\n",
            "Epoch 20 \t Max val Acc = 0.45871561765670776, 0.6265822649002075\n",
            "Epoch 21 \t Max val Acc = 0.4912280738353729, 0.6329113841056824\n",
            "Epoch 22 \t Max val Acc = 0.4955752193927765, 0.6392405033111572\n",
            "Epoch 23 \t Max val Acc = 0.5045045018196106, 0.6518987417221069\n",
            "Epoch 24 \t Max val Acc = 0.5090909600257874, 0.6582278609275818\n",
            "Epoch 28 \t Max val Acc = 0.5132743120193481, 0.6518987417221069\n",
            "Epoch 31 \t Max val Acc = 0.5263158082962036, 0.6582278609275818\n",
            "Epoch 43 \t Max val Acc = 0.5470085144042969, 0.6645569801330566\n",
            "Epoch 44 \t Max val Acc = 0.5517241358757019, 0.6708860993385315\n",
            "Epoch 46 \t Max val Acc = 0.5641025900840759, 0.6772152185440063\n",
            "Epoch 75 \t Max val Acc = 0.5666666626930237, 0.6708860993385315\n",
            "Epoch 79 \t Max val Acc = 0.5737705230712891, 0.6708860993385315\n",
            "Epoch =  1000 \t Train loss = 0.04988 \t  Train Acc = 0.98251 \t Val loss = 2.93783 \t Acc = 0.482760.259260.33735 \t Patience = 1079\n",
            "Epoch =  2000 \t Train loss = 0.03335 \t  Train Acc = 0.98728 \t Val loss = 4.14228 \t Acc = 0.481480.240740.32099 \t Patience = 79\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.48543688654899597, 0.6645569801330566\n",
            "Epoch =  0 \t Train loss = 0.77925 \t  Train Acc = 0.50079 \t Val loss = 0.68893 \t Acc = 0.510200.462960.48544 \t Patience = 2000\n",
            "Epoch 17 \t Max val Acc = 0.49056604504585266, 0.6582278609275818\n",
            "Epoch 20 \t Max val Acc = 0.5098039507865906, 0.6835442781448364\n",
            "Epoch 24 \t Max val Acc = 0.5148515105247498, 0.6898733973503113\n",
            "Epoch 39 \t Max val Acc = 0.529411792755127, 0.6962025165557861\n",
            "Epoch 41 \t Max val Acc = 0.5346534252166748, 0.702531635761261\n",
            "Epoch 49 \t Max val Acc = 0.5420560836791992, 0.6898733973503113\n",
            "Epoch =  1000 \t Train loss = 0.05439 \t  Train Acc = 0.97933 \t Val loss = 2.48392 \t Acc = 0.516130.296300.37647 \t Patience = 1049\n",
            "Epoch =  2000 \t Train loss = 0.05177 \t  Train Acc = 0.97456 \t Val loss = 3.22942 \t Acc = 0.575760.351850.43678 \t Patience = 49\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.5094339847564697, 0.3417721390724182\n",
            "Epoch =  0 \t Train loss = 0.80435 \t  Train Acc = 0.48490 \t Val loss = 0.67804 \t Acc = 0.341771.000000.50943 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.5167464017868042, 0.3607594966888428\n",
            "Epoch 2 \t Max val Acc = 0.5473684668540955, 0.4556961953639984\n",
            "Epoch =  1000 \t Train loss = 0.06900 \t  Train Acc = 0.97615 \t Val loss = 2.22648 \t Acc = 0.486490.333330.39560 \t Patience = 1002\n",
            "Epoch =  2000 \t Train loss = 0.05148 \t  Train Acc = 0.98410 \t Val loss = 3.18911 \t Acc = 0.468750.277780.34884 \t Patience = 2\n",
            "Training complete\n",
            "Max val losses________________\n",
            "[tensor(0.5983), tensor(0.5565), tensor(0.5472), tensor(0.5760), tensor(0.5347), tensor(0.5714), tensor(0.5234), tensor(0.5965), tensor(0.5902), tensor(0.5968), tensor(0.5920), tensor(0.5283), tensor(0.5208), tensor(0.5950), tensor(0.5556), tensor(0.5739), tensor(0.5581), tensor(0.5738), tensor(0.5421), tensor(0.5474)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sRPgoDmvrA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf3b68c-851e-4672-d656-b002f09bd24c"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i = 0\n",
        "pre_df = pd.DataFrame(index = autdata.index)\n",
        "oddsratio_df = pd.DataFrame(index = np.arange(0.0,1.05,0.05))\n",
        "fi_df = pd.DataFrame()\n",
        "combine = aut_exp_n\n",
        "label = pd.DataFrame(aut_risk_n, columns=[\"labels\"])\n",
        "max_val_accs = []\n",
        "aut_embed_state_dicts = []\n",
        "for train, test in cv.split(aut_exp_n, label):\n",
        "    inputs_train = torch.tensor(aut_exp_n[train], dtype=torch.float32, requires_grad=True)\n",
        "    inputs_val = torch.tensor(aut_exp_n[test], dtype=torch.float32, requires_grad=True)\n",
        "    labels_train = torch.tensor(aut_risk_n[train], dtype=torch.long)\n",
        "    labels_val = torch.tensor(aut_risk_n[test], dtype=torch.long)\n",
        "\n",
        "    for i in range(5):\n",
        "        aut_embed = CustomEmbedding(95, 60)\n",
        "        aut_cla = SimpleNN(aut_embed, 60, 2)\n",
        "        loss_func = nn.CrossEntropyLoss(torch.tensor([0.3,  0.7]))\n",
        "        optimizer = torch.optim.Adam(aut_cla.parameters())\n",
        "        model = aut_cla\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "        num_epochs = 100000\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        print_freq = 1000\n",
        "        patience = 2000\n",
        "        patience_2 = patience\n",
        "        max_val_acc = 0\n",
        "        aut_embed_state_dict = None\n",
        "        for epoch in range(num_epochs):\n",
        "            # train for one epoch, printing every 10 iterations\n",
        "            if patience==0:\n",
        "                print(\"Training complete\")\n",
        "                break\n",
        "            model.train()\n",
        "            train_loss, train_acc = train_one_epoch(model, inputs_train, labels_train, loss_func, optimizer, device, acc_func=calc_accuracy)\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "\n",
        "            # evaluate on the test dataset\n",
        "            model.eval()\n",
        "            val_loss, accuracy, precision, recall, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "            patience -= 1\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                patience =  patience_2\n",
        "                aut_embed_state_dict = aut_embed.state_dict().copy()\n",
        "                print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}, {accuracy}\")\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            if epoch%print_freq==0:    \n",
        "                print(f\"Epoch =  {epoch} \\t Train loss = {train_loss:.5f} \\t  Train Acc = {train_acc:.5f} \\t\",\n",
        "                f\"Val loss = {val_loss:.5f} \\t Acc = {precision:.5f}{recall:.5f}{val_acc:.5f} \\t Patience = {patience}\")\n",
        "        max_val_accs.append(max_val_acc)\n",
        "        aut_embed_state_dicts.append(aut_embed_state_dict)\n",
        "\n",
        "print(\"Max val losses________________\")\n",
        "print(max_val_accs)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t Max val Acc = 0.4651162624359131, 0.5632911324501038\n",
            "Epoch =  0 \t Train loss = 0.78558 \t  Train Acc = 0.48331 \t Val loss = 0.68657 \t Acc = 0.370370.625000.46512 \t Patience = 2000\n",
            "Epoch 4 \t Max val Acc = 0.5178571343421936, 0.6582278609275818\n",
            "Epoch 5 \t Max val Acc = 0.5471698045730591, 0.6962025165557861\n",
            "Epoch 6 \t Max val Acc = 0.557692289352417, 0.7088607549667358\n",
            "Epoch 160 \t Max val Acc = 0.5714285373687744, 0.7721518874168396\n",
            "Epoch 162 \t Max val Acc = 0.5783132910728455, 0.7784810066223145\n",
            "Epoch 164 \t Max val Acc = 0.5952380895614624, 0.7848101258277893\n",
            "Epoch =  1000 \t Train loss = 0.08693 \t  Train Acc = 0.97138 \t Val loss = 2.25698 \t Acc = 0.444440.250000.32000 \t Patience = 1164\n",
            "Epoch =  2000 \t Train loss = 0.05448 \t  Train Acc = 0.98410 \t Val loss = 2.96527 \t Acc = 0.482760.291670.36364 \t Patience = 164\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4660193920135498, 0.3037974536418915\n",
            "Epoch =  0 \t Train loss = 0.75394 \t  Train Acc = 0.44992 \t Val loss = 0.69110 \t Acc = 0.303801.000000.46602 \t Patience = 2000\n",
            "Epoch 5 \t Max val Acc = 0.4682926833629608, 0.31012657284736633\n",
            "Epoch 6 \t Max val Acc = 0.4729064404964447, 0.32278481125831604\n",
            "Epoch 11 \t Max val Acc = 0.4761904776096344, 0.3734177350997925\n",
            "Epoch 13 \t Max val Acc = 0.47999998927116394, 0.42405062913894653\n",
            "Epoch 14 \t Max val Acc = 0.5029940009117126, 0.474683552980423\n",
            "Epoch 15 \t Max val Acc = 0.5031446814537048, 0.5\n",
            "Epoch 16 \t Max val Acc = 0.5128205418586731, 0.5189873576164246\n",
            "Epoch 17 \t Max val Acc = 0.5194805264472961, 0.5316455960273743\n",
            "Epoch 20 \t Max val Acc = 0.5211267471313477, 0.5696202516555786\n",
            "Epoch 22 \t Max val Acc = 0.546875, 0.6329113841056824\n",
            "Epoch 23 \t Max val Acc = 0.5645161271095276, 0.6582278609275818\n",
            "Epoch 24 \t Max val Acc = 0.5882353186607361, 0.6898733973503113\n",
            "Epoch 25 \t Max val Acc = 0.6034482717514038, 0.7088607549667358\n",
            "Epoch 28 \t Max val Acc = 0.60550457239151, 0.7278481125831604\n",
            "Epoch 30 \t Max val Acc = 0.6095237731933594, 0.7405063509941101\n",
            "Epoch 48 \t Max val Acc = 0.6138613820075989, 0.753164529800415\n",
            "Epoch 87 \t Max val Acc = 0.6170213222503662, 0.7721518874168396\n",
            "Epoch 93 \t Max val Acc = 0.625, 0.7721518874168396\n",
            "Epoch 94 \t Max val Acc = 0.6315789818763733, 0.7784810066223145\n",
            "Epoch 99 \t Max val Acc = 0.6391752362251282, 0.7784810066223145\n",
            "Epoch =  1000 \t Train loss = 0.08677 \t  Train Acc = 0.97615 \t Val loss = 2.20904 \t Acc = 0.515150.354170.41975 \t Patience = 1099\n",
            "Epoch =  2000 \t Train loss = 0.06158 \t  Train Acc = 0.98251 \t Val loss = 3.19902 \t Acc = 0.548390.354170.43038 \t Patience = 99\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.03921568766236305, 0.6898733973503113\n",
            "Epoch =  0 \t Train loss = 0.78825 \t  Train Acc = 0.52941 \t Val loss = 0.69446 \t Acc = 0.333330.020830.03922 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.1454545557498932, 0.702531635761261\n",
            "Epoch 2 \t Max val Acc = 0.23728813230991364, 0.7151898741722107\n",
            "Epoch 3 \t Max val Acc = 0.2666666805744171, 0.7215189933776855\n",
            "Epoch 4 \t Max val Acc = 0.3692307770252228, 0.7405063509941101\n",
            "Epoch 5 \t Max val Acc = 0.4057970941066742, 0.7405063509941101\n",
            "Epoch 6 \t Max val Acc = 0.4675324559211731, 0.7405063509941101\n",
            "Epoch 7 \t Max val Acc = 0.4761905074119568, 0.7215189933776855\n",
            "Epoch 8 \t Max val Acc = 0.5054945349693298, 0.7151898741722107\n",
            "Epoch 10 \t Max val Acc = 0.5192307233810425, 0.6835442781448364\n",
            "Epoch 11 \t Max val Acc = 0.5225225687026978, 0.6645569801330566\n",
            "Epoch 12 \t Max val Acc = 0.5546218752861023, 0.6645569801330566\n",
            "Epoch 16 \t Max val Acc = 0.5555555820465088, 0.6455696225166321\n",
            "Epoch 18 \t Max val Acc = 0.5714285969734192, 0.6582278609275818\n",
            "Epoch 19 \t Max val Acc = 0.5760000348091125, 0.6645569801330566\n",
            "Epoch 22 \t Max val Acc = 0.5781249403953552, 0.6582278609275818\n",
            "Epoch 23 \t Max val Acc = 0.59375, 0.6708860993385315\n",
            "Epoch 61 \t Max val Acc = 0.5945945978164673, 0.7151898741722107\n",
            "Epoch 62 \t Max val Acc = 0.6168224215507507, 0.7405063509941101\n",
            "Epoch =  1000 \t Train loss = 0.07817 \t  Train Acc = 0.96979 \t Val loss = 2.82670 \t Acc = 0.562500.187500.28125 \t Patience = 1062\n",
            "Epoch =  2000 \t Train loss = 0.06291 \t  Train Acc = 0.98251 \t Val loss = 3.91164 \t Acc = 0.700000.145830.24138 \t Patience = 62\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.82728 \t  Train Acc = 0.50397 \t Val loss = 0.71797 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 4 \t Max val Acc = 0.040816325694322586, 0.702531635761261\n",
            "Epoch 5 \t Max val Acc = 0.0784313753247261, 0.702531635761261\n",
            "Epoch 6 \t Max val Acc = 0.178571417927742, 0.7088607549667358\n",
            "Epoch 7 \t Max val Acc = 0.2539682686328888, 0.702531635761261\n",
            "Epoch 8 \t Max val Acc = 0.3235293924808502, 0.7088607549667358\n",
            "Epoch 9 \t Max val Acc = 0.3661971986293793, 0.7151898741722107\n",
            "Epoch 10 \t Max val Acc = 0.44155845046043396, 0.7278481125831604\n",
            "Epoch 11 \t Max val Acc = 0.44999998807907104, 0.7215189933776855\n",
            "Epoch 12 \t Max val Acc = 0.45783132314682007, 0.7151898741722107\n",
            "Epoch 13 \t Max val Acc = 0.47058820724487305, 0.7151898741722107\n",
            "Epoch 17 \t Max val Acc = 0.4848484992980957, 0.6772152185440063\n",
            "Epoch 23 \t Max val Acc = 0.4859813451766968, 0.6518987417221069\n",
            "Epoch 24 \t Max val Acc = 0.5, 0.6708860993385315\n",
            "Epoch 25 \t Max val Acc = 0.5199999809265137, 0.6962025165557861\n",
            "Epoch 26 \t Max val Acc = 0.5208333134651184, 0.7088607549667358\n",
            "Epoch 27 \t Max val Acc = 0.5376344323158264, 0.7278481125831604\n",
            "Epoch 28 \t Max val Acc = 0.5434781908988953, 0.7341772317886353\n",
            "Epoch 29 \t Max val Acc = 0.5531915426254272, 0.7341772317886353\n",
            "Epoch 40 \t Max val Acc = 0.559999942779541, 0.7215189933776855\n",
            "Epoch 42 \t Max val Acc = 0.582524299621582, 0.7278481125831604\n",
            "Epoch 44 \t Max val Acc = 0.5961538553237915, 0.7341772317886353\n",
            "Epoch 46 \t Max val Acc = 0.6095237731933594, 0.7405063509941101\n",
            "Epoch =  1000 \t Train loss = 0.07730 \t  Train Acc = 0.97138 \t Val loss = 2.59808 \t Acc = 0.468750.312500.37500 \t Patience = 1046\n",
            "Epoch =  2000 \t Train loss = 0.05111 \t  Train Acc = 0.97933 \t Val loss = 3.40634 \t Acc = 0.636360.291670.40000 \t Patience = 46\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.76067 \t  Train Acc = 0.51192 \t Val loss = 0.69715 \t Acc = 0.000000.00000nan \t Patience = 1999\n",
            "Epoch 1 \t Max val Acc = 0.07407408207654953, 0.6835442781448364\n",
            "Epoch 2 \t Max val Acc = 0.1034482792019844, 0.6708860993385315\n",
            "Epoch 3 \t Max val Acc = 0.19672131538391113, 0.6898733973503113\n",
            "Epoch 4 \t Max val Acc = 0.2222222238779068, 0.6898733973503113\n",
            "Epoch 6 \t Max val Acc = 0.23880596458911896, 0.6772152185440063\n",
            "Epoch 7 \t Max val Acc = 0.2608695328235626, 0.6772152185440063\n",
            "Epoch 8 \t Max val Acc = 0.30985915660858154, 0.6898733973503113\n",
            "Epoch 9 \t Max val Acc = 0.3513513505458832, 0.6962025165557861\n",
            "Epoch 10 \t Max val Acc = 0.4000000059604645, 0.7151898741722107\n",
            "Epoch 17 \t Max val Acc = 0.4337349236011505, 0.702531635761261\n",
            "Epoch 18 \t Max val Acc = 0.4651162922382355, 0.7088607549667358\n",
            "Epoch 19 \t Max val Acc = 0.4943820536136627, 0.7151898741722107\n",
            "Epoch 20 \t Max val Acc = 0.5111110806465149, 0.7215189933776855\n",
            "Epoch 23 \t Max val Acc = 0.5168538689613342, 0.7278481125831604\n",
            "Epoch 26 \t Max val Acc = 0.5227272510528564, 0.7341772317886353\n",
            "Epoch 27 \t Max val Acc = 0.5393258333206177, 0.7405063509941101\n",
            "Epoch 40 \t Max val Acc = 0.5454545617103577, 0.746835470199585\n",
            "Epoch 41 \t Max val Acc = 0.5517241954803467, 0.753164529800415\n",
            "Epoch 44 \t Max val Acc = 0.5681818127632141, 0.7594936490058899\n",
            "Epoch 77 \t Max val Acc = 0.5714285969734192, 0.753164529800415\n",
            "Epoch 78 \t Max val Acc = 0.5777778029441833, 0.7594936490058899\n",
            "Epoch 92 \t Max val Acc = 0.6000000238418579, 0.7721518874168396\n",
            "Epoch 95 \t Max val Acc = 0.602150559425354, 0.7658227682113647\n",
            "Epoch 104 \t Max val Acc = 0.6105263233184814, 0.7658227682113647\n",
            "Epoch 155 \t Max val Acc = 0.6279070377349854, 0.797468364238739\n",
            "Epoch 158 \t Max val Acc = 0.6352941393852234, 0.8037974834442139\n",
            "Epoch =  1000 \t Train loss = 0.10655 \t  Train Acc = 0.96661 \t Val loss = 2.36683 \t Acc = 0.629630.354170.45333 \t Patience = 1158\n",
            "Epoch =  2000 \t Train loss = 0.05081 \t  Train Acc = 0.97456 \t Val loss = 3.00055 \t Acc = 0.550000.458330.50000 \t Patience = 158\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.40816327929496765, 0.44936707615852356\n",
            "Epoch =  0 \t Train loss = 0.74478 \t  Train Acc = 0.52305 \t Val loss = 0.69755 \t Acc = 0.306120.612240.40816 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4225352108478546, 0.4810126721858978\n",
            "Epoch 2 \t Max val Acc = 0.431654691696167, 0.5\n",
            "Epoch 3 \t Max val Acc = 0.4328358471393585, 0.5189873576164246\n",
            "Epoch 4 \t Max val Acc = 0.4761904776096344, 0.5822784900665283\n",
            "Epoch 5 \t Max val Acc = 0.484375, 0.5822784900665283\n",
            "Epoch 9 \t Max val Acc = 0.4955752491950989, 0.6392405033111572\n",
            "Epoch 10 \t Max val Acc = 0.5272727608680725, 0.6708860993385315\n",
            "Epoch 12 \t Max val Acc = 0.5370370745658875, 0.6835442781448364\n",
            "Epoch 36 \t Max val Acc = 0.5593219995498657, 0.6708860993385315\n",
            "Epoch 41 \t Max val Acc = 0.5666666626930237, 0.6708860993385315\n",
            "Epoch 44 \t Max val Acc = 0.5714285373687744, 0.6772152185440063\n",
            "Epoch =  1000 \t Train loss = 0.05875 \t  Train Acc = 0.98092 \t Val loss = 2.37333 \t Acc = 0.510200.510200.51020 \t Patience = 1044\n",
            "Epoch 1346 \t Max val Acc = 0.574257493019104, 0.7278481125831604\n",
            "Epoch 1658 \t Max val Acc = 0.5833333730697632, 0.746835470199585\n",
            "Epoch =  2000 \t Train loss = 0.04158 \t  Train Acc = 0.98887 \t Val loss = 3.12273 \t Acc = 0.448280.530610.48598 \t Patience = 1658\n",
            "Epoch =  3000 \t Train loss = 0.06199 \t  Train Acc = 0.98569 \t Val loss = 4.01187 \t Acc = 0.479170.469390.47423 \t Patience = 658\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4444444477558136, 0.33544304966926575\n",
            "Epoch =  0 \t Train loss = 0.74508 \t  Train Acc = 0.50397 \t Val loss = 0.68807 \t Acc = 0.300000.857140.44444 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.45989301800727844, 0.3607594966888428\n",
            "Epoch 2 \t Max val Acc = 0.46994537115097046, 0.3860759437084198\n",
            "Epoch 3 \t Max val Acc = 0.4804469048976898, 0.4113923907279968\n",
            "Epoch 5 \t Max val Acc = 0.4910179674625397, 0.46202531456947327\n",
            "Epoch 6 \t Max val Acc = 0.4968944191932678, 0.4873417615890503\n",
            "Epoch 8 \t Max val Acc = 0.5076922178268433, 0.594936728477478\n",
            "Epoch 10 \t Max val Acc = 0.5081967115402222, 0.6202531456947327\n",
            "Epoch 24 \t Max val Acc = 0.514851450920105, 0.6898733973503113\n",
            "Epoch 27 \t Max val Acc = 0.5192307829856873, 0.6835442781448364\n",
            "Epoch 31 \t Max val Acc = 0.5384616255760193, 0.6962025165557861\n",
            "Epoch 33 \t Max val Acc = 0.5420560836791992, 0.6898733973503113\n",
            "Epoch 35 \t Max val Acc = 0.5454545617103577, 0.6835442781448364\n",
            "Epoch 63 \t Max val Acc = 0.5470085144042969, 0.6645569801330566\n",
            "Epoch 77 \t Max val Acc = 0.5517241358757019, 0.6708860993385315\n",
            "Epoch 90 \t Max val Acc = 0.5641025900840759, 0.6772152185440063\n",
            "Epoch 102 \t Max val Acc = 0.5663716793060303, 0.6898733973503113\n",
            "Epoch =  1000 \t Train loss = 0.05815 \t  Train Acc = 0.97774 \t Val loss = 2.88303 \t Acc = 0.458330.448980.45361 \t Patience = 1102\n",
            "Epoch =  2000 \t Train loss = 0.06038 \t  Train Acc = 0.97774 \t Val loss = 3.94276 \t Acc = 0.500000.510200.50505 \t Patience = 102\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.47342994809150696, 0.31012657284736633\n",
            "Epoch =  0 \t Train loss = 0.74616 \t  Train Acc = 0.49603 \t Val loss = 0.69003 \t Acc = 0.310131.000000.47343 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4757281541824341, 0.3164556920528412\n",
            "Epoch 2 \t Max val Acc = 0.47804880142211914, 0.32278481125831604\n",
            "Epoch 3 \t Max val Acc = 0.4875621795654297, 0.34810125827789307\n",
            "Epoch 4 \t Max val Acc = 0.5025640726089478, 0.3860759437084198\n",
            "Epoch 5 \t Max val Acc = 0.5240641832351685, 0.43670886754989624\n",
            "Epoch 11 \t Max val Acc = 0.5413533449172974, 0.6139240264892578\n",
            "Epoch 24 \t Max val Acc = 0.5517241358757019, 0.6708860993385315\n",
            "Epoch 28 \t Max val Acc = 0.5614035129547119, 0.6835442781448364\n",
            "Epoch 42 \t Max val Acc = 0.5660377144813538, 0.7088607549667358\n",
            "Epoch 51 \t Max val Acc = 0.5794392228126526, 0.7151898741722107\n",
            "Epoch 56 \t Max val Acc = 0.5871559381484985, 0.7151898741722107\n",
            "Epoch 92 \t Max val Acc = 0.5981308221817017, 0.7278481125831604\n",
            "Epoch =  1000 \t Train loss = 0.06668 \t  Train Acc = 0.98092 \t Val loss = 2.23349 \t Acc = 0.540000.551020.54545 \t Patience = 1092\n",
            "Epoch 1860 \t Max val Acc = 0.6041666865348816, 0.7594936490058899\n",
            "Epoch =  2000 \t Train loss = 0.07754 \t  Train Acc = 0.97297 \t Val loss = 3.27630 \t Acc = 0.586960.551020.56842 \t Patience = 1860\n",
            "Epoch =  3000 \t Train loss = 0.03614 \t  Train Acc = 0.99046 \t Val loss = 3.72374 \t Acc = 0.555560.510200.53191 \t Patience = 860\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.46451613306999207, 0.474683552980423\n",
            "Epoch =  0 \t Train loss = 0.72531 \t  Train Acc = 0.57075 \t Val loss = 0.68976 \t Acc = 0.339620.734690.46452 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4933333098888397, 0.5189873576164246\n",
            "Epoch 2 \t Max val Acc = 0.529411792755127, 0.594936728477478\n",
            "Epoch 3 \t Max val Acc = 0.5737705230712891, 0.6708860993385315\n",
            "Epoch 38 \t Max val Acc = 0.5740740299224854, 0.7088607549667358\n",
            "Epoch 39 \t Max val Acc = 0.6486486196517944, 0.753164529800415\n",
            "Epoch =  1000 \t Train loss = 0.06343 \t  Train Acc = 0.97933 \t Val loss = 2.63935 \t Acc = 0.482140.551020.51429 \t Patience = 1039\n",
            "Epoch =  2000 \t Train loss = 0.05609 \t  Train Acc = 0.96820 \t Val loss = 3.47112 \t Acc = 0.500000.551020.52427 \t Patience = 39\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.47342994809150696, 0.31012657284736633\n",
            "Epoch =  0 \t Train loss = 0.80397 \t  Train Acc = 0.42289 \t Val loss = 0.69961 \t Acc = 0.310131.000000.47343 \t Patience = 2000\n",
            "Epoch 4 \t Max val Acc = 0.4757281541824341, 0.3164556920528412\n",
            "Epoch 6 \t Max val Acc = 0.47999998927116394, 0.3417721390724182\n",
            "Epoch 7 \t Max val Acc = 0.49723759293556213, 0.42405062913894653\n",
            "Epoch 10 \t Max val Acc = 0.5066666603088379, 0.5316455960273743\n",
            "Epoch 11 \t Max val Acc = 0.5135135054588318, 0.5443037748336792\n",
            "Epoch 12 \t Max val Acc = 0.5241379141807556, 0.5632911324501038\n",
            "Epoch 14 \t Max val Acc = 0.5285714268684387, 0.5822784900665283\n",
            "Epoch 15 \t Max val Acc = 0.5333333015441895, 0.6012658476829529\n",
            "Epoch 16 \t Max val Acc = 0.5343511700630188, 0.6139240264892578\n",
            "Epoch 17 \t Max val Acc = 0.5396825671195984, 0.6329113841056824\n",
            "Epoch 31 \t Max val Acc = 0.5454545617103577, 0.6518987417221069\n",
            "Epoch 60 \t Max val Acc = 0.5573770999908447, 0.6582278609275818\n",
            "Epoch 66 \t Max val Acc = 0.5645161271095276, 0.6582278609275818\n",
            "Epoch =  1000 \t Train loss = 0.05961 \t  Train Acc = 0.98251 \t Val loss = 2.75226 \t Acc = 0.470590.489800.48000 \t Patience = 1066\n",
            "Epoch =  2000 \t Train loss = 0.03731 \t  Train Acc = 0.98410 \t Val loss = 3.57917 \t Acc = 0.480770.510200.49505 \t Patience = 66\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4971098303794861, 0.4458598792552948\n",
            "Epoch =  0 \t Train loss = 0.76380 \t  Train Acc = 0.49206 \t Val loss = 0.68716 \t Acc = 0.344000.895830.49711 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.5135135650634766, 0.5414012670516968\n",
            "Epoch 2 \t Max val Acc = 0.5344827175140381, 0.656050980091095\n",
            "Epoch 57 \t Max val Acc = 0.5510204434394836, 0.7197452187538147\n",
            "Epoch 58 \t Max val Acc = 0.5567010641098022, 0.7261146306991577\n",
            "Epoch 91 \t Max val Acc = 0.5625, 0.7324841022491455\n",
            "Epoch =  1000 \t Train loss = 0.07337 \t  Train Acc = 0.97619 \t Val loss = 2.02906 \t Acc = 0.444440.416670.43011 \t Patience = 1091\n",
            "Epoch =  2000 \t Train loss = 0.06697 \t  Train Acc = 0.97937 \t Val loss = 2.81804 \t Acc = 0.408160.416670.41237 \t Patience = 91\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.32203391194343567, 0.4904458522796631\n",
            "Epoch =  0 \t Train loss = 0.80591 \t  Train Acc = 0.46984 \t Val loss = 0.69413 \t Acc = 0.271430.395830.32203 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.34146344661712646, 0.48407644033432007\n",
            "Epoch 4 \t Max val Acc = 0.4310344457626343, 0.5796178579330444\n",
            "Epoch 38 \t Max val Acc = 0.4565217196941376, 0.6815286874771118\n",
            "Epoch 40 \t Max val Acc = 0.4680851101875305, 0.6815286874771118\n",
            "Epoch 46 \t Max val Acc = 0.47311827540397644, 0.6878980994224548\n",
            "Epoch 68 \t Max val Acc = 0.4791666567325592, 0.6815286874771118\n",
            "Epoch 69 \t Max val Acc = 0.4842104911804199, 0.6878980994224548\n",
            "Epoch 79 \t Max val Acc = 0.49484536051750183, 0.6878980994224548\n",
            "Epoch 80 \t Max val Acc = 0.5102040767669678, 0.6942675113677979\n",
            "Epoch 81 \t Max val Acc = 0.5154639482498169, 0.7006369233131409\n",
            "Epoch 98 \t Max val Acc = 0.5208333134651184, 0.7070063948631287\n",
            "Epoch 102 \t Max val Acc = 0.5454545617103577, 0.7133758068084717\n",
            "Epoch =  1000 \t Train loss = 0.04955 \t  Train Acc = 0.99206 \t Val loss = 2.36581 \t Acc = 0.450980.479170.46465 \t Patience = 1102\n",
            "Epoch =  2000 \t Train loss = 0.06297 \t  Train Acc = 0.98254 \t Val loss = 3.08308 \t Acc = 0.500000.520830.51020 \t Patience = 102\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4482758343219757, 0.3885350227355957\n",
            "Epoch =  0 \t Train loss = 0.80953 \t  Train Acc = 0.44603 \t Val loss = 0.69446 \t Acc = 0.309520.812500.44828 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4615384638309479, 0.3757961690425873\n",
            "Epoch 2 \t Max val Acc = 0.4845360517501831, 0.36305731534957886\n",
            "Epoch 11 \t Max val Acc = 0.5, 0.47770699858665466\n",
            "Epoch 16 \t Max val Acc = 0.5228758454322815, 0.5350318551063538\n",
            "Epoch 47 \t Max val Acc = 0.5299144983291626, 0.6496815085411072\n",
            "Epoch 50 \t Max val Acc = 0.5405405759811401, 0.675159215927124\n",
            "Epoch 51 \t Max val Acc = 0.5565217137336731, 0.675159215927124\n",
            "Epoch 152 \t Max val Acc = 0.5591397881507874, 0.7388535141944885\n",
            "Epoch 282 \t Max val Acc = 0.559999942779541, 0.7197452187538147\n",
            "Epoch 395 \t Max val Acc = 0.5744680762290955, 0.7452229261398315\n",
            "Epoch 396 \t Max val Acc = 0.5894736647605896, 0.7515923380851746\n",
            "Epoch =  1000 \t Train loss = 0.09100 \t  Train Acc = 0.97302 \t Val loss = 2.47734 \t Acc = 0.450000.375000.40909 \t Patience = 1396\n",
            "Epoch =  2000 \t Train loss = 0.03211 \t  Train Acc = 0.98095 \t Val loss = 3.31803 \t Acc = 0.510640.500000.50526 \t Patience = 396\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4682926833629608, 0.30573248863220215\n",
            "Epoch =  0 \t Train loss = 0.76033 \t  Train Acc = 0.46508 \t Val loss = 0.69815 \t Acc = 0.305731.000000.46829 \t Patience = 2000\n",
            "Epoch 3 \t Max val Acc = 0.4776119589805603, 0.331210196018219\n",
            "Epoch 4 \t Max val Acc = 0.48730963468551636, 0.35668790340423584\n",
            "Epoch 5 \t Max val Acc = 0.5164835453033447, 0.4394904375076294\n",
            "Epoch 6 \t Max val Acc = 0.5283018946647644, 0.522292971611023\n",
            "Epoch 8 \t Max val Acc = 0.5467625856399536, 0.5987260937690735\n",
            "Epoch 9 \t Max val Acc = 0.5563910007476807, 0.6242038011550903\n",
            "Epoch 10 \t Max val Acc = 0.5648855566978455, 0.6369426846504211\n",
            "Epoch =  1000 \t Train loss = 0.04566 \t  Train Acc = 0.98571 \t Val loss = 2.43299 \t Acc = 0.527780.395830.45238 \t Patience = 1010\n",
            "Epoch =  2000 \t Train loss = 0.04177 \t  Train Acc = 0.98571 \t Val loss = 3.48229 \t Acc = 0.516130.333330.40506 \t Patience = 10\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.49462369084358215, 0.4012738764286041\n",
            "Epoch =  0 \t Train loss = 0.72853 \t  Train Acc = 0.47143 \t Val loss = 0.69158 \t Acc = 0.333330.958330.49462 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.5061728358268738, 0.4904458522796631\n",
            "Epoch 4 \t Max val Acc = 0.5112782120704651, 0.5859872698783875\n",
            "Epoch 12 \t Max val Acc = 0.5142857432365417, 0.675159215927124\n",
            "Epoch 58 \t Max val Acc = 0.5154639482498169, 0.7006369233131409\n",
            "Epoch 77 \t Max val Acc = 0.5208333134651184, 0.7070063948631287\n",
            "Epoch 78 \t Max val Acc = 0.5252525806427002, 0.7006369233131409\n",
            "Epoch 190 \t Max val Acc = 0.5287356376647949, 0.7388535141944885\n",
            "Epoch =  1000 \t Train loss = 0.06038 \t  Train Acc = 0.97778 \t Val loss = 2.28767 \t Acc = 0.466670.437500.45161 \t Patience = 1190\n",
            "Epoch =  2000 \t Train loss = 0.06810 \t  Train Acc = 0.97302 \t Val loss = 2.93441 \t Acc = 0.465120.416670.43956 \t Patience = 190\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.75958 \t  Train Acc = 0.49683 \t Val loss = 0.70526 \t Acc = 0.000000.00000nan \t Patience = 1999\n",
            "Epoch 1 \t Max val Acc = 0.13333334028720856, 0.668789803981781\n",
            "Epoch 2 \t Max val Acc = 0.23684212565422058, 0.6305732727050781\n",
            "Epoch 3 \t Max val Acc = 0.3177569806575775, 0.5350318551063538\n",
            "Epoch 4 \t Max val Acc = 0.4580152630805969, 0.5477706789970398\n",
            "Epoch 5 \t Max val Acc = 0.4637681245803833, 0.5286624431610107\n",
            "Epoch 6 \t Max val Acc = 0.4868420958518982, 0.5031847357749939\n",
            "Epoch 9 \t Max val Acc = 0.4907975494861603, 0.47133758664131165\n",
            "Epoch 22 \t Max val Acc = 0.4999999403953552, 0.6050955653190613\n",
            "Epoch 29 \t Max val Acc = 0.5040650367736816, 0.6114649772644043\n",
            "Epoch 32 \t Max val Acc = 0.5081967115402222, 0.6178343892097473\n",
            "Epoch 42 \t Max val Acc = 0.5084745287895203, 0.6305732727050781\n",
            "Epoch 45 \t Max val Acc = 0.5128204822540283, 0.6369426846504211\n",
            "Epoch 53 \t Max val Acc = 0.5132743716239929, 0.6496815085411072\n",
            "Epoch 54 \t Max val Acc = 0.5263157486915588, 0.656050980091095\n",
            "Epoch 143 \t Max val Acc = 0.5309734344482422, 0.662420392036438\n",
            "Epoch 144 \t Max val Acc = 0.5357142686843872, 0.668789803981781\n",
            "Epoch 150 \t Max val Acc = 0.5504586696624756, 0.6878980994224548\n",
            "Epoch 151 \t Max val Acc = 0.5607476830482483, 0.7006369233131409\n",
            "Epoch =  1000 \t Train loss = 0.10379 \t  Train Acc = 0.96349 \t Val loss = 3.31862 \t Acc = 0.409090.187500.25714 \t Patience = 1151\n",
            "Epoch =  2000 \t Train loss = 0.04658 \t  Train Acc = 0.98571 \t Val loss = 4.21617 \t Acc = 0.433330.270830.33333 \t Patience = 151\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.33112579584121704, 0.35668790340423584\n",
            "Epoch =  0 \t Train loss = 0.75323 \t  Train Acc = 0.51905 \t Val loss = 0.70598 \t Acc = 0.242720.520830.33113 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.36708858609199524, 0.36305731534957886\n",
            "Epoch 2 \t Max val Acc = 0.3855421841144562, 0.35031846165657043\n",
            "Epoch 4 \t Max val Acc = 0.4311377704143524, 0.3949044644832611\n",
            "Epoch 5 \t Max val Acc = 0.44970420002937317, 0.40764331817626953\n",
            "Epoch 7 \t Max val Acc = 0.4512195289134979, 0.42675158381462097\n",
            "Epoch 8 \t Max val Acc = 0.4675324857234955, 0.47770699858665466\n",
            "Epoch 9 \t Max val Acc = 0.4822694957256317, 0.5350318551063538\n",
            "Epoch 10 \t Max val Acc = 0.5039370059967041, 0.5987260937690735\n",
            "Epoch 17 \t Max val Acc = 0.5042017102241516, 0.6242038011550903\n",
            "Epoch 18 \t Max val Acc = 0.5166666507720947, 0.6305732727050781\n",
            "Epoch 22 \t Max val Acc = 0.5178571343421936, 0.656050980091095\n",
            "Epoch 23 \t Max val Acc = 0.5225225687026978, 0.662420392036438\n",
            "Epoch 25 \t Max val Acc = 0.5357142686843872, 0.668789803981781\n",
            "Epoch 26 \t Max val Acc = 0.5405405759811401, 0.675159215927124\n",
            "Epoch 27 \t Max val Acc = 0.5535714030265808, 0.6815286874771118\n",
            "Epoch =  1000 \t Train loss = 0.08057 \t  Train Acc = 0.96984 \t Val loss = 2.22936 \t Acc = 0.538460.437500.48276 \t Patience = 1027\n",
            "Epoch =  2000 \t Train loss = 0.04454 \t  Train Acc = 0.98095 \t Val loss = 3.01975 \t Acc = 0.526320.416670.46512 \t Patience = 27\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.46464648842811584, 0.3248407542705536\n",
            "Epoch =  0 \t Train loss = 0.77808 \t  Train Acc = 0.49524 \t Val loss = 0.68991 \t Acc = 0.306670.958330.46465 \t Patience = 2000\n",
            "Epoch 11 \t Max val Acc = 0.47311827540397644, 0.6878980994224548\n",
            "Epoch 17 \t Max val Acc = 0.47826087474823, 0.6942675113677979\n",
            "Epoch 18 \t Max val Acc = 0.5052632093429565, 0.7006369233131409\n",
            "Epoch 33 \t Max val Acc = 0.5376344323158264, 0.7261146306991577\n",
            "Epoch 51 \t Max val Acc = 0.5471698045730591, 0.6942675113677979\n",
            "Epoch 52 \t Max val Acc = 0.5607476830482483, 0.7006369233131409\n",
            "Epoch 60 \t Max val Acc = 0.5688073039054871, 0.7006369233131409\n",
            "Epoch 141 \t Max val Acc = 0.5714285373687744, 0.7133758068084717\n",
            "Epoch 142 \t Max val Acc = 0.582524299621582, 0.7261146306991577\n",
            "Epoch 143 \t Max val Acc = 0.5961538553237915, 0.7324841022491455\n",
            "Epoch 824 \t Max val Acc = 0.5999999642372131, 0.7452229261398315\n",
            "Epoch 868 \t Max val Acc = 0.6185567378997803, 0.7643312215805054\n",
            "Epoch =  1000 \t Train loss = 0.08419 \t  Train Acc = 0.97460 \t Val loss = 2.00469 \t Acc = 0.574470.562500.56842 \t Patience = 1868\n",
            "Epoch =  2000 \t Train loss = 0.05932 \t  Train Acc = 0.98254 \t Val loss = 2.56435 \t Acc = 0.517240.625000.56604 \t Patience = 868\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.42500001192092896, 0.41401273012161255\n",
            "Epoch =  0 \t Train loss = 0.73273 \t  Train Acc = 0.51270 \t Val loss = 0.68688 \t Acc = 0.303570.708330.42500 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4324324429035187, 0.46496814489364624\n",
            "Epoch 15 \t Max val Acc = 0.46000000834465027, 0.656050980091095\n",
            "Epoch 16 \t Max val Acc = 0.48543691635131836, 0.662420392036438\n",
            "Epoch 18 \t Max val Acc = 0.49056604504585266, 0.656050980091095\n",
            "Epoch 19 \t Max val Acc = 0.5137614607810974, 0.662420392036438\n",
            "Epoch 84 \t Max val Acc = 0.5225225687026978, 0.662420392036438\n",
            "Epoch 86 \t Max val Acc = 0.5309734344482422, 0.662420392036438\n",
            "Epoch 596 \t Max val Acc = 0.5319148898124695, 0.7197452187538147\n",
            "Epoch =  1000 \t Train loss = 0.07664 \t  Train Acc = 0.96190 \t Val loss = 2.54530 \t Acc = 0.500000.416670.45455 \t Patience = 1596\n",
            "Epoch 1116 \t Max val Acc = 0.5434781908988953, 0.7324841022491455\n",
            "Epoch 1132 \t Max val Acc = 0.5531915426254272, 0.7324841022491455\n",
            "Epoch =  2000 \t Train loss = 0.06205 \t  Train Acc = 0.97302 \t Val loss = 3.64937 \t Acc = 0.434780.416670.42553 \t Patience = 1132\n",
            "Epoch =  3000 \t Train loss = 0.03321 \t  Train Acc = 0.98413 \t Val loss = 4.04886 \t Acc = 0.521740.500000.51064 \t Patience = 132\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.43877553939819336, 0.29936304688453674\n",
            "Epoch =  0 \t Train loss = 0.73056 \t  Train Acc = 0.47937 \t Val loss = 0.69572 \t Acc = 0.290540.895830.43878 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4479166269302368, 0.3248407542705536\n",
            "Epoch 11 \t Max val Acc = 0.46052631735801697, 0.47770699858665466\n",
            "Epoch 12 \t Max val Acc = 0.49673205614089966, 0.5095541477203369\n",
            "Epoch 13 \t Max val Acc = 0.5199999809265137, 0.5414012670516968\n",
            "Epoch 14 \t Max val Acc = 0.5241379141807556, 0.5605095624923706\n",
            "Epoch 19 \t Max val Acc = 0.5255473852157593, 0.5859872698783875\n",
            "Epoch 20 \t Max val Acc = 0.529411792755127, 0.5923566818237305\n",
            "Epoch 21 \t Max val Acc = 0.5373134613037109, 0.6050955653190613\n",
            "Epoch 28 \t Max val Acc = 0.5483870506286621, 0.6433120965957642\n",
            "Epoch 35 \t Max val Acc = 0.550000011920929, 0.656050980091095\n",
            "Epoch 48 \t Max val Acc = 0.5517241358757019, 0.668789803981781\n",
            "Epoch 50 \t Max val Acc = 0.5546218752861023, 0.662420392036438\n",
            "Epoch 54 \t Max val Acc = 0.5593219995498657, 0.668789803981781\n",
            "Epoch =  1000 \t Train loss = 0.07853 \t  Train Acc = 0.97460 \t Val loss = 2.71680 \t Acc = 0.447370.354170.39535 \t Patience = 1054\n",
            "Epoch =  2000 \t Train loss = 0.05597 \t  Train Acc = 0.97302 \t Val loss = 3.38138 \t Acc = 0.357140.312500.33333 \t Patience = 54\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.38571426272392273, 0.4522292912006378\n",
            "Epoch =  0 \t Train loss = 0.78423 \t  Train Acc = 0.46667 \t Val loss = 0.69698 \t Acc = 0.293480.562500.38571 \t Patience = 2000\n",
            "Epoch 4 \t Max val Acc = 0.3999999761581421, 0.656050980091095\n",
            "Epoch 42 \t Max val Acc = 0.42222222685813904, 0.668789803981781\n",
            "Epoch 55 \t Max val Acc = 0.43478259444236755, 0.668789803981781\n",
            "Epoch 56 \t Max val Acc = 0.4395604431629181, 0.675159215927124\n",
            "Epoch 57 \t Max val Acc = 0.44680851697921753, 0.668789803981781\n",
            "Epoch 62 \t Max val Acc = 0.4536082446575165, 0.662420392036438\n",
            "Epoch 92 \t Max val Acc = 0.4565217196941376, 0.6815286874771118\n",
            "Epoch 94 \t Max val Acc = 0.4615384638309479, 0.6878980994224548\n",
            "Epoch 96 \t Max val Acc = 0.47826087474823, 0.6942675113677979\n",
            "Epoch 99 \t Max val Acc = 0.49462366104125977, 0.7006369233131409\n",
            "Epoch =  1000 \t Train loss = 0.07843 \t  Train Acc = 0.97143 \t Val loss = 2.49787 \t Acc = 0.380950.333330.35556 \t Patience = 1099\n",
            "Epoch =  2000 \t Train loss = 0.04601 \t  Train Acc = 0.97778 \t Val loss = 2.99642 \t Acc = 0.422220.395830.40860 \t Patience = 99\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.46305420994758606, 0.30573248863220215\n",
            "Epoch =  0 \t Train loss = 0.76419 \t  Train Acc = 0.51270 \t Val loss = 0.69783 \t Acc = 0.303230.979170.46305 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.4705882668495178, 0.31210190057754517\n",
            "Epoch 5 \t Max val Acc = 0.47120416164398193, 0.35668790340423584\n",
            "Epoch 6 \t Max val Acc = 0.49180325865745544, 0.40764331817626953\n",
            "Epoch 34 \t Max val Acc = 0.5043478012084961, 0.6369426846504211\n",
            "Epoch 35 \t Max val Acc = 0.5087718963623047, 0.6433120965957642\n",
            "Epoch 36 \t Max val Acc = 0.5172414183616638, 0.6433120965957642\n",
            "Epoch 37 \t Max val Acc = 0.5344827175140381, 0.656050980091095\n",
            "Epoch 39 \t Max val Acc = 0.539130449295044, 0.662420392036438\n",
            "Epoch =  1000 \t Train loss = 0.05339 \t  Train Acc = 0.97619 \t Val loss = 2.86718 \t Acc = 0.422220.395830.40860 \t Patience = 1039\n",
            "Epoch =  2000 \t Train loss = 0.03414 \t  Train Acc = 0.99365 \t Val loss = 3.77310 \t Acc = 0.390240.333330.35955 \t Patience = 39\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.40414509177207947, 0.2675159275531769\n",
            "Epoch =  0 \t Train loss = 0.73628 \t  Train Acc = 0.48254 \t Val loss = 0.69478 \t Acc = 0.268970.812500.40415 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.41379314661026, 0.35031846165657043\n",
            "Epoch 2 \t Max val Acc = 0.42352941632270813, 0.3757961690425873\n",
            "Epoch 6 \t Max val Acc = 0.4303797483444214, 0.42675158381462097\n",
            "Epoch 7 \t Max val Acc = 0.4430379867553711, 0.4394904375076294\n",
            "Epoch 15 \t Max val Acc = 0.4592592418193817, 0.5350318551063538\n",
            "Epoch 27 \t Max val Acc = 0.469696968793869, 0.5541401505470276\n",
            "Epoch 47 \t Max val Acc = 0.47154471278190613, 0.5859872698783875\n",
            "Epoch 159 \t Max val Acc = 0.48148149251937866, 0.6433120965957642\n",
            "Epoch 164 \t Max val Acc = 0.4955751895904541, 0.6369426846504211\n",
            "Epoch 178 \t Max val Acc = 0.5045045018196106, 0.6496815085411072\n",
            "Epoch 186 \t Max val Acc = 0.5046729445457458, 0.662420392036438\n",
            "Epoch 187 \t Max val Acc = 0.5094339847564697, 0.668789803981781\n",
            "Epoch 232 \t Max val Acc = 0.5199999809265137, 0.6942675113677979\n",
            "Epoch 233 \t Max val Acc = 0.5252525806427002, 0.7006369233131409\n",
            "Epoch 285 \t Max val Acc = 0.5306122303009033, 0.7070063948631287\n",
            "Epoch 301 \t Max val Acc = 0.5346534252166748, 0.7006369233131409\n",
            "Epoch =  1000 \t Train loss = 0.06442 \t  Train Acc = 0.97619 \t Val loss = 3.10914 \t Acc = 0.439020.375000.40449 \t Patience = 1301\n",
            "Epoch =  2000 \t Train loss = 0.04500 \t  Train Acc = 0.98571 \t Val loss = 3.89933 \t Acc = 0.405410.312500.35294 \t Patience = 301\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.4375, 0.42675158381462097\n",
            "Epoch =  0 \t Train loss = 0.82612 \t  Train Acc = 0.45079 \t Val loss = 0.69286 \t Acc = 0.312500.729170.43750 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.4561403691768646, 0.40764331817626953\n",
            "Epoch 2 \t Max val Acc = 0.4565216898918152, 0.36305731534957886\n",
            "Epoch 3 \t Max val Acc = 0.4864864945411682, 0.3949044644832611\n",
            "Epoch 4 \t Max val Acc = 0.4972972571849823, 0.40764331817626953\n",
            "Epoch 7 \t Max val Acc = 0.5029940009117126, 0.47133758664131165\n",
            "Epoch 8 \t Max val Acc = 0.5061728358268738, 0.4904458522796631\n",
            "Epoch 9 \t Max val Acc = 0.522292971611023, 0.522292971611023\n",
            "Epoch 27 \t Max val Acc = 0.5271317958831787, 0.6114649772644043\n",
            "Epoch 33 \t Max val Acc = 0.53125, 0.6178343892097473\n",
            "Epoch =  1000 \t Train loss = 0.05327 \t  Train Acc = 0.97937 \t Val loss = 3.08545 \t Acc = 0.418600.375000.39560 \t Patience = 1033\n",
            "Epoch =  2000 \t Train loss = 0.08324 \t  Train Acc = 0.96984 \t Val loss = 3.86831 \t Acc = 0.456520.437500.44681 \t Patience = 33\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.3870967924594879, 0.6369426846504211\n",
            "Epoch =  0 \t Train loss = 0.76119 \t  Train Acc = 0.53333 \t Val loss = 0.69024 \t Acc = 0.400000.375000.38710 \t Patience = 2000\n",
            "Epoch 3 \t Max val Acc = 0.40860214829444885, 0.6496815085411072\n",
            "Epoch 5 \t Max val Acc = 0.41999998688697815, 0.6305732727050781\n",
            "Epoch 6 \t Max val Acc = 0.4615384638309479, 0.6433120965957642\n",
            "Epoch 7 \t Max val Acc = 0.4716980755329132, 0.6433120965957642\n",
            "Epoch 18 \t Max val Acc = 0.4859813451766968, 0.6496815085411072\n",
            "Epoch 23 \t Max val Acc = 0.4952380955219269, 0.662420392036438\n",
            "Epoch 24 \t Max val Acc = 0.5094339847564697, 0.668789803981781\n",
            "Epoch 32 \t Max val Acc = 0.5137614607810974, 0.662420392036438\n",
            "Epoch 36 \t Max val Acc = 0.5178571343421936, 0.656050980091095\n",
            "Epoch 41 \t Max val Acc = 0.5263157486915588, 0.656050980091095\n",
            "Epoch 87 \t Max val Acc = 0.5272727012634277, 0.668789803981781\n",
            "Epoch 93 \t Max val Acc = 0.5309734344482422, 0.662420392036438\n",
            "Epoch 97 \t Max val Acc = 0.5357142686843872, 0.668789803981781\n",
            "Epoch 101 \t Max val Acc = 0.5420560240745544, 0.6878980994224548\n",
            "Epoch =  1000 \t Train loss = 0.06402 \t  Train Acc = 0.96667 \t Val loss = 2.59801 \t Acc = 0.433960.479170.45545 \t Patience = 1101\n",
            "Epoch 1894 \t Max val Acc = 0.5473684072494507, 0.7261146306991577\n",
            "Epoch =  2000 \t Train loss = 0.04571 \t  Train Acc = 0.98730 \t Val loss = 3.40179 \t Acc = 0.500000.500000.50000 \t Patience = 1894\n",
            "Epoch =  3000 \t Train loss = 0.05419 \t  Train Acc = 0.98571 \t Val loss = 3.73414 \t Acc = 0.442310.479170.46000 \t Patience = 894\n",
            "Training complete\n",
            "Max val losses________________\n",
            "[tensor(0.5952), tensor(0.6392), tensor(0.6168), tensor(0.6095), tensor(0.6353), tensor(0.5833), tensor(0.5664), tensor(0.6042), tensor(0.6486), tensor(0.5645), tensor(0.5625), tensor(0.5455), tensor(0.5895), tensor(0.5649), tensor(0.5287), tensor(0.5607), tensor(0.5536), tensor(0.6186), tensor(0.5532), tensor(0.5593), tensor(0.4946), tensor(0.5391), tensor(0.5347), tensor(0.5312), tensor(0.5474)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1KrSkOSWvF7",
        "outputId": "f0eec8b6-deb8-4918-d698-bd1603d5a1d9"
      },
      "source": [
        "len(aut_embed_state_dicts)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffHjXjDNW5Mt",
        "outputId": "5bd5943a-6f8d-46f3-f5b5-6838cb5c4e3e"
      },
      "source": [
        "aut_embed2 = CustomEmbedding(95, 60)\n",
        "aut_embed2.load_state_dict(aut_embed_state_dicts[np.argmax(max_val_accs)])\n",
        "aut_cla = SimpleNN(aut_embed2,  60,2)\n",
        "aut_cla.eval()\n",
        "evaluate(aut_cla, inputs_train, labels_train, loss_func, device, calc_accuracy, pr=True)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7486),\n",
              " tensor(0.3143),\n",
              " tensor(0.3082),\n",
              " tensor(0.9948),\n",
              " tensor(0.4706))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyZS_v_sXJ5H",
        "outputId": "ac76c463-3365-486e-946b-76da04766d90"
      },
      "source": [
        "aut_cla.eval()\n",
        "evaluate(aut_cla, inputs_train, labels_train, loss_func, device, calc_accuracy, pr=True)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7486),\n",
              " tensor(0.3143),\n",
              " tensor(0.3082),\n",
              " tensor(0.9948),\n",
              " tensor(0.4706))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNwyoKqPiMCO",
        "outputId": "6741437d-7367-41e8-f9d5-0c46a809803f"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 30 - dropout"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6SkATC5iLQu",
        "outputId": "37172715-81c1-4e96-a0ca-faa02203f1e1"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 60 - dropout"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I93UGO1Xh75w",
        "outputId": "71e63c42-b7d0-4540-85d6-1274e3c0563a"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 100 - dropout"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cijwk2zwg6e1",
        "outputId": "257f01e3-7ecb-4156-a6af-9df8015de409"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 30 - dropout"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5GNdKX0glpg",
        "outputId": "1126272f-97b4-421f-8f05-9d11cf99c93f"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 60 - dropout"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or-jzBNYflyv",
        "outputId": "628ecce3-bc5f-4138-f966-a31da89b7ab7"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 100 - dropout"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpLEJBMLebpc",
        "outputId": "8d3748c4-ea2a-4463-dea3-9d264d408b2f"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 30 - dropout"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3W01BR6eU6T",
        "outputId": "933c5d21-a14f-4578-bf2c-8b902b25e466"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 100 - dropout"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1esMYRO1c1fd",
        "outputId": "6d23bbb9-1bdd-43f4-86fa-5677497b0274"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 60 - dropout"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2HnS1Bm1avD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "93f17e55-2fe5-4d16-83ec-f84a8e8174ca"
      },
      "source": [
        "plt.plot(train_losses,label=\"Training loss\")\n",
        "plt.plot(val_losses,label=\"Validation loss\")\n",
        "plt.title('Autism Training and Validation Losses')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(val_accuracies, label=\"Validation accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Training accuracy\")\n",
        "plt.title('Autism Validation Accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e+bHkgIvUaaCCgtQEAFRcAGgh0LIorY1nUtWLGzlp/u6q4uu5ZFXSyAoK66KqKCdFFp0ptUCb0moSSknN8f5w5TMkkmZTKT5P08T56599z2zk3yzp1zzz1HjDEopZSqPiJCHYBSSqmKpYlfKaWqGU38SilVzWjiV0qpakYTv1JKVTOa+JVSqprRxF8Jici5IrI+1HEESkSmicjN5b1uKInIVhG5IAj7nS0itznTw0Tk+0DWLcVxmovIERGJLG2sqvLSxF+BnH/UQyISW8LtjIi0cc0bY+YZY9qVf4Rexzzi8ZMvIsc95oeVZF/GmIHGmPfLe91wJCKjRWSun/L6InJCRDoGui9jzERjzEXlFJfXB5Ux5ndjTIIxJq889u9zLK+/VxV+NPFXEBFpCZwLGOCykAYTACcpJBhjEoDfgUs9yia61hORqNBFGZYmAL1EpJVP+fXASmPMqhDEpJQXTfwV5ybgZ+A9wKsqw/cru4iMEJH5zrTr6nG5c7V9nYj0FZE0j/UfFZEdIpIpIutF5HynfIyIfCIiE5xlK0WkrYg8JiJ7RWS7iJToitJ1bOeYu4HxIlJHRL4WkX3ON5qvRSTZ3/tzvTcRecVZd4uIDCzluq1EZK7z3maIyOsiMqGQuAOJ8TkR+dHZ3/ciUt9j+XAR2SYiB0TkicLOjzEmDZgJDPdZdBPwQXFx+MR88u/Amb9QRNaJSLqI/AsQj2WnishMJ779IjJRRGo7yz4EmgNfOX9Dj4hIS+fKPMpZp6mIfCkiB0Vko4jc7rHvMSLysYh84Jyb1SKSWtg5KIyIJDn72OecyydFJMJZ1kZE5jjvbb+ITHHKRURedf5eM5y/4Y7Osljnb+N3EdkjIm+JSLyzrL5zbg8772me61hKE39FugmY6PxcLCKNAtnIGNPHmeziXG1P8VwuIu2APwE9jDGJwMXAVo9VLgU+BOoAvwLfYX/vzYBngX+X4r00BuoCLYA7nP2Nd+abA8eBfxWx/ZnAeqA+8FfgXRGRUqw7CVgI1APGUDDZegokxhuAW4CGQAzwEICInAG86ey/qXM8v8na8b5nLM7vKMWJt6TnyrWP+sBnwJPYc7EJ6O25CvCiE9/pwCnYc4IxZjje39r+6ucQk4E0Z/shwP+JSH+P5Zc569QGvgwkZj/+CSQBrYHzsP8TtzjLngO+x/6dJjvrAlwE9AHaOtteCxxwlr3klKcAbbB/0087yx503k8DoBHwOPbbtgIwxuhPkH+Ac4AcoL4zvw4Y5bF8NnCbx/wIYL7HvAHaeMz3BdKc6TbAXuACINrnuGOA6R7zlwJHgEhnPtHZd+1i4t8KXOBx7BNAXBHrpwCH/L0/571t9FhWw4mhcUnWxSbNXKCGx/IJwIQAfyf+YnzSY/6PwLfO9NPAZI9lNZ1zcEEh+64BZAC9nPkXgP+V8lzNd6ZvAn72WE+wie22QvZ7BfCrv9+hM9/SOZdR2A+JPCDRY/mLwHsef0czPJadARwv4tx6/b06ZZHOOTvDo+xOYLYz/QEwDkj22a4/sAE4C4jwef9HgVM9ys4GtjjTzwL/841Df+yPXvFXjJuB740x+535SfhU95SWMWYjcD/2n3OviEwWkaYeq+zxmD4O7DfuG3rHndeEEh52nzEmyzUjIjVE5N/O1/cMYC5QWwpvMbLbI/5jxcRQ2LpNgYMeZQDbCws4wBh3e0wf84ipqee+jTFHcV91FuDE9Alwk/PtZBg2sZXmXLn4xmA850WkkfO73+HsdwL2m0EgXOcy06NsG/YK2sX33MRJye7v1Aeinf36O8Yj2GS+0KlKGglgjJmJ/XbxOvbve5yI1MJeydcAljjVOYeBb51ygJeBjcD3IrJZREaXINYqTxN/kDl1jtcC54nIbrH14qOALiLSxVntKPaP2KVxSY5hjJlkjDkHW31ggL+UPfKiD+kz/yDQDjjTGFML+9UcPOqgg2AXUFdEPM/bKUWsX5YYd3nu2zlmvWK2eR/7e78Q+83qqzLG4RuD4P1+/w/7e+nk7PdGn30WVc2xE3suEz3KmgM7iompJPZjv/W28HcMY8xuY8ztxpim2G8Cb4jTMsgYM9YY0x37TaMt8LCzv+NAB2NMbecnydjGCBhjMo0xDxpjWmOrqR4Q596X0sRfEa7Afo0+A/u1PgVbBzsP+/UdYBlwlXM12Aa41Wcfe7D1ogWISDsR6S+2iWgW9p8hv9zfRdESneMeFpG6wDPBPqAxZhuwGBgjIjEicja2KisYMX4KDBaRc0QkBluNUNz/zjzgMLb6YrIx5kQZ45gKdBCRq5wr7XvxvkBIxFbjpYtIM2xy9FTo35AxZjuwAHhRROJEpDP2b9DvjfIAxTj7ihOROKfsY+AFEUkUkRbAA65jiMg14r7JfQj7QZUvIj1E5EwRicZeIGUB+caYfOBt4FURaejso5mIXOxMD3ZuGAuQjv0frOj/i7CliT/4bgbGG9tuerfrB/v1dZjzT/wqtv5zD/ZKcaLPPsYA7ztfaa/1WRaLvcm1H/t1vCHwWNDejX+vAfFODD9jv3JXhGHYet0DwPPAFCC7kHVLHaMxZjVwN7aKbhc2MaUVs43BVu+0cF7LFIdTTXgN9nd9ADgN+NFjlT8D3bBJbir2RrCnF4Ennb+hh/wcYii23n8n8DnwjDFmRiCxFWI19gPO9XMLcA82eW8G5mPP53+c9XsAv4jIEezN4/uMMZuBWtgEfwhbNXQAW40D8Ci2Oudnp3prBvbbFNjzMwP7YfgT8IYxZlYZ3k+VIs6NEKUqPacJ4DpjTNC/cShVmekVv6q0nGqAU0UkQkQGAJcDX4Q6LqXCnT51qSqzxtgqjXrYqpe7jDG/hjYkpcKfVvUopVQ1o1U9SilVzYRVVU/9+vVNy5YtQx2GUkpVGkuWLNlvjGlQ/JpuYZX4W7ZsyeLFi0MdhlJKVRoisq34tbxpVY9SSlUzmviVUqqa0cSvlFLVTFjV8fuTk5NDWloaWVlZxa+sQiouLo7k5GSio6NDHYpSqghhn/jT0tJITEykZcuWFD5Whwo1YwwHDhwgLS2NVq18Rx1USoWTsK/qycrKol69epr0w5yIUK9ePf1mplQlEPaJH9CkX0no70mpyqFSJH6llKqS8vPh14mQsbNCD6uJvwgHDhwgJSWFlJQUGjduTLNmzU7OnzhxoshtFy9ezL333lvsMXr16lUusc6ePZvBgweXy76UUkF2/DAseQ+erQP/+yN8eU+FHj7sb+6GUr169Vi2bBkAY8aMISEhgYceco9hkZubS1SU/1OYmppKampqscdYsGBB+QSrlKo8/tLCe37Pmgo9vF7xl9CIESP4wx/+wJlnnskjjzzCwoULOfvss+natSu9evVi/fr1gPcV+JgxYxg5ciR9+/aldevWjB079uT+EhISTq7ft29fhgwZQvv27Rk2bBiunlO/+eYb2rdvT/fu3bn33nuLvbI/ePAgV1xxBZ07d+ass85ixYoVAMyZM+fkN5auXbuSmZnJrl276NOnDykpKXTs2JF58+aV+zlTqlLKz4P08hx22OGvR+QKvj9Wqa74//zVatbszCjXfZ7RtBbPXNqhRNukpaWxYMECIiMjycjIYN68eURFRTFjxgwef/xx/vvf/xbYZt26dcyaNYvMzEzatWvHXXfdVaC9+6+//srq1atp2rQpvXv35scffyQ1NZU777yTuXPn0qpVK4YOHVpsfM888wxdu3bliy++YObMmdx0000sW7aMV155hddff53evXtz5MgR4uLiGDduHBdffDFPPPEEeXl5HDt2rETnQqkqa/F/4JuH4I8/Q8PTy7av9d/CJyPg4Y0QFVsu4ZVFpUr84eKaa64hMjISgPT0dG6++WZ+++03RIScnBy/2wwaNIjY2FhiY2Np2LAhe/bsITk52Wudnj17nixLSUlh69atJCQk0Lp165Nt44cOHcq4ceOKjG/+/PknP3z69+/PgQMHyMjIoHfv3jzwwAMMGzaMq666iuTkZHr06MHIkSPJycnhiiuuICUlpUznRqkqY+MP9vXg5rIn/u8eh9zjkLEDEhsXXJ4RhG8WRahUib+kV+bBUrNmzZPTTz31FP369ePzzz9n69at9O3b1+82sbHuT/nIyEhyc3NLtU5ZjB49mkGDBvHNN9/Qu3dvvvvuO/r06cPcuXOZOnUqI0aM4IEHHuCmm24q1+MqFZZys+HHf0DqSKhZv+DyCHtxx+bZ0H5Q2Y51/JB9/eQWOKVH2fZVDrSOv4zS09Np1qwZAO+99165779du3Zs3ryZrVu3AjBlypRitzn33HOZOHEiYO8d1K9fn1q1arFp0yY6derEo48+So8ePVi3bh3btm2jUaNG3H777dx2220sXbq03N+DUmEjbTH89Lqd/m06zHoB5r/qf938PPu6cJxtdunp8O/wQhNbhROI4wft697VtjVPiGniL6NHHnmExx57jK5du5b7FTpAfHw8b7zxBgMGDKB79+4kJiaSlJRU5DZjxoxhyZIldO7cmdGjR/P+++8D8Nprr9GxY0c6d+5MdHQ0AwcOZPbs2XTp0oWuXbsyZcoU7rvvvnJ/D0qFjXcvstUuebmQ6zxlfmire3l6mv0BqNvaXe5K3C5piyHnGCybYOcPbIKPhkJWesFjLnm/+LjuXxXwWygPYTXmbmpqqvEdiGXt2rWcfnoZ69cquSNHjpCQkIAxhrvvvpvTTjuNUaNGhTosv/T3pSpUfj58dQ+k3grNuhW//hjnoun+lbB5Dnz5Jzv/+C6IqeFePiYdpj9tq4IA+jwC/Z9w7+er++yV+xmXw7UfwAdXwOZZMPwLOLWf/2MWGZefD4wAicgSY0zxbcc96BV/JfD222+TkpJChw4dSE9P58477wx1SEqFh+OH4NcJ8LaTbPPz4fdf3NOHf4e9awtut+hde8XuMuOZguvkZrun5/7VPX1ws7u6Rpz7AMcOFNwGIM9/Yw8A7l1W+LIgq1Q3d6urUaNGhe0VvlIhlevRKeDMF2DDNNi9EoZ/Dis/hWX2XleBK+razSHrsHv+4Bbv5Rt/sEk8uibkHLVlJ47CKp+m2hFR3nH8+BrM+Qvc/BXEJsCJI/7jHvQ3qBu6Xmw18SulKqfjhyDnuHve86r845sh2+eZn0MeQ9PmHIMTHlf8xw961/VPuAq63ADxddyJ/8jegl0ruFr+uK70f//Jvr7YzH7YZDuJPyYRTmS6t+txW0BvMViCnvhFJBJYDOwwxmhnMkqp0ts4A2q3tM0v/9ISWp7rfz3fpA9wdL/H8kzvqp4dS+AfXbzXP5EJsYnueX9t7fNyYNkkW6Xkz68f2tfUEbDgn3b63l/dy/s/BXVa+t82iCriiv8+YC1QqwKOpZSqqk4cgwlX2+kRU+3r1gC7GMnNhjyPjhWzM23VTVGyMrwT/4QhBdfJ3AVf3FXIMU/A+ml2usU57sTv2Vqoz0MFt6sAQb25KyLJwCDgnWAeRylVxWWlw/81cc8XdoVdmPXfQJ7HjdfsDO8rfn+2zIEY98Oa5B4vuE769sK3Hz/A1vE3S4VG4fHwqUuwW/W8BjwC5Be2gojcISKLRWTxvn37ghxOyfXr14/vvvvOq+y1117jrrsK+ZQH+vbti6tZ6iWXXMLhw4cLrDNmzBheeeWVIo/9xRdfsGaNu9e+p59+mhkzZpQkfL+0C2dVZvvWw4bvil+vPORmw6Tr3PMJjWy3xp7aFfNk7ScjvNvYZ2d61/F7OrW/e3rvGhjwUiHrnV/0B9COJbYFUFJyWPTP4yloiV9EBgN7jTFLilrPGDPOGJNqjElt0KBBsMIptaFDhzJ58mSvssmTJwfUWRrYnjVr165dqmP7Jv5nn32WCy64oFT7Uqpcvd4TJl1b/vvN3FOwbOE4901TgIjogq1lEhv531+E0xFiTAJ87HRFEl3DuTF8FJJ7Ftwmvq57OjvT+4PAU0wN/+W+kpIhMrr49SpQMK/4ewOXichWYDLQX0QmBPF4QTFkyBCmTp16cuCVrVu3snPnTs4991zuuusuUlNT6dChA88846cdMNCyZUv277c3lV544QXatm3LOeecc7L7ZrDt9Hv06EGXLl24+uqrOXbsGAsWLODLL7/k4YcfJiUlhU2bNjFixAg+/fRTAH744Qe6du1Kp06dGDlyJNnZ2SeP98wzz9CtWzc6derEunXrinx/2oWzKpPyfAB09yr4W9uCT7q62si7ZKTBUZ/agRoefe3UqOeebtnbvnp+UMTXgS1zbR88MTUgKt57X/F13NMmH+r4aXZZv63/5wMAOlzlPZ+UDJEx/tcNkaDd3DXGPAY8BiAifYGHjDE3lmmn00bbNrrlqXEnGFjIVzmgbt269OzZk2nTpnH55ZczefJkrr32WkSEF154gbp165KXl8f555/PihUr6Ny5s9/9LFmyhMmTJ7Ns2TJyc3Pp1q0b3bt3B+Cqq67i9ttvB+DJJ5/k3Xff5Z577uGyyy5j8ODBDBnifVMpKyuLESNG8MMPP9C2bVtuuukm3nzzTe6//34A6tevz9KlS3njjTd45ZVXeOedwm+xaBfOqkzyc8vvavYrZ8S6376H7jfbqpm4JPdDUp4ObPKe9+xkLdKpVjlnFPS6F/7qk7g9W+ccOwD3LbcfOC6e1TKt+kCUn6Qdk+B9s/ih32DrfPj0Ftuh2+rP3MvCMPHrk7sB8Kzu8azm+fjjj+nWrRtdu3Zl9erVXtUyvubNm8eVV15JjRo1qFWrFpdddtnJZatWreLcc8+lU6dOTJw4kdWrVxcZz/r162nVqhVt29o/1ptvvpm5c+eeXH7VVfaKo3v37ic7dyvM/PnzGT58OOC/C+exY8dy+PBhoqKi6NGjB+PHj2fMmDGsXLmSxMTEIvetwlR+Xvldqfs+qVpaxtg6cZfti+Cl5rD2a+8E6+Jbt+6Z+F1dN5x9D9SoS5GO7LXVRNd5VEa4nrY9/VIYMt5OP7gB/rTYPpgFIBFwzXvubRIaQocr7Todr/Y+Rlxt+6BXzzvhhk+KjqeCVMgDXMaY2cDsMu+oiCvzYLr88ssZNWoUS5cu5dixY3Tv3p0tW7bwyiuvsGjRIurUqcOIESPIysoqfmd+jBgxgi+++IIuXbrw3nvvMXv27DLF6+reuSxdO2sXzlXYs3Wh641w+etl31dutn1Ctcz78fjfkQjY6bR1XzAWtv9ScP0Dv3nPe9bLX/lv2LMKatbDrz/8CG85VUCuB6xOv9S9vHEn+9pxiPu9JTayP67ulSUC6p8Gd85zf7iI2DJPXYfDKT3tskv+SrjQK/4AJCQk0K9fP0aOHHnyaj8jI4OaNWuSlJTEnj17mDZtWpH76NOnD1988QXHjx8nMzOTr7766uSyzMxMmjRpQk5OzsnulAESExPJzMwssK927dqxdetWNm7cCMCHH37IeeedV6r3pl04VzOuroZ/Lafbba6EnZcLOQFc+Bw/BLuW2+n10+ArWz3p1ab+wEZIW2Sn/SV9f5qfbV973G6TdfOz3MuGf+69br1T4Ynddvri593ljTra1643wq3TbQdsvhq0hyYpcP7Tdr5JZ1uVU4BAww5w+b/CrkUPaJcNARs6dChXXnnlySofV1fG7du355RTTqF3795Fbt+tWzeuu+46unTpQsOGDenRwz0Yw3PPPceZZ55JgwYNOPPMM08m++uvv57bb7+dsWPHnrypCxAXF8f48eO55ppryM3NpUePHvzhD38o1ftyjQfcuXNnatSo4dWF86xZs4iIiKBDhw4MHDiQyZMn8/LLLxMdHU1CQgIffPBBqY6pQqiwvmNKy9U2fsKV9oZpYb1MZuyCv7d3z49Jh4+ut9MXPecd19419qc4MR594UTFFn5s31Y5UXH2Ctx3/dtn2nsWIvYq3Z+4WnDnnOJjG73NfisIU9otsypX+vsKc+lp8KrzMJG/RHloG9RqBpHFXBO6uhq+4RNoe5F3d8b+rPgEPvPon+aZw/Bnp5nzXT/ZD5BxfYs+5rD/2lY44wfa+Xqnuat8iuvW2LNr5DJ0gRyOtFtmpVTRPB9aWvM/75u8R/bBPzrbgUoCNeka73lXVVJxNs9yT394JUwNoOuC0y6AFr3c876DoxTl4v8LfN1qQBO/UlXRqv/aRO7Ls0rl45tsVwYurm6KN04v2bFyPVrdfPMwbP3RXmH/9AZMHmabZXomerDJ3uXIbtjh/U2/gJu/LlhWkiaSZ98NI76BJ8Ovd4BQqBSJP5yqo1Th9PdUgQ5tg09H+h/q7/ghu8zfk7W+/dMc8XhS1jg9q5T09+jZE+bid2H2i3b6u8dg3dfwzoXufvGL46+lUXIPaOXRC+et073jDVTL3v7b5FdDYX9zNy4ujgMHDlCvXj1EJNThqEIYYzhw4ABxcXGhDqV6WDbRXtVn7IIr3vDuFsDVR/1OP62ufPunyUq31TN5Jzw+REqY+H0/fDJ3e8/vX09ATjkL2l1SsNzVwselgXOTOLEJnPeovSehSiTsE39ycjJpaWmEYwduyltcXBzJyf6atqly5+qi4PcFMDYFUkfC4FdtWW4RzSp9W/XMGGO/PSz9AIxTP39oK/w2w9apu2xfZJtButqsx9e1dextB9j+bDz5trGPrmkHLPHXR/7wz93VPvG1bXcJp11ku0Te/rMtb+/ToWBcLfvwVPOzIbFx4e9VFSrsE390dDStWoVuiDKlwsr+jXDYYySphEa2umbtVx6Jv4inaddNLVi2fLI76btMvBqun2SvwEXg3Qugdgu43/bldPLp1pzj/hO6p5wi+r33bGp5cIs91jDn6VZXS5zr/Dxz0OHKgmUqYJWijl8p5fhXdzssoKtveFcdvWdrGs/Eb4xNoD+/absIXuU8D5LY1GN9P/3MA0y+wT5k5dr34W3w81s26ec7iT87016dl8U5znjScUn+l2sVb7nTxK9UZfHdE+7ptV95Lzt+EKY9aq/APRO/q0+bb0fD2K7u8j8t8u6FsjCTh3r3lfPto/DzG+4r/p1L7QDnAEMnF9y+lccT5T3vhFFr3MMlNnOanrfua199n3C94RO49B/Fx6hKLOyrepSq0pZPgakPwCNbCm9xkp9vOzD76V/uMt8bngC/vGX7mj+1n7vMt0tjl5iaQIBX0r6dpGXu9q4acnX/4K9v+76j7UhWANHxkNQMRnxtv30kOH3ou1rn+D7p2vaiwOJTJaZX/EqF0rSH7Q3XPatsAj3sM5SfMfDDGFvHHoi0Rd5X/Bk7/a8nYm8IB8LVMZnvfIxP76yxibaLY0/R8e6bs9EeA5fUbe0e1rCW0yDA8wNLBZVe8SsVSq4qk7c9kp6rS4EdS2H604EPKA621Y1nq54pwwqu09kZxrDfE/DjP9z19YX5Rxfv+eUf2dcTPq15omKgTkv3/OBXbYdmrpY3hX2jadDWfmDUblF0HKrc6BW/UsFkDMz7e+Fjs/rra94YmDDEfhgUlfQvHQudr/cuW/O/goOUFDimk+gjIvwPH1jYGLOBum6CbaaZOtJ+s3Alft9vDp7qtNSbuBVIE79SwZSeBj/8GT66wbv8txmwfaHtDdLX8UOBdZuQlGzHqOjr07fOzOeK3s7zJqprTNpr3rdX5wCN/Y8iV0BsIa1wTr/Uu5mmqyonr3RjQ6jyp4lfqWBy3QR19YMD9mbtxKvh3QsLrh8VD0f3+9/XfSu85xMb25Y5PW/3Lvf3YeLSLBUuGOOev/BZ+3rahfYq/b7lkJwK7QbZgUiK4tmNQlE6Xg3nPgTnPRLY+iroNPErFUyuG61e7ewLaTfvWvahn4eTntwHdVrYPuNdEpwqlPg6toviTn765gG44k14LA2uHAe3/+D9tGvXYfaeQkxNe3+gTkv7jWDopIKjSfny/cApTFQMnP+UfTJXhQVN/EqVxrJJ9olXgA3fwx6PgUMOb4d0Z0Bv18hSnh2K7ffp0sCljdNyJyOt4DLXjdEIj4HNXe3wReCexTDolYLbxSRAyg22xU2X64p+T77OLGZwn+ia7umR35ds3yqkNPErVRpf3AWf32mHG5x0Dbx5tnvZax3h1TPstKujNJNnb7qmp8G4QobJbDug+OM26mjr9B9cb2/OeopLgt73eZeVZdi/+NrQ6173vGeiB9tUE6B2c2h+ZumPoyqcNudUqqR2ePR6+d9b3dPbfrJJ0CXnOEx90E4fPwz/7OZ/f6m3QpfrbWdpLvXa2LFnfUVEQN9HC49ti08roM4lvMr35Wp7f9bd0OZ8G9M0p67elfi1O+5KR6/4lfIn9wR8cgvsWw8bvoO5HtUonm3u13kMEDJ+AHxwmXv+yF73dIzP1bKn/k/aMV4TG7nLrv3A9lIJtj/6QN30P6jZAB7dCnfMgYueL3aTIrmGYIyKsYn/zDvt/ClnuT8UwnhsWeWfXvEr5c/eNbD6M9i/wT5VC7ZljmfVhz+eV+nfPW6rbzZ8692qx5erq+NWfdxlsYnQ6x747Xv7lGug4mrBw04MgfTFU5x8595EhEeqeHyXnY+MhnMegM6F3FRWYUs/qpXyJyLSvmbscJct+CfM+Wvg+1j3deFj0F4wpuhtY2vZvm/6PAIXvxj4MYPF8+Z0TA37DUAELngGGp4eurhUqegVv1L+5DjdHvg+bbro7ZLtx/dBrBa9YduP0LIPnP+Md++VnmIT7YdP/yf8L68o9U61r0mnhDYOVa408SvlT1Ft7QNRK9l/s8xzRtkuDWrUheTuBZen3mrHrXV94wi1jlfb1j2t+xe/rqo0tKpHKX9yypj4+zzov/y0C911+v4M/ru7k7ZwIGKfL/BtOqoqNf1tqqpp13L3jclALXzbjlaVlQGTArhhmXIj3PiZd6ub+1fCo9vgjCsKru/58JVSIaSJX1U9O5bAv/vA/L/b+TVfFt0z5Nqv4fef4ZuH7PxLxdRnuzonS7nB3bbdpR91cbsAACAASURBVGYDWzVSoy7cMbu070CpoNI6flU5bPjODtnnrxthT2lL3OPK7vzVPin78XDbY6S/QbvBf5/1vq79AKLioO3Fdt4YdzfCw79wP40bFefepkY97310KqbTM6UqiCZ+Ff52LLVVLz1ut/3R7FkD710Cdy2AWk29133H4ybkuq9tFwdQcIxafxIauQcv93XG5d7znn3HN02BQX+HzbO8y+OdunyJsH3ZFNfpmVIVRKt6VPhLd4YjdLWp//l1W3XzWwAdg83xGFRk2qOQfcQ9n5frPUyhb9JveEbgMfa4teA3ipia9iGsW6fDKT20d0oVNjTxq/CX7Qzx5+oiwNXiZtkk735ijh0sej+/vAUvNrP7O3oAnqsHE64ufP0//gSnng8X/1/p4haxXSYkp5Zue6WCRKt6VHjKybLDEh7ZA984nYJlZ8CsF22rG4Dtv8DhbbYP+dwT8NdWge37xWR39wi+Qxv2usc+oesy/LMyvQ2lwpEmfhWeXmhUsOy37wtW72TusYn/eDFX+762zPVf3nYgdLu5ZPtSqpLRqh4Venm5tv3890+VfNt9a2HzbPe3gLJKaGhvwuqNWFWFBS3xi0iciCwUkeUislpE/hysY6kwlJdr+6f3tWs5/K09HNkH+zbAqs9gwzS7bMHYkh9n2mj44HI4UMioVgDdRwS+v5oNSh6DUpVMMK/4s4H+xpguQAowQETOCuLxVKhl7rEPSwHMe8X2T//7L97rLPgnZO6ynZe93gM+vQWm3Oi9jr8PjMK4+tSZfIO7rPst7ulT+8Pg1+C2mQQkLinwYytVSQUt8RvL1XYu2vnRoXqqsknX2oelsjJg71pb5ttRmavbgtkv4Vderv3AKAvPq/Yrx9nWNcndbR849dsVXL9Bexg6xY4x69kOX6kqKqh1/CISKSLLgL3AdGPML8Vtoyqxw87QgdkZkJ9rpyOibDJ3cbXJP7wNv7IC6KBs4F9h4Mv+lw36G5zr0UGa7xV8ow4Ft+l9H7QbAAP/UvyxlaoCgpr4jTF5xpgUIBnoKSIdfdcRkTtEZLGILN63b18ww1HB5rqaP37YPSThjDHwfEPbdn73yoLNJ1NHes97jlRVp1XBJ2af2G2H/zvzDv8x9LgNouPghk+g35N2wBBPl/3TdrEAtiO1cx+CTtcE/BaVqgoqpDmnMeawiMwCBgCrfJaNA8YBpKamalVQZeaqJjlx1F12cLN9nXQ9bJtfcJsLxsD6abbeH2Dxf9zLLn4B2g+yTS9jEuyHgmuAb3963+eebnuR/fEVmwCn9oOHNtqhCSO1RbOqfoL2Vy8iDYAcJ+nHAxcC+l26Osg5WrDMX9IHWxXj2V3xT/9yT7uG+/Mci7YwyT2h7+OBx5igrXdU9RXMqp4mwCwRWQEswtbxfx3E46lQSE+zbfBnPu8uO17EwOKeBr9mXwu76k7w8xCXpy4eLXlum26reJRSxQraFb8xZgXQNVj7V2Fg3t/hB+fxjLkv2xu5AEf3F71dyjA7hmuq0+zS3wAl17wHp/Qsej+XvgbLJ0EDHexbqZLQCk5Vej/4PJPnaslTWNfGV79r+9Uf9Dfvuvrap8D+9d7rtg2gSWdULDz0W9H1/kqpAjTxK//y8wHjHvR7/0ZY+YkdTCS+LiwcV/i2817xX97wdP+DkVz1NqybCk27wlu9bVlUgNU2CQ0DW08pdZImfmVNGQ61m0Pn6+zgJWO72OQ/ahUsegcWj4e9q23/9u0Hu5trlkR0IaNn1agL3YZDXo67TB+kUipoNPEra63T1cJP/4JLXoHDv9v53SvdY9G6eI4xW5RLx8LuFfaDA2wvmkWJ1MHIlaoI2junKmjN/9zTs14ouHzfusK37TLUPR0RBRd5bK9X8UqFBb3iV7ZJpifPp2s3fFv89vcshVrNbPXP6ZfB8o9secer7Q3Y5B5w1l2BxRIVB7GJga2rlCoVTfwKXvXTf02gLh0L9U61064btw+stVVErnb1t80IfH8PrvPfvFMpVW60qkcF7tbpBctiEwqW1WoKbS8u3THi6/jfp1Kq3Gjirw7y820fOK5Bykuj7UD/D1TFaLWMUpWNJv7qYO2X8PUodx/4P46Ff59np/Pzi9/+oY1ww2Q7ffafbDt+V3cKcbXKP16lVFBp4q8OXAORHztg+8af/hTsWgb/vQ3GDyx621une3dodvEL8OgW9w3YuNrBiVkpFTSa+Ku6I/tgz2o7feIorPvKvWzlJ7D9Zzs9ZDxc/oYdqtCl6/DC+8vpfR9IBNRqEpy4lVJBI8aETxf4qampZvHixaEOo2rIz4fvn4Cf3whs/Ue3Qbxz9X5kn/2WUKelbY5Z6DHy3F06KKVCQkSWGGNSS7KNNuesqnYsDjzpn/kHd9IHW7UTSH/1mvSVqpQ08Vc1J47Z3io/HVn8ugC3zbQDkSulqg1N/FXJ4e3wWkc7GLlrUHN/et0LkTGwaSY0bF9x8SmlwoLe3K1K9m+wrwvf9i6/yme+5x1w/lNwxyyIqVkxsSmlwoYm/qpkzl/ta1a6d3mDdlDTo84+sXHFxaSUCjua+KuCE0fh+6fcTTOP7vVeHpMAw7+w01e8pd0fK1XNaR1/ZbJvA9RtbXvMbNAe6rex5eu+gQVjC98uNtF2pPbIFjvoiVKqWtPEX1nsWw+v94R+T7j7yH9oIxzZDbnF9MET43R6pklfKYVW9VQe66fZ153L3GVv94e3zoH0He4y8dO2XgcjV0p50MQf7vLzbTPNGc/Y+fVT3cvSneER57zkLus+wj39h/m2aaeOfKWU8qBVPeFu/t9g5vO23X3eieLXr9MSRv8OsbVswm/cKeghKqUqF0384W7zHPsaSNIHO85tXFLw4lFKVXpa1RPusjNKtn7NAPrYUUpVa3rFH86O7oddywNbt2lX241yx6uDG5NSqtIL6IpfRO4TkVpivSsiS0XkomAHV+0d3ee//KkD7ukLn7OvNRtCj1shQr/EKaWKFmiWGGmMyQAuAuoAw4GXit5ElZox9obu1vkFl9VKhsgoOyg5QNsBkDIMBr9asTEqpSqtQKt6XO0BLwE+NMasFtE2gkGx8QdIbAJzX3aXJfeAtEWQOhIG/d2WnXIWbJgGNevDFQH2u6+UUgSe+JeIyPdAK+AxEUkEAhilW5VI5h6YcJWttvGUlGwTf+4Jd5v8q9+B3Sv0aVylVIkFWtVzKzAa6GGMOQZEA7cELarq5ue3YPwgyM60876drLmezF0x2V0WmwAtelVMfEqpKiXQxH82sN4Yc1hEbgSeBNKL2UYVJ3MP7FoB3z4K2+bD90/6X++6D+3rHXMqLjalVJUVaFXPm0AXEekCPAi8A3wAnBeswKqF9y+F/evd8xumFVxn1Brbf/4Y/ZxVSpWPQK/4c40xBrgc+Jcx5nUgMXhhVXE7ltr+dzyTfmFq1g9+PEqpaiXQK/5MEXkM24zzXBGJwNbzq5LKOQ5v97PTkbGQl+1/vYEvwxmXQ1RsxcWmlKoWAk381wE3YNvz7xaR5sDLxWyjPO1ZDeP6QX6Ou8wz6ddKhow0O92ki+1lMyqmQkNUSlUPAVX1GGN2AxOBJBEZDGQZYz4oahsROUVEZonIGhFZLSL3lUO8ldebvWyiN4W0gr3pf9DpGrhtJtw5V5O+UipoArriF5FrsVf4s7EPc/1TRB42xnxaxGa5wIPGmKVOu/8lIjLdGLOmrEFXKr//Unh1DkDt5nD4d/t69TsVF5dSqtoKtKrnCWwb/r0AItIAmAEUmviNMbuAXc50poisBZoB1Svx/6eYLo0u+xe01sZRSqmKE2irnghX0nccKMG2iEhLoCvwS8CRVQea9JVSIRDoFf+3IvId8JEzfx3wTSAbikgC8F/gfqejN9/ldwB3ADRv3jzAcKqIzteGOgKlVDUUUOI3xjwsIlcDvZ2iccaYz4vbTkSisUl/ojHms0L2PQ4YB5CammoCirqyqnsq9HkIWp0H6du1qaZSKiQCHojFGPNfbBIPiNN757vAWmPM30sRW+U2Jgk6X+dddu0H0LijnU5qVvExKaUUxSR+EckE/F2FC2CMMbWK2Lw39oGvlSKyzCl73BgTUBVRpTbr/+zriinusgF/cSd9pZQKoSITvzGm1N0yGGPm4+7Hv/pY8yXM+Yt32cCXoeftoYlHKaV86Dh95ckY+Hh4wfIWvdz96CulVIjpYOvlad3X3vO3z4RVn0GD9qGJRyml/NDEX15yjsOUG93zj2614+I26x6ykJRSyh+t6ikP+fnwQmPvstii7nsrpVToaOIvD56tdwCufhciIkMTi1JKFUMTf3lYNtF7vtOQ0MShlFIB0MRfVtt+gq3z3POnXxa6WJRSKgCa+Msqc6f3/LVFDlOglFIhp616ymLCENi+0D3/5D5tr6+UCnua+Etrw/ewcbp7/oF1OmqWUqpS0Kqe0ji0FSZd411Wq0lIQlFKqZLSxF8aWene8/2fDE0cSilVClrVUxpfj3JPX/EWpAwNXSxKKVVCesVfUvl5sGOJe77NBaGLRSmlSkETf0l9dof3fEKD0MShlFKlpIm/JLIzYdWnoY5CKaXKROv4S2KCR1cMHYdAYuPC11VKqTCliT8QG2fYB7UObrLzcbVhyLuhjUkppUpJE38gJlztPf/g+tDEoZRS5UDr+Euq7QCIjgt1FEopVWqa+EvqhinFr6OUUmFME39x9m1wT1/ySujiUEqpcqKJvyjZmfB6D/d8m/NDF4tSSpUTTfxFeTHZe75u69DEoZRS5UgTf6DumB3qCJRSqlxo4i9M+g7v+aZdQxOHUkqVM038/mQfgVfPcM/3vKPwdZVSqpLRxO/P1Afd0/F14OIXQxeLUkqVM31y158Vk93Tj24NWRhKKRUMesXvK/uIe/qBtaGLQymlgkQTv68VHk/m1moaujiUUipINPF7On4Ypj5gp4d/HtpYlFIqSDTxe5p0nXu6ftvQxaGUUkGkid8lczds/9k9n5Rc+LpKKVWJaeJ3WfuVe/rBDYWvp5RSlZwmfpej++xr486Q2Ci0sSilVBBp4gc4dhDm/MVO3/p9aGNRSqkgC1riF5H/iMheEVkVrGOUmyXvuaej40MWhlJKVYRgXvG/BwwI4v7Lx49j4Yc/2+n7V4Y2FqWUqgBBS/zGmLnAwWDtv9xMf8o9XUtb8iilqr6Q1/GLyB0islhEFu/bt69iD77zV+/5iJCfDqWUCrqQZzpjzDhjTKoxJrVBgwYVd+D8fBjX1z3/pyUVd2yllAqhkCf+kNk00z3ddTjUbxO6WJRSqgJV38Sfvt2+NmgPl/8rtLEopVQFCmZzzo+An4B2IpImIrcG61il8vX99lXb7SulqpmgDcRijBkarH2X2cEt7um4pNDFoZRSIVD9qnqyM2Fsip2+4s3QxqKUUiFQ/RL/L/+2r1HxkHJDaGNRSqkQqF6Jf9E7MPM5Oz3y29DGopRSIVK9Ev/UB93TTVNCF4dSSoVQlUj8/1u2g2XbDxe90oFN7unbZxa+nlJKVXFBa9VTUbJy8rhv8jIAtr40qPAVNzhVO3fOhSZdKiAypZQKT5X+ij8uOrL4ldZPg/mvQtOudqAVpZSqxip94gdIlr0I+WTl5BVc+PvP8NH1doSts+4GkYoPUCmlwkilr+rh2EG+jh/D0pyWdH8qm9UvDbHludkw9QFY9Zmdv/EzaHN+6OJUSqkwUfkTf3wdNp3+R/qseolPYp4ld/4uojJ3wC/Ow1kNToehk6Bu69DGqZRSYaLyJ34ROl35MLf9mstfoscRNeNJWx5fFy56HroM1X72lVLKQ+VP/EBMVASmzYWcuSGFJI5gEFaMuTbUYSmlVFiqMpfCb93YHYB0EsigJi1HTyU718/NXqWUquaqTOKPj4lk8ZMXeJW1e/Jbdh4+HqKIlFIqPFWZxA9QPyGWDc8P9Crr9dJMpq7YFaKIlFIq/FSpxA+2vv/TP5ztVXb3pKXk5uWHKCKllAovVS7xA6S2rMuWFy/xKmvzxDR+3nwgRBEppVT4qJKJH0BE2PiCd7XP9eN+DlE0SikVPqps4geIiozgqz+d41W2Zf/REEWjlFLhoUonfoBOyUn88ri7q4Z+r8ym5eipfLY0LYRRKaVU6FT5xA/QMDGW0xomeJW98t36EEWjlFKhVS0Sv4jw3f19vMp2pmdx98Sl5OWbEEWllFKhUS0SP0BEhLD8mYu8yqau3MWy7YdCFJFSSoVGtUn8AEnx0SwY3d+r7Oo3f9I2/kqpaqVaJX6AprXj+c2nmWf352eEKBqllKp41S7xA0RHer/t9OM5DBo7T/v1UUpVC9Uy8QNMH9WHpwefcXJ+9c4Mer00k7fmbAphVEopFXzVNvGf1iiRkee0YvZDfb3KX5q2jg5Pf8uxE7mhCUwppYKs2iZ+l5b1a/LmsG5eZUdP5HHG09+dnP921S4WbNxf0aEppVRQVIkRuMpqQMfGfstbjp7qNf/eLT3o265hRYSklFJBU+2v+ME+4LX1pUE8MqBdkeuNGL+owIeBUkpVNpr4PfyxbxtW/fniYtd78ouVFRCNUkoFhxgTPl0WpKammsWLF4c6DABuGb+QuOhIpq3aXey69RNiGdSpMe//tI3vR/WhbaPECohQKaVARJYYY1JLtI0m/qKlHTrGOX+ZVert+7VrwJs3dicmMoIpi7fzwU/buKRjY+b9tp9HBrTj2Ik8+rRtUOQ+9mVmI2I/YJRSypMm/iA5cCSb7s/PoEFiLPsys8t9/+e1bcAVXZsyaspyAJrVjmfGA+fx2g8bOLV+Ao/8dwUAW18aRFZOHsbYweVdZq7bw7Lt6Yy64DREBIDcvHzSj+dQz/mwOHT0BEnx0URECOt2ZwDQvnGtArHsSj9OjZgokuKjy/19VpQZa/YQHxNJ7zb1Qx2KUkGnib8CbNx7hO9W7+bzX3ewce+RCj32Pf3b8M+ZG0/O92xVl/+7siMX/H3uybKbz27BU4PP4MkvVjF50Xa6t6jDkaxc1u/JLLC/U+rG8/wVnTjP+caRk5fPaU9MA2DlmItIjLPJf+v+o8RFR3LgaDZb9h9lcOemgP1w2bL/KNPX7uGiMxqxckc6nZNr8+PG/Ww/eIwnBp3Bhj2ZJMVH06hWnNexs3LyOH4ijzo1Ywp9v/4+5AAOHzvBm3M2USM6ij/1b0NkhHgtd92AX/fcACIjhKgIYfP+o7w+ayOfLd3B1/ecQ4QIZzQt+MFXHezNzOK9H7fyxuxN/PrUhYX+DowxGGM7OPSVmZVz8u+jJHYcPs57P25h1IVtqRGjjQrLgyb+EKuKLX7eHNaNuyYuLfN+PvnD2XRqlkRuvmH59sMMe+cXAGY8cB5b9h/l9g/s771NwwRuO6cVr87YwJ4M++3q6m7JJMVHc+d5rWlUK46uz37PoWM5ADx8cTuu6Z5MnZoxjHxvEcl14vlo4faTx21WO56UU2ozdeWuAjE9fHE7RvZuxdYDR1mzM4OuzWvTuoH3uA3GGPZmZnPPR7/yt2u60CQpjiiPLj+MMXy0cDu14qPYefg4B46eoHZ8DI2TYmmaFM+S3w+xemcGAzo05tIuTVm7K4NvVu6iY7Mk3p23hfdH9iQmKgJjDFGREaxIO8xl//qR+84/jfvOP81v0vUnP9+Q7+wD7Id1ZlYuHZvV4kh2LjViohDg81938OAny722nXbfuTSrE8+e9Czenb+FZy7tQHSk8NAny/li2U62vjQIgEv+MY81uzK8zl/68RzOa9vA69vVgo37+WXLQUZd2LZAnKc/9S3Hc/JOzn94a0/q1Yw9+SH84rS1/HvOZv53d2+6nFL75HrHT+SRbww1Y+2HxZ6MLNbtzqT3qfW8fh/+HDiSTb2EWGau28PI9xZzR5/WPHJxuwLbZeXkkZtvSIgt/gPp7klLOaNJLe7u16bYdf05ePQEezKyOL1J2S8+wi7xi8gA4B9AJPCOMealotav7Inf04JN+1m9I4Pb+7RmybaDjP9xK/uPZNMkKZ4xl3agy7PfhzpEVUqXdmnKV8t3lvt+k+KjST+eU+jyK7s2o2W9mrSoV4P7pywLaJ9x0RFk5djeZ2OiIjiRW7qeaDsnJ7EiLb3Q5Vd1a8b2g8dYtNV/N+cdm9Vi1Y4Mv8sK8/U951AjJpLlaYdPVoOe374hP6zbe3KdxNgo7rvgNJ6furbA9k2T4tiZnlXo/qMjhZw8w9mt6/HT5gN+1+nXrgGz1u/jprNbMKR7Mh8t3M5HC3/3u+7kO87i4NETDOzYmG9X7aZRUhwNE2N5YMpyFm496Hebb+8/lwGvzWNEr5aMuaxDobEWJawSv4hEAhuAC4E0YBEw1BizprBtqlLiD4Tr3C/edoipK3bx2CXt+XzpDt6YvYn+7RtyepNE+rRtwK70LJZvP8xL09aRnZtP41px7M7w/oOe8cB5vDNvM5MXbfd3KAAiI0QHnlEqTG18YWCx3178CbfEfzYwxhhzsTP/GIAx5sXCtqluib+sCquDzcrJIy468uT0Y5+t5IquzU7W5YOt512Zlk58dCSnN6lF1+em06dtAxZuOcBPo88nJy+fX7Yc5J6PfgXgzFZ1eXlIF1buSGfn4ePcdm4rXJ8hvV+aSb/2Ddi07ygHj544ee/jqq7NuKRTEz5a+LvXVRpAhMAr13ThgY/tldzpTWpxaZcmnN6kFreMXxTQ+29RrwbbDhwLaN2mSXHceHYLhp3Zgns++pXmdeOZ8LP/K7fq4NzT6lMrPpqpKwpWganQcFWplVS4Jf4hwABjzG3O/HDgTGPMn3zWuwO4A6B58+bdt23bFpR4VNUwd8M+WtarSfN6NYJ6nP1HsqlXM+ZkKymX/HzDviPZrNmZQXKdeKd+HprViT/Z3bcxpsB2efmG/UeyaVQrDmMM6cdzqF0jhsysHLJy8qmfEMPS3w+TFB9Fm4aJ5OcbNu8/Qo2YKJrWjic/3zBl8XZWpKVzNDuXId2T6XJK7QKtr7Jz8xDssfONYd5v+zmanUuXU2rz255MujavQ4NEd7PgnLx8jp3IIzcvn8S4aGKivK84Xe8lNy+ffEOB5Z7nZenvhziRm8++I9lcntKM/HzD36avp03DBCJEiImMoF/7hicvSlz7n79xP6c1TCQ+OpKkGvb97E7PYvraPZzIzaduzWjOP70RCTFRfLd6N91a1KFRrTh2HD5OvZoxzN2wjwaJsbSoV5NN+46wOz2Lc9rUJz4mkrx8930Bz2Mu/f0wp9SN5/OltpHGzb1aAtCxWRIHj54gOlKoGRNFRITw/erdNKsTT36+rTprWb8m0ZERLNi0n6T4aHanZ/H1il10b1GHNbsymPTL7zSuFceBo9nk5Nn8On5ED2KjI/h0SRqpLery+Ocr6XVqPRZsOsCfL+vAsDObl+pqHypp4vekV/xKKVUypUn8weyyYQdwisd8slOmlFIqhIKZ+BcBp4lIKxGJAa4Hvgzi8ZRSSgUgaE9QGGNyReRPwHfY5pz/McasDtbxlFJKBSaoj84ZY74BvgnmMZRSSpWMdsuslFLVjCZ+pZSqZjTxK6VUNaOJXymlqpmw6p1TRPYBpX10tz6wvxzDKU8aW+lobKWjsZVOZY2thTGm6NGcfIRV4i8LEVlc0qfXKorGVjoaW+lobKVTnWLTqh6llKpmNPErpVQ1U5US/7hQB1AEja10NLbS0dhKp9rEVmXq+JVSSgWmKl3xK6WUCoAmfqWUqmYqfeIXkQEisl5ENorI6BDFsFVEVorIMhFZ7JTVFZHpIvKb81rHKRcRGevEu0JEugUhnv+IyF4RWeVRVuJ4RORmZ/3fROTmIMY2RkR2OOdvmYhc4rHsMSe29SJysUd5uf7eReQUEZklImtEZLWI3OeUh/y8FRFbyM+bs884EVkoIsud+P7slLcSkV+cY01xumdHRGKd+Y3O8pbFxR2E2N4TkS0e5y7FKa/o/4dIEflVRL525ivmnNlxWyvnD7a7501AayAGWA6cEYI4tgL1fcr+Cox2pkcDf3GmLwGmAQKcBfwShHj6AN2AVaWNB6gLbHZe6zjTdYIU2xjgIT/rnuH8TmOBVs7vOjIYv3egCdDNmU4ENjjHD/l5KyK2kJ8353gCJDjT0cAvzjn5GLjeKX8LuMuZ/iPwljN9PTClqLiDFNt7wBA/61f0/8MDwCTga2e+Qs5ZZb/i7wlsNMZsNsacACYDl4c4JpfLgfed6feBKzzKPzDWz0BtEWlSngc2xswFDpYxnouB6caYg8aYQ8B0YECQYivM5cBkY0y2MWYLsBH7Oy/337sxZpcxZqkznQmsBZoRBuetiNgKU2HnzYnJGGOOOLPRzo8B+gOfOuW+5851Tj8FzhcRKSLuYMRWmAr7vYpIMjAIeMeZFyronFX2xN8M2O4xn0bR/xDBYoDvRWSJ2MHjARoZY3Y507uBRs50qGIuaTwVHeefnK/W/3FVp4QqNudrdFfs1WFYnTef2CBMzptTZbEM2ItNipuAw8aYXD/HOhmHszwdqBes+HxjM8a4zt0Lzrl7VURcI9BX5Ll7DXgEyHfm61FB56yyJ/5wcY4xphswELhbRPp4LjT2O1nYtJsNt3iAN4FTgRRgF/C3UAUiIgnAf4H7jTEZnstCfd78xBY2580Yk2eMScGOrd0TaB+qWHz5xiYiHYHHsDH2wFbfPFqRMYnIYGCvMWZJRR7XpbIn/rAY0N0Ys8N53Qt8jv3D3+OqwnFe9zqrhyrmksZTYXEaY/Y4/5z5wNu4v6pWaGwiEo1NrBONMZ85xWFx3vzFFi7nzZMx5jAwCzgbW03iGuXP81gn43CWJwEHgh2fR2wDnOozY4zJBsZT8eeuN3CZiGzFVrn1B/5BRZ2zst6cCOUPdujIzdibGq6bVR0qOIaaQKLH9AJs3d/LeN8U/KszPQjvm0cLgxRXS7xvoJYoHuxV0Bbsjaw6znTdIMXWxGN6FLbOEqAD3jeu0R+3tAAAAxZJREFUNmNvUJb77915/x8Ar/mUh/y8FRFbyM+bc7wGQG1nOh6YBwwGPsH7RuUfnem78b5R+XFRcQcptiYe5/Y14KUQ/j/0xX1zt0LOWbknnIr+wd6F34CtU3wiBMdv7Zz45cBqVwzY+rcfgN+AGa4/EucP6nUn3pVAahBi+gj71T8HW+d3a2niAUZibxZtBG4JYmwfOsdeAXyJd0J7woltPTAwWL934BxsNc4KYJnzc0k4nLciYgv5eXP22Rn41YljFfC0x//GQuc8fALEOuVxzvxGZ3nr4uIOQmwznXO3CpiAu+VPhf4/OPvtizvxV8g50y4blFKqmqnsdfxKKaVKSBO/UkpVM5r4lVKqmtHEr5RS1YwmfqWUqmY08StVBiLS19WzolKVhSZ+pZSqZjTxq2pBRG50+mVfJiL/djruOuJ00LVaRH4QkQbOuiki8rPTgdfn4u6Dv42IzHD6dl8qIqc6u08QkU9FZJ2ITHR6TUREXhLbh/4KEXklRG9dqQI08asqT0ROB64DehvbWVceMAzbxcZiY0wHYA7wjLPJB8CjxpjO2Kc3XeUTgdeNMV2AXtgnkMH2lnk/tm/01kBvEakHXIntEqEz8Hxw36VSgdPEr6qD84HuwCKne97zsQk6H5jirDMBOEdEkrB9u8xxyt8H+ohIItDMGPM5gDEmyxhzzFlnoTEmzdjO0pZh+yJKB7KAd0XkKsC1rlIhp4lfVQcCvG+MSXF+2hljxvhZr7T9l2R7TOcBUcb2md4TO2jGYODbUu5bqXKniV9VBz8AQ0SkIZwcR7cF9u9/iLPODcB8Y0w6cEhEznXKhwNzjB35Kk1ErnD2ESsiNQo7oNN3fpIx5htsz5ldgvHGlCqNqOJXUapyM8asEZEnsaOkRWB7Br0bOIodmONJbD/71zmb3Ay85ST2zcAtTvlw4N8i8qyzj2uKOGwi8D8RicN+43ignN+WUqWmvXOqaktEjhhjEkIdh1IVTat6lFKqmtErfqWUqmb0il8ppaoZTfxKKVXNaOJXSqlqRhO/UkpVM5r4lVKqmvl/g/EA0QpsmpMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibi9UwP-l8Cg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeyzlYfFmBWV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw_eWlb-mCDb"
      },
      "source": [
        "# Schizo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNXFz9F2l8Z_",
        "outputId": "1814b890-97fa-4e14-8330-8562b0cea35f"
      },
      "source": [
        "sch_exp_n = np.array(sch_exp).T\n",
        "sch_risk_n = np.array(sch_risk)\n",
        "np.mean(sch_risk_n), len(sch_risk_n)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6428571428571429, 882)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbpLs0hJl8aB"
      },
      "source": [
        "inputs_train, inputs_val, labels_train, labels_val = model_selection.train_test_split(sch_exp_n, sch_risk_n, test_size=0.25,\n",
        "                                                        random_state=0)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kw1fKjAl8aB"
      },
      "source": [
        "inputs_train = torch.tensor(inputs_train, dtype=torch.float32, requires_grad=True)\n",
        "inputs_val = torch.tensor(inputs_val, dtype=torch.float32, requires_grad=True)\n",
        "labels_train = torch.tensor(labels_train, dtype=torch.long)\n",
        "labels_val = torch.tensor(labels_val, dtype=torch.long)\n",
        "[inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, \n",
        " inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, \n",
        " labels_val.shape, labels_val.dtype]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmJRBun4l8aC"
      },
      "source": [
        "inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, labels_val.shape, labels_val.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfLVHLTwl8aD"
      },
      "source": [
        "## Init model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI882Naml8aE"
      },
      "source": [
        "sch_embed = CustomEmbedding(78, 100)\n",
        "sch_cla = SimpleNN(sch_embed, 100, 2)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(sch_cla.parameters())\n",
        "model = sch_cla\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPsSVxG5l8aE"
      },
      "source": [
        "## train for schizo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mbBCbQSl8aE",
        "outputId": "090fa652-818d-4d6b-e5ba-9cf5624a8c7a"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i = 0\n",
        "pre_df = pd.DataFrame(index = data.index)\n",
        "oddsratio_df = pd.DataFrame(index = np.arange(0.0,1.05,0.05))\n",
        "fi_df = pd.DataFrame()\n",
        "combine = sch_exp_n\n",
        "label = pd.DataFrame(sch_risk_n, columns=[\"labels\"])\n",
        "max_val_accs = []\n",
        "sch_embed_state_dicts = []\n",
        "for train, test in cv.split(sch_exp_n, label):\n",
        "    inputs_train = torch.tensor(sch_exp_n[train], dtype=torch.float32, requires_grad=True)\n",
        "    inputs_val = torch.tensor(sch_exp_n[test], dtype=torch.float32, requires_grad=True)\n",
        "    labels_train = torch.tensor(sch_risk_n[train], dtype=torch.long)\n",
        "    labels_val = torch.tensor(sch_risk_n[test], dtype=torch.long)\n",
        "\n",
        "    for i in range(5):\n",
        "        sch_embed = CustomEmbedding(78, 60)\n",
        "        sch_cla = SimpleNN(sch_embed, 60, 2)\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(sch_cla.parameters())\n",
        "        model = sch_cla\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "        num_epochs = 100000\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        print_freq = 1000\n",
        "        patience = 2000\n",
        "        patience_2 = patience\n",
        "        max_val_acc = 0\n",
        "        sch_embed_state_dict = None\n",
        "        for epoch in range(num_epochs):\n",
        "            # train for one epoch, printing every 10 iterations\n",
        "            if patience==0:\n",
        "                print(\"Training complete\")\n",
        "                break\n",
        "            model.train()\n",
        "            train_loss, train_acc = train_one_epoch(model, inputs_train, labels_train, loss_func, optimizer, device, acc_func=calc_accuracy)\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "\n",
        "            # evaluate on the test dataset\n",
        "            model.eval()\n",
        "            val_loss, accuracy, precision, recall, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "            patience -= 1\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                patience =  patience_2\n",
        "                sch_embed_state_dict = sch_embed.state_dict().copy()\n",
        "                print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}, {accuracy}\")\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            if epoch%print_freq==0:    \n",
        "                print(f\"Epoch =  {epoch} \\t Train loss = {train_loss:.5f} \\t  Train Acc = {train_acc:.5f} \\t\",\n",
        "                f\"Val loss = {val_loss:.5f} \\t Acc = {precision:.5f}{recall:.5f}{val_acc:.5f} \\t Patience = {patience}\")\n",
        "        max_val_accs.append(max_val_acc)\n",
        "        sch_embed_state_dicts.append(sch_embed_state_dict)\n",
        "\n",
        "print(\"Max val losses________________\")\n",
        "print(max_val_accs)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch =  0 \t Train loss = 0.76086 \t  Train Acc = 0.51915 \t Val loss = 0.70242 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 1 \t Max val Acc = 0.5257143378257751, 0.5310734510421753\n",
            "Epoch 2 \t Max val Acc = 0.663551390171051, 0.5932203531265259\n",
            "Epoch 3 \t Max val Acc = 0.6929824352264404, 0.604519784450531\n",
            "Epoch 4 \t Max val Acc = 0.7136929035186768, 0.6101694703102112\n",
            "Epoch 5 \t Max val Acc = 0.7177419066429138, 0.604519784450531\n",
            "Epoch 7 \t Max val Acc = 0.7364341020584106, 0.6158192157745361\n",
            "Epoch 8 \t Max val Acc = 0.7573529481887817, 0.6271186470985413\n",
            "Epoch 9 \t Max val Acc = 0.7591240406036377, 0.6271186470985413\n",
            "Epoch 10 \t Max val Acc = 0.7681159377098083, 0.6384180784225464\n",
            "Epoch 11 \t Max val Acc = 0.7857142686843872, 0.6610169410705566\n",
            "Epoch 12 \t Max val Acc = 0.7885305285453796, 0.6666666865348816\n",
            "Epoch 13 \t Max val Acc = 0.7956989407539368, 0.6779661178588867\n",
            "Epoch 327 \t Max val Acc = 0.796875, 0.7062146663665771\n",
            "Epoch 329 \t Max val Acc = 0.801556408405304, 0.7118644118309021\n",
            "Epoch 592 \t Max val Acc = 0.8015872836112976, 0.7175140976905823\n",
            "Epoch 828 \t Max val Acc = 0.8032127618789673, 0.7231638431549072\n",
            "Epoch 969 \t Max val Acc = 0.8063240051269531, 0.7231638431549072\n",
            "Epoch =  1000 \t Train loss = 0.44792 \t  Train Acc = 0.79007 \t Val loss = 1.05416 \t Acc = 0.716310.885960.79216 \t Patience = 1969\n",
            "Epoch 1144 \t Max val Acc = 0.8095238208770752, 0.7288135886192322\n",
            "Epoch =  2000 \t Train loss = 0.40256 \t  Train Acc = 0.82979 \t Val loss = 1.37146 \t Acc = 0.692310.868420.77043 \t Patience = 1144\n",
            "Epoch =  3000 \t Train loss = 0.34570 \t  Train Acc = 0.84823 \t Val loss = 1.57095 \t Acc = 0.686130.824560.74900 \t Patience = 144\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.04999999701976776, 0.35593220591545105\n",
            "Epoch =  0 \t Train loss = 0.74086 \t  Train Acc = 0.49220 \t Val loss = 0.69803 \t Acc = 0.500000.026320.05000 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.6388888955116272, 0.5593220591545105\n",
            "Epoch 2 \t Max val Acc = 0.6724890470504761, 0.5762711763381958\n",
            "Epoch 3 \t Max val Acc = 0.6779661178588867, 0.5706214904785156\n",
            "Epoch 4 \t Max val Acc = 0.6833333373069763, 0.5706214904785156\n",
            "Epoch 5 \t Max val Acc = 0.6859503984451294, 0.5706214904785156\n",
            "Epoch 6 \t Max val Acc = 0.6913580298423767, 0.5762711763381958\n",
            "Epoch 9 \t Max val Acc = 0.6963562965393066, 0.5762711763381958\n",
            "Epoch 12 \t Max val Acc = 0.7016128301620483, 0.5819209218025208\n",
            "Epoch 16 \t Max val Acc = 0.707818865776062, 0.598870038986206\n",
            "Epoch 17 \t Max val Acc = 0.7295082211494446, 0.6271186470985413\n",
            "Epoch 19 \t Max val Acc = 0.730290412902832, 0.6327683329582214\n",
            "Epoch 20 \t Max val Acc = 0.7416666150093079, 0.6497175097465515\n",
            "Epoch 54 \t Max val Acc = 0.7426159977912903, 0.6553672552108765\n",
            "Epoch 57 \t Max val Acc = 0.7603305578231812, 0.6723163723945618\n",
            "Epoch 58 \t Max val Acc = 0.7634854912757874, 0.6779661178588867\n",
            "Epoch 84 \t Max val Acc = 0.7692307233810425, 0.6779661178588867\n",
            "Epoch 97 \t Max val Acc = 0.7698412537574768, 0.6723163723945618\n",
            "Epoch 98 \t Max val Acc = 0.7747035622596741, 0.6779661178588867\n",
            "Epoch 99 \t Max val Acc = 0.7795275449752808, 0.6836158037185669\n",
            "Epoch 101 \t Max val Acc = 0.78125, 0.6836158037185669\n",
            "Epoch 120 \t Max val Acc = 0.7829457521438599, 0.6836158037185669\n",
            "Epoch 131 \t Max val Acc = 0.7846153974533081, 0.6836158037185669\n",
            "Epoch 162 \t Max val Acc = 0.7849056124687195, 0.6779661178588867\n",
            "Epoch 163 \t Max val Acc = 0.7865168452262878, 0.6779661178588867\n",
            "Epoch 164 \t Max val Acc = 0.7970479726791382, 0.6892655491828918\n",
            "Epoch 165 \t Max val Acc = 0.8014705181121826, 0.694915235042572\n",
            "Epoch 187 \t Max val Acc = 0.8045113682746887, 0.7062146663665771\n",
            "Epoch 188 \t Max val Acc = 0.8089887499809265, 0.7118644118309021\n",
            "Epoch =  1000 \t Train loss = 0.44837 \t  Train Acc = 0.79574 \t Val loss = 0.99606 \t Acc = 0.713240.850880.77600 \t Patience = 1188\n",
            "Epoch 1090 \t Max val Acc = 0.8091602921485901, 0.7175140976905823\n",
            "Epoch 1317 \t Max val Acc = 0.8110235929489136, 0.7288135886192322\n",
            "Epoch =  2000 \t Train loss = 0.37279 \t  Train Acc = 0.83546 \t Val loss = 0.95440 \t Acc = 0.711270.885960.78906 \t Patience = 1317\n",
            "Epoch =  3000 \t Train loss = 0.35511 \t  Train Acc = 0.84539 \t Val loss = 1.30519 \t Acc = 0.701390.885960.78295 \t Patience = 317\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7209302186965942, 0.5932203531265259\n",
            "Epoch =  0 \t Train loss = 0.73360 \t  Train Acc = 0.51064 \t Val loss = 0.68860 \t Acc = 0.645830.815790.72093 \t Patience = 2000\n",
            "Epoch 25 \t Max val Acc = 0.7280334830284119, 0.6327683329582214\n",
            "Epoch 60 \t Max val Acc = 0.7288135290145874, 0.6384180784225464\n",
            "Epoch 61 \t Max val Acc = 0.7394957542419434, 0.6497175097465515\n",
            "Epoch 62 \t Max val Acc = 0.7520660758018494, 0.6610169410705566\n",
            "Epoch 91 \t Max val Acc = 0.7569721341133118, 0.6553672552108765\n",
            "Epoch 104 \t Max val Acc = 0.7599999904632568, 0.6610169410705566\n",
            "Epoch 106 \t Max val Acc = 0.7649402022361755, 0.6666666865348816\n",
            "Epoch 121 \t Max val Acc = 0.7716535329818726, 0.6723163723945618\n",
            "Epoch 123 \t Max val Acc = 0.7764706015586853, 0.6779661178588867\n",
            "Epoch 171 \t Max val Acc = 0.7849056124687195, 0.6779661178588867\n",
            "Epoch 220 \t Max val Acc = 0.7937743067741394, 0.700564980506897\n",
            "Epoch 275 \t Max val Acc = 0.7984496355056763, 0.7062146663665771\n",
            "Epoch 304 \t Max val Acc = 0.7985075116157532, 0.694915235042572\n",
            "Epoch 319 \t Max val Acc = 0.8106060028076172, 0.7175140976905823\n",
            "Epoch =  1000 \t Train loss = 0.42694 \t  Train Acc = 0.80000 \t Val loss = 1.17086 \t Acc = 0.683100.850880.75781 \t Patience = 1319\n",
            "Epoch =  2000 \t Train loss = 0.38623 \t  Train Acc = 0.82411 \t Val loss = 1.41163 \t Acc = 0.687940.850880.76078 \t Patience = 319\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.03361344709992409, 0.3502824902534485\n",
            "Epoch =  0 \t Train loss = 0.83095 \t  Train Acc = 0.47376 \t Val loss = 0.71879 \t Acc = 0.400000.017540.03361 \t Patience = 2000\n",
            "Epoch 6 \t Max val Acc = 0.4844720661640167, 0.5310734510421753\n",
            "Epoch 7 \t Max val Acc = 0.4939759373664856, 0.5254237055778503\n",
            "Epoch 8 \t Max val Acc = 0.529411792755127, 0.5480226278305054\n",
            "Epoch 9 \t Max val Acc = 0.5423728823661804, 0.5423728823661804\n",
            "Epoch 10 \t Max val Acc = 0.5683059692382812, 0.5536723136901855\n",
            "Epoch 11 \t Max val Acc = 0.5913978219032288, 0.5706214904785156\n",
            "Epoch 12 \t Max val Acc = 0.6250000596046448, 0.5932203531265259\n",
            "Epoch 13 \t Max val Acc = 0.6565656065940857, 0.6158192157745361\n",
            "Epoch 16 \t Max val Acc = 0.6571428179740906, 0.5932203531265259\n",
            "Epoch 18 \t Max val Acc = 0.6729857921600342, 0.6101694703102112\n",
            "Epoch 20 \t Max val Acc = 0.6854459643363953, 0.6214689016342163\n",
            "Epoch 21 \t Max val Acc = 0.6886792182922363, 0.6271186470985413\n",
            "Epoch 43 \t Max val Acc = 0.6912442445755005, 0.6214689016342163\n",
            "Epoch 46 \t Max val Acc = 0.699999988079071, 0.6271186470985413\n",
            "Epoch 48 \t Max val Acc = 0.7027027010917664, 0.6271186470985413\n",
            "Epoch 49 \t Max val Acc = 0.7142857909202576, 0.6384180784225464\n",
            "Epoch 50 \t Max val Acc = 0.7200000286102295, 0.6440678238868713\n",
            "Epoch 62 \t Max val Acc = 0.7248908281326294, 0.6440678238868713\n",
            "Epoch 65 \t Max val Acc = 0.7319148778915405, 0.6440678238868713\n",
            "Epoch 70 \t Max val Acc = 0.732758641242981, 0.6497175097465515\n",
            "Epoch 80 \t Max val Acc = 0.7359308004379272, 0.6553672552108765\n",
            "Epoch 82 \t Max val Acc = 0.739130437374115, 0.6610169410705566\n",
            "Epoch 97 \t Max val Acc = 0.7460317611694336, 0.6384180784225464\n",
            "Epoch 133 \t Max val Acc = 0.7519999742507935, 0.6497175097465515\n",
            "Epoch 145 \t Max val Acc = 0.7567567825317383, 0.6440678238868713\n",
            "Epoch 146 \t Max val Acc = 0.7633588314056396, 0.6497175097465515\n",
            "Epoch 148 \t Max val Acc = 0.7640449404716492, 0.6440678238868713\n",
            "Epoch 149 \t Max val Acc = 0.7651515603065491, 0.6497175097465515\n",
            "Epoch 150 \t Max val Acc = 0.7662835717201233, 0.6553672552108765\n",
            "Epoch 168 \t Max val Acc = 0.7722007632255554, 0.6666666865348816\n",
            "Epoch 222 \t Max val Acc = 0.7727272510528564, 0.6610169410705566\n",
            "Epoch 239 \t Max val Acc = 0.7761194109916687, 0.6610169410705566\n",
            "Epoch 240 \t Max val Acc = 0.7790261507034302, 0.6666666865348816\n",
            "Epoch 250 \t Max val Acc = 0.7819548845291138, 0.6723163723945618\n",
            "Epoch 330 \t Max val Acc = 0.7846153974533081, 0.6836158037185669\n",
            "Epoch 340 \t Max val Acc = 0.7892720103263855, 0.6892655491828918\n",
            "Epoch 424 \t Max val Acc = 0.7923076748847961, 0.694915235042572\n",
            "Epoch 591 \t Max val Acc = 0.800000011920929, 0.7062146663665771\n",
            "Epoch 592 \t Max val Acc = 0.801556408405304, 0.7118644118309021\n",
            "Epoch =  1000 \t Train loss = 0.43127 \t  Train Acc = 0.79574 \t Val loss = 0.83584 \t Acc = 0.710140.859650.77778 \t Patience = 1592\n",
            "Epoch =  2000 \t Train loss = 0.37684 \t  Train Acc = 0.82979 \t Val loss = 1.08654 \t Acc = 0.702900.850880.76984 \t Patience = 592\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.03361344709992409, 0.3502824902534485\n",
            "Epoch =  0 \t Train loss = 0.78297 \t  Train Acc = 0.47660 \t Val loss = 0.73603 \t Acc = 0.400000.017540.03361 \t Patience = 2000\n",
            "Epoch 8 \t Max val Acc = 0.3918919265270233, 0.49152541160583496\n",
            "Epoch 9 \t Max val Acc = 0.48192769289016724, 0.5141242742538452\n",
            "Epoch 10 \t Max val Acc = 0.5423728823661804, 0.5423728823661804\n",
            "Epoch 11 \t Max val Acc = 0.5913978219032288, 0.5706214904785156\n",
            "Epoch 13 \t Max val Acc = 0.5979381203651428, 0.5593220591545105\n",
            "Epoch 15 \t Max val Acc = 0.6091370582580566, 0.5649717450141907\n",
            "Epoch 16 \t Max val Acc = 0.616161584854126, 0.5706214904785156\n",
            "Epoch 17 \t Max val Acc = 0.623115599155426, 0.5762711763381958\n",
            "Epoch 18 \t Max val Acc = 0.6403940320014954, 0.5875706076622009\n",
            "Epoch 20 \t Max val Acc = 0.6536585688591003, 0.598870038986206\n",
            "Epoch 22 \t Max val Acc = 0.6699028611183167, 0.6158192157745361\n",
            "Epoch 23 \t Max val Acc = 0.6763284206390381, 0.6214689016342163\n",
            "Epoch 24 \t Max val Acc = 0.6796116828918457, 0.6271186470985413\n",
            "Epoch 25 \t Max val Acc = 0.6893203854560852, 0.6384180784225464\n",
            "Epoch 29 \t Max val Acc = 0.691588819026947, 0.6271186470985413\n",
            "Epoch 30 \t Max val Acc = 0.6976743936538696, 0.6327683329582214\n",
            "Epoch 31 \t Max val Acc = 0.7004607915878296, 0.6327683329582214\n",
            "Epoch 38 \t Max val Acc = 0.7027027010917664, 0.6271186470985413\n",
            "Epoch 39 \t Max val Acc = 0.7058823704719543, 0.6327683329582214\n",
            "Epoch 40 \t Max val Acc = 0.7090909481048584, 0.6384180784225464\n",
            "Epoch 65 \t Max val Acc = 0.7117117047309875, 0.6384180784225464\n",
            "Epoch 69 \t Max val Acc = 0.719298243522644, 0.6384180784225464\n",
            "Epoch 71 \t Max val Acc = 0.7248908281326294, 0.6440678238868713\n",
            "Epoch 89 \t Max val Acc = 0.730290412902832, 0.6327683329582214\n",
            "Epoch 91 \t Max val Acc = 0.7407407760620117, 0.6440678238868713\n",
            "Epoch 92 \t Max val Acc = 0.7459016442298889, 0.6497175097465515\n",
            "Epoch 93 \t Max val Acc = 0.7661290168762207, 0.6723163723945618\n",
            "Epoch 95 \t Max val Acc = 0.7710843086242676, 0.6779661178588867\n",
            "Epoch 98 \t Max val Acc = 0.7729083895683289, 0.6779661178588867\n",
            "Epoch 147 \t Max val Acc = 0.7761194109916687, 0.6610169410705566\n",
            "Epoch 148 \t Max val Acc = 0.7806692123413086, 0.6666666865348816\n",
            "Epoch 149 \t Max val Acc = 0.7835820913314819, 0.6723163723945618\n",
            "Epoch 157 \t Max val Acc = 0.7883211970329285, 0.6723163723945618\n",
            "Epoch 183 \t Max val Acc = 0.7941176295280457, 0.6836158037185669\n",
            "Epoch 332 \t Max val Acc = 0.7984496355056763, 0.7062146663665771\n",
            "Epoch 354 \t Max val Acc = 0.8000000715255737, 0.694915235042572\n",
            "Epoch 392 \t Max val Acc = 0.8045113682746887, 0.7062146663665771\n",
            "Epoch 425 \t Max val Acc = 0.8062015771865845, 0.7175140976905823\n",
            "Epoch =  1000 \t Train loss = 0.44842 \t  Train Acc = 0.81135 \t Val loss = 1.01689 \t Acc = 0.708030.850880.77291 \t Patience = 1425\n",
            "Epoch =  2000 \t Train loss = 0.40412 \t  Train Acc = 0.82695 \t Val loss = 1.19364 \t Acc = 0.709220.877190.78431 \t Patience = 425\n",
            "Epoch 2248 \t Max val Acc = 0.8130081295967102, 0.7401130199432373\n",
            "Epoch 2249 \t Max val Acc = 0.8145161271095276, 0.7401130199432373\n",
            "Epoch =  3000 \t Train loss = 0.36616 \t  Train Acc = 0.83830 \t Val loss = 1.12427 \t Acc = 0.690650.842110.75889 \t Patience = 1249\n",
            "Epoch =  4000 \t Train loss = 0.32906 \t  Train Acc = 0.86241 \t Val loss = 1.19002 \t Acc = 0.714290.833330.76923 \t Patience = 249\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.109375, 0.35593220591545105\n",
            "Epoch =  0 \t Train loss = 0.77232 \t  Train Acc = 0.48511 \t Val loss = 0.71351 \t Acc = 0.500000.061400.10938 \t Patience = 2000\n",
            "Epoch 15 \t Max val Acc = 0.380952388048172, 0.4858756959438324\n",
            "Epoch 16 \t Max val Acc = 0.42105263471603394, 0.5028248429298401\n",
            "Epoch 18 \t Max val Acc = 0.4487179219722748, 0.5141242742538452\n",
            "Epoch 19 \t Max val Acc = 0.4596273601055145, 0.508474588394165\n",
            "Epoch 20 \t Max val Acc = 0.4691358208656311, 0.5141242742538452\n",
            "Epoch 21 \t Max val Acc = 0.5060241222381592, 0.5367231369018555\n",
            "Epoch 22 \t Max val Acc = 0.5433526039123535, 0.5536723136901855\n",
            "Epoch 23 \t Max val Acc = 0.5635359287261963, 0.5536723136901855\n",
            "Epoch 24 \t Max val Acc = 0.5652174353599548, 0.5480226278305054\n",
            "Epoch 25 \t Max val Acc = 0.5729729533195496, 0.5536723136901855\n",
            "Epoch 26 \t Max val Acc = 0.5820105671882629, 0.5536723136901855\n",
            "Epoch 28 \t Max val Acc = 0.6020408272743225, 0.5593220591545105\n",
            "Epoch 30 \t Max val Acc = 0.6138613820075989, 0.5593220591545105\n",
            "Epoch 31 \t Max val Acc = 0.6274510025978088, 0.5706214904785156\n",
            "Epoch 32 \t Max val Acc = 0.6376811861991882, 0.5762711763381958\n",
            "Epoch 33 \t Max val Acc = 0.6411483287811279, 0.5762711763381958\n",
            "Epoch 41 \t Max val Acc = 0.6415094137191772, 0.5706214904785156\n",
            "Epoch 42 \t Max val Acc = 0.6478873491287231, 0.5762711763381958\n",
            "Epoch 43 \t Max val Acc = 0.6542055606842041, 0.5819209218025208\n",
            "Epoch 51 \t Max val Acc = 0.6543778777122498, 0.5762711763381958\n",
            "Epoch 59 \t Max val Acc = 0.6545454859733582, 0.5706214904785156\n",
            "Epoch 61 \t Max val Acc = 0.6636363863945007, 0.5819209218025208\n",
            "Epoch 62 \t Max val Acc = 0.6696832776069641, 0.5875706076622009\n",
            "Epoch 66 \t Max val Acc = 0.6697247624397278, 0.5932203531265259\n",
            "Epoch 73 \t Max val Acc = 0.6727272868156433, 0.5932203531265259\n",
            "Epoch 83 \t Max val Acc = 0.6902655363082886, 0.604519784450531\n",
            "Epoch 98 \t Max val Acc = 0.6960352659225464, 0.6101694703102112\n",
            "Epoch 99 \t Max val Acc = 0.6986899375915527, 0.6101694703102112\n",
            "Epoch 104 \t Max val Acc = 0.701298713684082, 0.6101694703102112\n",
            "Epoch 105 \t Max val Acc = 0.703862726688385, 0.6101694703102112\n",
            "Epoch 106 \t Max val Acc = 0.7099567651748657, 0.6214689016342163\n",
            "Epoch 108 \t Max val Acc = 0.714893639087677, 0.6214689016342163\n",
            "Epoch 109 \t Max val Acc = 0.7196653485298157, 0.6214689016342163\n",
            "Epoch 110 \t Max val Acc = 0.7250000238418579, 0.6271186470985413\n",
            "Epoch 111 \t Max val Acc = 0.7272727489471436, 0.6271186470985413\n",
            "Epoch 124 \t Max val Acc = 0.7346938848495483, 0.6327683329582214\n",
            "Epoch 126 \t Max val Acc = 0.7368420362472534, 0.6327683329582214\n",
            "Epoch 132 \t Max val Acc = 0.7410358786582947, 0.6327683329582214\n",
            "Epoch 133 \t Max val Acc = 0.7509881258010864, 0.6440678238868713\n",
            "Epoch 144 \t Max val Acc = 0.751937985420227, 0.6384180784225464\n",
            "Epoch 146 \t Max val Acc = 0.7586207389831543, 0.6440678238868713\n",
            "Epoch 147 \t Max val Acc = 0.7604563236236572, 0.6440678238868713\n",
            "Epoch 149 \t Max val Acc = 0.7651515603065491, 0.6497175097465515\n",
            "Epoch 175 \t Max val Acc = 0.765625, 0.6610169410705566\n",
            "Epoch 176 \t Max val Acc = 0.7667983770370483, 0.6666666865348816\n",
            "Epoch 181 \t Max val Acc = 0.7716535329818726, 0.6723163723945618\n",
            "Epoch 205 \t Max val Acc = 0.7799227833747864, 0.6779661178588867\n",
            "Epoch 256 \t Max val Acc = 0.7810218334197998, 0.6610169410705566\n",
            "Epoch 257 \t Max val Acc = 0.7838827967643738, 0.6666666865348816\n",
            "Epoch 307 \t Max val Acc = 0.7878788113594055, 0.6836158037185669\n",
            "Epoch 320 \t Max val Acc = 0.7892720103263855, 0.6892655491828918\n",
            "Epoch 335 \t Max val Acc = 0.7940074801445007, 0.6892655491828918\n",
            "Epoch 397 \t Max val Acc = 0.7955389618873596, 0.6892655491828918\n",
            "Epoch 398 \t Max val Acc = 0.8014981150627136, 0.700564980506897\n",
            "Epoch 399 \t Max val Acc = 0.8059702515602112, 0.7062146663665771\n",
            "Epoch 400 \t Max val Acc = 0.8089887499809265, 0.7118644118309021\n",
            "Epoch 459 \t Max val Acc = 0.810408890247345, 0.7118644118309021\n",
            "Epoch 460 \t Max val Acc = 0.8148148059844971, 0.7175140976905823\n",
            "Epoch 585 \t Max val Acc = 0.8178438544273376, 0.7231638431549072\n",
            "Epoch 800 \t Max val Acc = 0.8195488452911377, 0.7288135886192322\n",
            "Epoch 801 \t Max val Acc = 0.8226415514945984, 0.7344632744789124\n",
            "Epoch =  1000 \t Train loss = 0.44774 \t  Train Acc = 0.77589 \t Val loss = 0.79042 \t Acc = 0.692810.929820.79401 \t Patience = 1801\n",
            "Epoch =  2000 \t Train loss = 0.40148 \t  Train Acc = 0.82270 \t Val loss = 1.01140 \t Acc = 0.713290.894740.79377 \t Patience = 801\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7835052013397217, 0.6440678238868713\n",
            "Epoch =  0 \t Train loss = 0.72743 \t  Train Acc = 0.52482 \t Val loss = 0.66857 \t Acc = 0.644071.000000.78351 \t Patience = 2000\n",
            "Epoch 17 \t Max val Acc = 0.7847222089767456, 0.6497175097465515\n",
            "Epoch 29 \t Max val Acc = 0.7859649658203125, 0.6553672552108765\n",
            "Epoch 38 \t Max val Acc = 0.7870036363601685, 0.6666666865348816\n",
            "Epoch 134 \t Max val Acc = 0.7955389618873596, 0.6892655491828918\n",
            "Epoch 265 \t Max val Acc = 0.7969925403594971, 0.694915235042572\n",
            "Epoch 369 \t Max val Acc = 0.7985075116157532, 0.694915235042572\n",
            "Epoch 425 \t Max val Acc = 0.8014981150627136, 0.700564980506897\n",
            "Epoch 553 \t Max val Acc = 0.8015267252922058, 0.7062146663665771\n",
            "Epoch 592 \t Max val Acc = 0.807692289352417, 0.7175140976905823\n",
            "Epoch 749 \t Max val Acc = 0.8185327649116516, 0.7344632744789124\n",
            "Epoch =  1000 \t Train loss = 0.45468 \t  Train Acc = 0.78440 \t Val loss = 1.03505 \t Acc = 0.708610.938600.80755 \t Patience = 1749\n",
            "Epoch 1641 \t Max val Acc = 0.8195488452911377, 0.7288135886192322\n",
            "Epoch 1642 \t Max val Acc = 0.8226415514945984, 0.7344632744789124\n",
            "Epoch =  2000 \t Train loss = 0.38824 \t  Train Acc = 0.83262 \t Val loss = 1.14347 \t Acc = 0.681530.938600.78967 \t Patience = 1642\n",
            "Epoch =  3000 \t Train loss = 0.34769 \t  Train Acc = 0.83830 \t Val loss = 1.05782 \t Acc = 0.684210.912280.78195 \t Patience = 642\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.77102 \t  Train Acc = 0.50638 \t Val loss = 0.73559 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 8 \t Max val Acc = 0.017391305416822433, 0.3615819215774536\n",
            "Epoch 13 \t Max val Acc = 0.34246575832366943, 0.4576271176338196\n",
            "Epoch 14 \t Max val Acc = 0.41025638580322266, 0.48022598028182983\n",
            "Epoch 15 \t Max val Acc = 0.4375, 0.49152541160583496\n",
            "Epoch 16 \t Max val Acc = 0.5, 0.5254237055778503\n",
            "Epoch 17 \t Max val Acc = 0.5517241358757019, 0.5593220591545105\n",
            "Epoch 18 \t Max val Acc = 0.5604395866394043, 0.5480226278305054\n",
            "Epoch 19 \t Max val Acc = 0.5820105671882629, 0.5536723136901855\n",
            "Epoch 21 \t Max val Acc = 0.5989847779273987, 0.5536723136901855\n",
            "Epoch 22 \t Max val Acc = 0.6138613820075989, 0.5593220591545105\n",
            "Epoch 23 \t Max val Acc = 0.6310679316520691, 0.5706214904785156\n",
            "Epoch 24 \t Max val Acc = 0.6346153616905212, 0.5706214904785156\n",
            "Epoch 25 \t Max val Acc = 0.6445497870445251, 0.5762711763381958\n",
            "Epoch 27 \t Max val Acc = 0.6509434580802917, 0.5819209218025208\n",
            "Epoch 29 \t Max val Acc = 0.6542055606842041, 0.5819209218025208\n",
            "Epoch 33 \t Max val Acc = 0.6604651808738708, 0.5875706076622009\n",
            "Epoch 44 \t Max val Acc = 0.6635944247245789, 0.5875706076622009\n",
            "Epoch 68 \t Max val Acc = 0.6666666865348816, 0.5875706076622009\n",
            "Epoch 70 \t Max val Acc = 0.6787330508232117, 0.598870038986206\n",
            "Epoch 81 \t Max val Acc = 0.6872246861457825, 0.598870038986206\n",
            "Epoch 82 \t Max val Acc = 0.6926407217979431, 0.598870038986206\n",
            "Epoch 83 \t Max val Acc = 0.6982759237289429, 0.604519784450531\n",
            "Epoch 93 \t Max val Acc = 0.7355372309684753, 0.6384180784225464\n",
            "Epoch 95 \t Max val Acc = 0.7490039467811584, 0.6440678238868713\n",
            "Epoch 96 \t Max val Acc = 0.768627405166626, 0.6666666865348816\n",
            "Epoch 143 \t Max val Acc = 0.7692307233810425, 0.6779661178588867\n",
            "Epoch 145 \t Max val Acc = 0.7710843086242676, 0.6779661178588867\n",
            "Epoch 154 \t Max val Acc = 0.7722007632255554, 0.6666666865348816\n",
            "Epoch 157 \t Max val Acc = 0.7803030610084534, 0.6723163723945618\n",
            "Epoch 158 \t Max val Acc = 0.7878788113594055, 0.6836158037185669\n",
            "Epoch 161 \t Max val Acc = 0.7969925403594971, 0.694915235042572\n",
            "Epoch 167 \t Max val Acc = 0.7985075116157532, 0.694915235042572\n",
            "Epoch 327 \t Max val Acc = 0.7999999523162842, 0.6892655491828918\n",
            "Epoch 334 \t Max val Acc = 0.8014705181121826, 0.694915235042572\n",
            "Epoch 335 \t Max val Acc = 0.8088235259056091, 0.7062146663665771\n",
            "Epoch 459 \t Max val Acc = 0.8102189898490906, 0.7062146663665771\n",
            "Epoch 660 \t Max val Acc = 0.8106060028076172, 0.7175140976905823\n",
            "Epoch 667 \t Max val Acc = 0.8134328126907349, 0.7175140976905823\n",
            "Epoch 686 \t Max val Acc = 0.8181818723678589, 0.7288135886192322\n",
            "Epoch 869 \t Max val Acc = 0.8191881775856018, 0.7231638431549072\n",
            "Epoch 886 \t Max val Acc = 0.8199233412742615, 0.7344632744789124\n",
            "Epoch =  1000 \t Train loss = 0.44293 \t  Train Acc = 0.79291 \t Val loss = 1.69063 \t Acc = 0.692810.929820.79401 \t Patience = 1886\n",
            "Epoch =  2000 \t Train loss = 0.37129 \t  Train Acc = 0.83546 \t Val loss = 1.91573 \t Acc = 0.697370.929820.79699 \t Patience = 886\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.24489794671535492, 0.37288135290145874\n",
            "Epoch =  0 \t Train loss = 0.76318 \t  Train Acc = 0.49078 \t Val loss = 0.69897 \t Acc = 0.545450.157890.24490 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.4842105507850647, 0.44632768630981445\n",
            "Epoch 3 \t Max val Acc = 0.691983163356781, 0.5875706076622009\n",
            "Epoch 4 \t Max val Acc = 0.6978723406791687, 0.598870038986206\n",
            "Epoch 5 \t Max val Acc = 0.7088607549667358, 0.6101694703102112\n",
            "Epoch 81 \t Max val Acc = 0.7124463319778442, 0.6214689016342163\n",
            "Epoch 84 \t Max val Acc = 0.714893639087677, 0.6214689016342163\n",
            "Epoch 85 \t Max val Acc = 0.7203389406204224, 0.6271186470985413\n",
            "Epoch 86 \t Max val Acc = 0.7394957542419434, 0.6497175097465515\n",
            "Epoch 87 \t Max val Acc = 0.7438016533851624, 0.6497175097465515\n",
            "Epoch 88 \t Max val Acc = 0.7591835856437683, 0.6666666865348816\n",
            "Epoch 92 \t Max val Acc = 0.7619048357009888, 0.6610169410705566\n",
            "Epoch 94 \t Max val Acc = 0.7649402022361755, 0.6666666865348816\n",
            "Epoch 144 \t Max val Acc = 0.765625, 0.6610169410705566\n",
            "Epoch 155 \t Max val Acc = 0.7662835717201233, 0.6553672552108765\n",
            "Epoch 156 \t Max val Acc = 0.7674418091773987, 0.6610169410705566\n",
            "Epoch 157 \t Max val Acc = 0.7704280018806458, 0.6666666865348816\n",
            "Epoch 159 \t Max val Acc = 0.7716535329818726, 0.6723163723945618\n",
            "Epoch 160 \t Max val Acc = 0.7734375, 0.6723163723945618\n",
            "Epoch 162 \t Max val Acc = 0.7739464044570923, 0.6666666865348816\n",
            "Epoch 168 \t Max val Acc = 0.7751937508583069, 0.6723163723945618\n",
            "Epoch 211 \t Max val Acc = 0.7769230604171753, 0.6723163723945618\n",
            "Epoch 214 \t Max val Acc = 0.7786259651184082, 0.6723163723945618\n",
            "Epoch 220 \t Max val Acc = 0.7797834873199463, 0.6553672552108765\n",
            "Epoch 221 \t Max val Acc = 0.7854545712471008, 0.6666666865348816\n",
            "Epoch 268 \t Max val Acc = 0.7862595319747925, 0.6836158037185669\n",
            "Epoch 276 \t Max val Acc = 0.7892720103263855, 0.6892655491828918\n",
            "Epoch 277 \t Max val Acc = 0.7894736528396606, 0.6836158037185669\n",
            "Epoch 285 \t Max val Acc = 0.7925925850868225, 0.6836158037185669\n",
            "Epoch 300 \t Max val Acc = 0.7985347509384155, 0.6892655491828918\n",
            "Epoch 302 \t Max val Acc = 0.8000000715255737, 0.694915235042572\n",
            "Epoch 304 \t Max val Acc = 0.8029739856719971, 0.700564980506897\n",
            "Epoch 377 \t Max val Acc = 0.8058608174324036, 0.700564980506897\n",
            "Epoch 396 \t Max val Acc = 0.8060837388038635, 0.7118644118309021\n",
            "Epoch 409 \t Max val Acc = 0.8075471520423889, 0.7118644118309021\n",
            "Epoch 468 \t Max val Acc = 0.8153846263885498, 0.7288135886192322\n",
            "Epoch 505 \t Max val Acc = 0.8161764144897461, 0.7175140976905823\n",
            "Epoch 509 \t Max val Acc = 0.8178438544273376, 0.7231638431549072\n",
            "Epoch 566 \t Max val Acc = 0.8226415514945984, 0.7344632744789124\n",
            "Epoch =  1000 \t Train loss = 0.43647 \t  Train Acc = 0.80000 \t Val loss = 0.69701 \t Acc = 0.683540.947370.79412 \t Patience = 1566\n",
            "Epoch =  2000 \t Train loss = 0.36380 \t  Train Acc = 0.84681 \t Val loss = 0.87829 \t Acc = 0.689870.956140.80147 \t Patience = 566\n",
            "Epoch 2247 \t Max val Acc = 0.8235294222831726, 0.7288135886192322\n",
            "Epoch =  3000 \t Train loss = 0.35564 \t  Train Acc = 0.84681 \t Val loss = 1.22859 \t Acc = 0.692810.929820.79401 \t Patience = 1247\n",
            "Epoch =  4000 \t Train loss = 0.34422 \t  Train Acc = 0.85390 \t Val loss = 1.46410 \t Acc = 0.690790.921050.78947 \t Patience = 247\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.663551390171051, 0.5932203531265259\n",
            "Epoch =  0 \t Train loss = 0.74373 \t  Train Acc = 0.52482 \t Val loss = 0.69548 \t Acc = 0.710000.622810.66355 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.6666666865348816, 0.5932203531265259\n",
            "Epoch 6 \t Max val Acc = 0.6759259104728699, 0.604519784450531\n",
            "Epoch 7 \t Max val Acc = 0.6849315166473389, 0.6101694703102112\n",
            "Epoch 10 \t Max val Acc = 0.6960352659225464, 0.6101694703102112\n",
            "Epoch 12 \t Max val Acc = 0.6986899375915527, 0.6101694703102112\n",
            "Epoch 13 \t Max val Acc = 0.7017543911933899, 0.6158192157745361\n",
            "Epoch 56 \t Max val Acc = 0.7068966031074524, 0.6158192157745361\n",
            "Epoch 59 \t Max val Acc = 0.7094017267227173, 0.6158192157745361\n",
            "Epoch 64 \t Max val Acc = 0.7118644714355469, 0.6158192157745361\n",
            "Epoch 65 \t Max val Acc = 0.7172995805740356, 0.6214689016342163\n",
            "Epoch 90 \t Max val Acc = 0.7213114500045776, 0.6158192157745361\n",
            "Epoch 91 \t Max val Acc = 0.7317072749137878, 0.6271186470985413\n",
            "Epoch 95 \t Max val Acc = 0.7360000610351562, 0.6271186470985413\n",
            "Epoch 99 \t Max val Acc = 0.7421875, 0.6271186470985413\n",
            "Epoch 100 \t Max val Acc = 0.7490347623825073, 0.6327683329582214\n",
            "Epoch 105 \t Max val Acc = 0.7557252645492554, 0.6384180784225464\n",
            "Epoch 107 \t Max val Acc = 0.7604563236236572, 0.6440678238868713\n",
            "Epoch 109 \t Max val Acc = 0.7698112726211548, 0.6553672552108765\n",
            "Epoch 110 \t Max val Acc = 0.7744360566139221, 0.6610169410705566\n",
            "Epoch 173 \t Max val Acc = 0.7756654024124146, 0.6666666865348816\n",
            "Epoch 177 \t Max val Acc = 0.7773584723472595, 0.6666666865348816\n",
            "Epoch 185 \t Max val Acc = 0.7806692123413086, 0.6666666865348816\n",
            "Epoch 186 \t Max val Acc = 0.7851851582527161, 0.6723163723945618\n",
            "Epoch 213 \t Max val Acc = 0.7854545712471008, 0.6666666865348816\n",
            "Epoch 223 \t Max val Acc = 0.7896679043769836, 0.6779661178588867\n",
            "Epoch 224 \t Max val Acc = 0.7910448312759399, 0.6836158037185669\n",
            "Epoch 225 \t Max val Acc = 0.7955389618873596, 0.6892655491828918\n",
            "Epoch 275 \t Max val Acc = 0.8000000715255737, 0.694915235042572\n",
            "Epoch 330 \t Max val Acc = 0.8015267252922058, 0.7062146663665771\n",
            "Epoch 343 \t Max val Acc = 0.8045977354049683, 0.7118644118309021\n",
            "Epoch 395 \t Max val Acc = 0.8058608174324036, 0.700564980506897\n",
            "Epoch 398 \t Max val Acc = 0.8134328126907349, 0.7175140976905823\n",
            "Epoch 416 \t Max val Acc = 0.8164794445037842, 0.7231638431549072\n",
            "Epoch =  1000 \t Train loss = 0.44829 \t  Train Acc = 0.79433 \t Val loss = 0.90235 \t Acc = 0.696770.947370.80297 \t Patience = 1416\n",
            "Epoch 1001 \t Max val Acc = 0.8208955526351929, 0.7288135886192322\n",
            "Epoch 1014 \t Max val Acc = 0.8265682458877563, 0.7344632744789124\n",
            "Epoch =  2000 \t Train loss = 0.41657 \t  Train Acc = 0.82695 \t Val loss = 0.91672 \t Acc = 0.684560.894740.77567 \t Patience = 1014\n",
            "Epoch =  3000 \t Train loss = 0.37621 \t  Train Acc = 0.84113 \t Val loss = 1.07331 \t Acc = 0.692860.850880.76378 \t Patience = 14\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7820068597793579, 0.6420454382896423\n",
            "Epoch =  0 \t Train loss = 0.72327 \t  Train Acc = 0.52691 \t Val loss = 0.68426 \t Acc = 0.642051.000000.78201 \t Patience = 2000\n",
            "Epoch 6 \t Max val Acc = 0.7847222089767456, 0.6477272510528564\n",
            "Epoch 9 \t Max val Acc = 0.7874564528465271, 0.6534090638160706\n",
            "Epoch 10 \t Max val Acc = 0.7902097702026367, 0.6590909361839294\n",
            "Epoch 448 \t Max val Acc = 0.7908746004104614, 0.6875\n",
            "Epoch 517 \t Max val Acc = 0.7910447716712952, 0.6818181872367859\n",
            "Epoch =  1000 \t Train loss = 0.40163 \t  Train Acc = 0.82436 \t Val loss = 0.81127 \t Acc = 0.660130.893810.75940 \t Patience = 1517\n",
            "Epoch 1196 \t Max val Acc = 0.7924528121948242, 0.6875\n",
            "Epoch =  2000 \t Train loss = 0.40727 \t  Train Acc = 0.82295 \t Val loss = 0.87892 \t Acc = 0.681820.929200.78652 \t Patience = 1196\n",
            "Epoch 2227 \t Max val Acc = 0.7954545617103577, 0.6931818127632141\n",
            "Epoch 2965 \t Max val Acc = 0.7984790205955505, 0.6988636255264282\n",
            "Epoch =  3000 \t Train loss = 0.34823 \t  Train Acc = 0.85127 \t Val loss = 0.87374 \t Acc = 0.698630.902650.78764 \t Patience = 1965\n",
            "Epoch 3711 \t Max val Acc = 0.8014980554580688, 0.6988636255264282\n",
            "Epoch 3742 \t Max val Acc = 0.801526665687561, 0.7045454382896423\n",
            "Epoch =  4000 \t Train loss = 0.31965 \t  Train Acc = 0.87110 \t Val loss = 0.83337 \t Acc = 0.681820.929200.78652 \t Patience = 1742\n",
            "Epoch =  5000 \t Train loss = 0.33261 \t  Train Acc = 0.85127 \t Val loss = 0.92872 \t Acc = 0.695360.929200.79545 \t Patience = 742\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.736842155456543, 0.6306818127632141\n",
            "Epoch =  0 \t Train loss = 0.75062 \t  Train Acc = 0.52408 \t Val loss = 0.68454 \t Acc = 0.679100.805310.73684 \t Patience = 2000\n",
            "Epoch 103 \t Max val Acc = 0.738095223903656, 0.625\n",
            "Epoch 104 \t Max val Acc = 0.7410358786582947, 0.6306818127632141\n",
            "Epoch 124 \t Max val Acc = 0.7421875, 0.625\n",
            "Epoch 125 \t Max val Acc = 0.7461538910865784, 0.625\n",
            "Epoch 126 \t Max val Acc = 0.7575758099555969, 0.6363636255264282\n",
            "Epoch 127 \t Max val Acc = 0.7651515007019043, 0.6477272510528564\n",
            "Epoch 152 \t Max val Acc = 0.7669172883033752, 0.6477272510528564\n",
            "Epoch 158 \t Max val Acc = 0.7720588445663452, 0.6477272510528564\n",
            "Epoch 159 \t Max val Acc = 0.7732343077659607, 0.6534090638160706\n",
            "Epoch 228 \t Max val Acc = 0.7794117331504822, 0.6590909361839294\n",
            "Epoch 314 \t Max val Acc = 0.781954824924469, 0.6704545617103577\n",
            "Epoch 475 \t Max val Acc = 0.783270001411438, 0.6761363744735718\n",
            "Epoch 658 \t Max val Acc = 0.7892720103263855, 0.6875\n",
            "Epoch 659 \t Max val Acc = 0.7984790205955505, 0.6988636255264282\n",
            "Epoch =  1000 \t Train loss = 0.40815 \t  Train Acc = 0.82436 \t Val loss = 0.88114 \t Acc = 0.655410.858410.74330 \t Patience = 1659\n",
            "Epoch =  2000 \t Train loss = 0.36779 \t  Train Acc = 0.84419 \t Val loss = 0.94068 \t Acc = 0.677850.893810.77099 \t Patience = 659\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7820068597793579, 0.6420454382896423\n",
            "Epoch =  0 \t Train loss = 0.73638 \t  Train Acc = 0.51133 \t Val loss = 0.68770 \t Acc = 0.642051.000000.78201 \t Patience = 2000\n",
            "Epoch 188 \t Max val Acc = 0.7822878360748291, 0.6647727489471436\n",
            "Epoch 199 \t Max val Acc = 0.7838827967643738, 0.6647727489471436\n",
            "Epoch 228 \t Max val Acc = 0.7883211970329285, 0.6704545617103577\n",
            "Epoch 249 \t Max val Acc = 0.7912087440490723, 0.6761363744735718\n",
            "Epoch =  1000 \t Train loss = 0.43362 \t  Train Acc = 0.80170 \t Val loss = 0.82607 \t Acc = 0.647060.876110.74436 \t Patience = 1249\n",
            "Epoch =  2000 \t Train loss = 0.39651 \t  Train Acc = 0.83003 \t Val loss = 0.89295 \t Acc = 0.680850.849560.75591 \t Patience = 249\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.017543859779834747, 0.3636363744735718\n",
            "Epoch =  0 \t Train loss = 0.78561 \t  Train Acc = 0.45042 \t Val loss = 0.73361 \t Acc = 1.000000.008850.01754 \t Patience = 2000\n",
            "Epoch 10 \t Max val Acc = 0.40506330132484436, 0.46590909361839294\n",
            "Epoch 11 \t Max val Acc = 0.4539877474308014, 0.4943181872367859\n",
            "Epoch 12 \t Max val Acc = 0.5, 0.5113636255264282\n",
            "Epoch 13 \t Max val Acc = 0.5086705088615417, 0.5170454382896423\n",
            "Epoch 22 \t Max val Acc = 0.517241358757019, 0.5227272510528564\n",
            "Epoch 23 \t Max val Acc = 0.5340908765792847, 0.5340909361839294\n",
            "Epoch 25 \t Max val Acc = 0.5745856761932373, 0.5625\n",
            "Epoch 34 \t Max val Acc = 0.5957447290420532, 0.5681818127632141\n",
            "Epoch 37 \t Max val Acc = 0.6020407676696777, 0.5568181872367859\n",
            "Epoch 38 \t Max val Acc = 0.6060606241226196, 0.5568181872367859\n",
            "Epoch 39 \t Max val Acc = 0.6130653619766235, 0.5625\n",
            "Epoch 44 \t Max val Acc = 0.623115599155426, 0.5738636255264282\n",
            "Epoch 51 \t Max val Acc = 0.6336633563041687, 0.5795454382896423\n",
            "Epoch 54 \t Max val Acc = 0.6372548937797546, 0.5795454382896423\n",
            "Epoch 55 \t Max val Acc = 0.6504855155944824, 0.5909090638160706\n",
            "Epoch 59 \t Max val Acc = 0.6507177352905273, 0.5852272510528564\n",
            "Epoch 60 \t Max val Acc = 0.6571428179740906, 0.5909090638160706\n",
            "Epoch 61 \t Max val Acc = 0.6698113083839417, 0.6022727489471436\n",
            "Epoch 62 \t Max val Acc = 0.6760563850402832, 0.6079545617103577\n",
            "Epoch 73 \t Max val Acc = 0.6857142448425293, 0.625\n",
            "Epoch 80 \t Max val Acc = 0.6877828240394592, 0.6079545617103577\n",
            "Epoch 84 \t Max val Acc = 0.6933333277702332, 0.6079545617103577\n",
            "Epoch 86 \t Max val Acc = 0.6964285969734192, 0.6136363744735718\n",
            "Epoch 118 \t Max val Acc = 0.7038626670837402, 0.6079545617103577\n",
            "Epoch 119 \t Max val Acc = 0.7063829898834229, 0.6079545617103577\n",
            "Epoch 120 \t Max val Acc = 0.7142857313156128, 0.6136363744735718\n",
            "Epoch 121 \t Max val Acc = 0.7219916582107544, 0.6193181872367859\n",
            "Epoch 122 \t Max val Acc = 0.7242798209190369, 0.6193181872367859\n",
            "Epoch 123 \t Max val Acc = 0.7265306115150452, 0.6193181872367859\n",
            "Epoch 124 \t Max val Acc = 0.7317072153091431, 0.625\n",
            "Epoch 129 \t Max val Acc = 0.7330678105354309, 0.6193181872367859\n",
            "Epoch 130 \t Max val Acc = 0.7360000014305115, 0.625\n",
            "Epoch 132 \t Max val Acc = 0.7460317611694336, 0.6363636255264282\n",
            "Epoch 153 \t Max val Acc = 0.7567567229270935, 0.6420454382896423\n",
            "Epoch 166 \t Max val Acc = 0.7662834525108337, 0.6534090638160706\n",
            "Epoch 169 \t Max val Acc = 0.7727272510528564, 0.6590909361839294\n",
            "Epoch 184 \t Max val Acc = 0.7753623127937317, 0.6477272510528564\n",
            "Epoch 198 \t Max val Acc = 0.7773584723472595, 0.6647727489471436\n",
            "Epoch 218 \t Max val Acc = 0.7781818509101868, 0.6534090638160706\n",
            "Epoch 219 \t Max val Acc = 0.7797834277153015, 0.6534090638160706\n",
            "Epoch 220 \t Max val Acc = 0.7841726541519165, 0.6590909361839294\n",
            "Epoch 222 \t Max val Acc = 0.7912087440490723, 0.6761363744735718\n",
            "Epoch 335 \t Max val Acc = 0.7955390214920044, 0.6875\n",
            "Epoch 360 \t Max val Acc = 0.8014980554580688, 0.6988636255264282\n",
            "Epoch 684 \t Max val Acc = 0.804511308670044, 0.7045454382896423\n",
            "Epoch 685 \t Max val Acc = 0.8045976758003235, 0.7102272510528564\n",
            "Epoch =  1000 \t Train loss = 0.43626 \t  Train Acc = 0.81728 \t Val loss = 0.81070 \t Acc = 0.671050.902650.76981 \t Patience = 1685\n",
            "Epoch 1882 \t Max val Acc = 0.8059701323509216, 0.7045454382896423\n",
            "Epoch =  2000 \t Train loss = 0.37463 \t  Train Acc = 0.84419 \t Val loss = 0.83751 \t Acc = 0.700680.911500.79231 \t Patience = 1882\n",
            "Epoch =  3000 \t Train loss = 0.37100 \t  Train Acc = 0.85411 \t Val loss = 1.03178 \t Acc = 0.673330.893810.76806 \t Patience = 882\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7820068597793579, 0.6420454382896423\n",
            "Epoch =  0 \t Train loss = 0.74091 \t  Train Acc = 0.50142 \t Val loss = 0.67282 \t Acc = 0.642051.000000.78201 \t Patience = 2000\n",
            "Epoch 4 \t Max val Acc = 0.7847222089767456, 0.6477272510528564\n",
            "Epoch =  1000 \t Train loss = 0.40900 \t  Train Acc = 0.81870 \t Val loss = 0.78535 \t Acc = 0.669060.823010.73810 \t Patience = 1004\n",
            "Epoch =  2000 \t Train loss = 0.39820 \t  Train Acc = 0.83286 \t Val loss = 0.96924 \t Acc = 0.669060.823010.73810 \t Patience = 4\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.08196721225976944, 0.3636363744735718\n",
            "Epoch =  0 \t Train loss = 0.75451 \t  Train Acc = 0.48442 \t Val loss = 0.69354 \t Acc = 0.555560.044250.08197 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.7053571939468384, 0.625\n",
            "Epoch 2 \t Max val Acc = 0.7179487347602844, 0.625\n",
            "Epoch 3 \t Max val Acc = 0.7468879222869873, 0.6534090638160706\n",
            "Epoch 17 \t Max val Acc = 0.7519999742507935, 0.6477272510528564\n",
            "Epoch 25 \t Max val Acc = 0.7588932514190674, 0.6534090638160706\n",
            "Epoch 26 \t Max val Acc = 0.761904776096344, 0.6590909361839294\n",
            "Epoch 56 \t Max val Acc = 0.765625, 0.6590909361839294\n",
            "Epoch 58 \t Max val Acc = 0.7662834525108337, 0.6534090638160706\n",
            "Epoch 61 \t Max val Acc = 0.7698112726211548, 0.6534090638160706\n",
            "Epoch 62 \t Max val Acc = 0.7744361162185669, 0.6590909361839294\n",
            "Epoch 63 \t Max val Acc = 0.7790262699127197, 0.6647727489471436\n",
            "Epoch 68 \t Max val Acc = 0.7849056720733643, 0.6761363744735718\n",
            "Epoch 120 \t Max val Acc = 0.7894736528396606, 0.6818181872367859\n",
            "Epoch 159 \t Max val Acc = 0.7927272319793701, 0.6761363744735718\n",
            "Epoch 161 \t Max val Acc = 0.7942237854003906, 0.6761363744735718\n",
            "Epoch 162 \t Max val Acc = 0.795698881149292, 0.6761363744735718\n",
            "Epoch 180 \t Max val Acc = 0.7971529960632324, 0.6761363744735718\n",
            "Epoch 208 \t Max val Acc = 0.8014705777168274, 0.6931818127632141\n",
            "Epoch 224 \t Max val Acc = 0.8029197454452515, 0.6931818127632141\n",
            "Epoch 281 \t Max val Acc = 0.807272732257843, 0.6988636255264282\n",
            "Epoch 902 \t Max val Acc = 0.8088235259056091, 0.7045454382896423\n",
            "Epoch 904 \t Max val Acc = 0.8148148059844971, 0.7159090638160706\n",
            "Epoch =  1000 \t Train loss = 0.45931 \t  Train Acc = 0.81303 \t Val loss = 0.85138 \t Acc = 0.670730.973450.79422 \t Patience = 1904\n",
            "Epoch =  2000 \t Train loss = 0.41947 \t  Train Acc = 0.82436 \t Val loss = 0.98667 \t Acc = 0.672960.946900.78676 \t Patience = 904\n",
            "Epoch 2058 \t Max val Acc = 0.8161764740943909, 0.7159090638160706\n",
            "Epoch =  3000 \t Train loss = 0.35295 \t  Train Acc = 0.84703 \t Val loss = 1.09580 \t Acc = 0.679250.955750.79412 \t Patience = 1058\n",
            "Epoch =  4000 \t Train loss = 0.33297 \t  Train Acc = 0.86119 \t Val loss = 1.21168 \t Acc = 0.677420.929200.78358 \t Patience = 58\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.80379 \t  Train Acc = 0.49008 \t Val loss = 0.73725 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 6 \t Max val Acc = 0.5287356376647949, 0.5340909361839294\n",
            "Epoch 7 \t Max val Acc = 0.672897219657898, 0.6022727489471436\n",
            "Epoch 8 \t Max val Acc = 0.6964285969734192, 0.6136363744735718\n",
            "Epoch 9 \t Max val Acc = 0.7017543911933899, 0.6136363744735718\n",
            "Epoch 10 \t Max val Acc = 0.7166666388511658, 0.6136363744735718\n",
            "Epoch 11 \t Max val Acc = 0.7279999852180481, 0.6136363744735718\n",
            "Epoch 12 \t Max val Acc = 0.7480315566062927, 0.6363636255264282\n",
            "Epoch 13 \t Max val Acc = 0.751937985420227, 0.6363636255264282\n",
            "Epoch 15 \t Max val Acc = 0.7559054493904114, 0.6477272510528564\n",
            "Epoch 95 \t Max val Acc = 0.7580645084381104, 0.6590909361839294\n",
            "Epoch 97 \t Max val Acc = 0.763052225112915, 0.6647727489471436\n",
            "Epoch 98 \t Max val Acc = 0.7649403214454651, 0.6647727489471436\n",
            "Epoch 99 \t Max val Acc = 0.7747035026550293, 0.6761363744735718\n",
            "Epoch 100 \t Max val Acc = 0.78125, 0.6818181872367859\n",
            "Epoch 117 \t Max val Acc = 0.7829457521438599, 0.6818181872367859\n",
            "Epoch 122 \t Max val Acc = 0.7862595915794373, 0.6818181872367859\n",
            "Epoch 123 \t Max val Acc = 0.7908746004104614, 0.6875\n",
            "Epoch 126 \t Max val Acc = 0.7924528121948242, 0.6875\n",
            "Epoch 127 \t Max val Acc = 0.7969924807548523, 0.6931818127632141\n",
            "Epoch 128 \t Max val Acc = 0.800000011920929, 0.6988636255264282\n",
            "Epoch 130 \t Max val Acc = 0.8014980554580688, 0.6988636255264282\n",
            "Epoch 153 \t Max val Acc = 0.804427981376648, 0.6988636255264282\n",
            "Epoch 160 \t Max val Acc = 0.8088235259056091, 0.7045454382896423\n",
            "Epoch 939 \t Max val Acc = 0.8102189302444458, 0.7045454382896423\n",
            "Epoch =  1000 \t Train loss = 0.44397 \t  Train Acc = 0.79037 \t Val loss = 0.74384 \t Acc = 0.664670.982300.79286 \t Patience = 1939\n",
            "Epoch =  2000 \t Train loss = 0.37332 \t  Train Acc = 0.83144 \t Val loss = 0.88124 \t Acc = 0.664600.946900.78102 \t Patience = 939\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7615658640861511, 0.6193181872367859\n",
            "Epoch =  0 \t Train loss = 0.76515 \t  Train Acc = 0.52125 \t Val loss = 0.68154 \t Acc = 0.636900.946900.76157 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.7703180313110352, 0.6306818127632141\n",
            "Epoch 4 \t Max val Acc = 0.7746478915214539, 0.6363636255264282\n",
            "Epoch 6 \t Max val Acc = 0.7789473533630371, 0.6420454382896423\n",
            "Epoch 17 \t Max val Acc = 0.7804878354072571, 0.6420454382896423\n",
            "Epoch 26 \t Max val Acc = 0.7832167148590088, 0.6477272510528564\n",
            "Epoch 99 \t Max val Acc = 0.7872340679168701, 0.6590909361839294\n",
            "Epoch 129 \t Max val Acc = 0.7900355458259583, 0.6647727489471436\n",
            "Epoch 205 \t Max val Acc = 0.7928571105003357, 0.6704545617103577\n",
            "Epoch 243 \t Max val Acc = 0.7943262457847595, 0.6704545617103577\n",
            "Epoch 254 \t Max val Acc = 0.8057554364204407, 0.6931818127632141\n",
            "Epoch 460 \t Max val Acc = 0.8058608174324036, 0.6988636255264282\n",
            "Epoch 495 \t Max val Acc = 0.8086642622947693, 0.6988636255264282\n",
            "Epoch 496 \t Max val Acc = 0.8115941882133484, 0.7045454382896423\n",
            "Epoch 589 \t Max val Acc = 0.8161764740943909, 0.7159090638160706\n",
            "Epoch =  1000 \t Train loss = 0.48696 \t  Train Acc = 0.76204 \t Val loss = 0.88028 \t Acc = 0.674850.973450.79710 \t Patience = 1589\n",
            "Epoch =  2000 \t Train loss = 0.39127 \t  Train Acc = 0.82720 \t Val loss = 1.17695 \t Acc = 0.677220.946900.78967 \t Patience = 589\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.3658536672592163, 0.40909090638160706\n",
            "Epoch =  0 \t Train loss = 0.78336 \t  Train Acc = 0.48584 \t Val loss = 0.69650 \t Acc = 0.588240.265490.36585 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.6634615659713745, 0.6022727489471436\n",
            "Epoch 2 \t Max val Acc = 0.6986899971961975, 0.6079545617103577\n",
            "Epoch 3 \t Max val Acc = 0.7196653485298157, 0.6193181872367859\n",
            "Epoch 4 \t Max val Acc = 0.7372881174087524, 0.6477272510528564\n",
            "Epoch 17 \t Max val Acc = 0.7385892271995544, 0.6420454382896423\n",
            "Epoch 26 \t Max val Acc = 0.7394958138465881, 0.6477272510528564\n",
            "Epoch 27 \t Max val Acc = 0.75, 0.6590909361839294\n",
            "Epoch 36 \t Max val Acc = 0.7591836452484131, 0.6647727489471436\n",
            "Epoch 61 \t Max val Acc = 0.7626459002494812, 0.6534090638160706\n",
            "Epoch 67 \t Max val Acc = 0.7674418687820435, 0.6590909361839294\n",
            "Epoch 69 \t Max val Acc = 0.7680608630180359, 0.6534090638160706\n",
            "Epoch 71 \t Max val Acc = 0.7732343077659607, 0.6534090638160706\n",
            "Epoch 72 \t Max val Acc = 0.7737225890159607, 0.6477272510528564\n",
            "Epoch 73 \t Max val Acc = 0.7753623127937317, 0.6477272510528564\n",
            "Epoch 88 \t Max val Acc = 0.7765568494796753, 0.6534090638160706\n",
            "Epoch 112 \t Max val Acc = 0.7781818509101868, 0.6534090638160706\n",
            "Epoch 126 \t Max val Acc = 0.7794117331504822, 0.6590909361839294\n",
            "Epoch 127 \t Max val Acc = 0.7838827967643738, 0.6647727489471436\n",
            "Epoch 145 \t Max val Acc = 0.7841726541519165, 0.6590909361839294\n",
            "Epoch 159 \t Max val Acc = 0.7885304093360901, 0.6647727489471436\n",
            "Epoch 178 \t Max val Acc = 0.7898550629615784, 0.6704545617103577\n",
            "Epoch 179 \t Max val Acc = 0.7942237854003906, 0.6761363744735718\n",
            "Epoch 181 \t Max val Acc = 0.7971014380455017, 0.6818181872367859\n",
            "Epoch 241 \t Max val Acc = 0.800000011920929, 0.6875\n",
            "Epoch 666 \t Max val Acc = 0.8029197454452515, 0.6931818127632141\n",
            "Epoch 848 \t Max val Acc = 0.8057554364204407, 0.6931818127632141\n",
            "Epoch 964 \t Max val Acc = 0.807272732257843, 0.6988636255264282\n",
            "Epoch =  1000 \t Train loss = 0.45306 \t  Train Acc = 0.79603 \t Val loss = 0.82897 \t Acc = 0.679490.938050.78810 \t Patience = 1964\n",
            "Epoch 1017 \t Max val Acc = 0.8074074387550354, 0.7045454382896423\n",
            "Epoch 1039 \t Max val Acc = 0.8102189302444458, 0.7045454382896423\n",
            "Epoch 1062 \t Max val Acc = 0.813186764717102, 0.7102272510528564\n",
            "Epoch 1969 \t Max val Acc = 0.8161764740943909, 0.7159090638160706\n",
            "Epoch =  2000 \t Train loss = 0.41623 \t  Train Acc = 0.81728 \t Val loss = 0.85457 \t Acc = 0.694270.964600.80741 \t Patience = 1969\n",
            "Epoch =  3000 \t Train loss = 0.37623 \t  Train Acc = 0.84278 \t Val loss = 0.86444 \t Acc = 0.693330.920350.79087 \t Patience = 969\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7242798209190369, 0.6193181872367859\n",
            "Epoch =  0 \t Train loss = 0.74883 \t  Train Acc = 0.54674 \t Val loss = 0.69981 \t Acc = 0.676920.778760.72428 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.7360000014305115, 0.625\n",
            "Epoch 2 \t Max val Acc = 0.7432950139045715, 0.6193181872367859\n",
            "Epoch 4 \t Max val Acc = 0.74349445104599, 0.6079545617103577\n",
            "Epoch 5 \t Max val Acc = 0.7472527623176575, 0.6079545617103577\n",
            "Epoch 7 \t Max val Acc = 0.7553957104682922, 0.6136363744735718\n",
            "Epoch 8 \t Max val Acc = 0.764285683631897, 0.625\n",
            "Epoch 9 \t Max val Acc = 0.7730495929718018, 0.6363636255264282\n",
            "Epoch 10 \t Max val Acc = 0.7789473533630371, 0.6420454382896423\n",
            "Epoch 11 \t Max val Acc = 0.7832167148590088, 0.6477272510528564\n",
            "Epoch 38 \t Max val Acc = 0.7872340679168701, 0.6590909361839294\n",
            "Epoch 41 \t Max val Acc = 0.7885304093360901, 0.6647727489471436\n",
            "Epoch 53 \t Max val Acc = 0.7915194630622864, 0.6647727489471436\n",
            "Epoch 118 \t Max val Acc = 0.7928571105003357, 0.6704545617103577\n",
            "Epoch 121 \t Max val Acc = 0.7943262457847595, 0.6704545617103577\n",
            "Epoch 122 \t Max val Acc = 0.7971529960632324, 0.6761363744735718\n",
            "Epoch 167 \t Max val Acc = 0.8014440536499023, 0.6875\n",
            "Epoch 259 \t Max val Acc = 0.8028673529624939, 0.6875\n",
            "Epoch 271 \t Max val Acc = 0.804347813129425, 0.6931818127632141\n",
            "Epoch 345 \t Max val Acc = 0.8057554364204407, 0.6931818127632141\n",
            "Epoch 386 \t Max val Acc = 0.8086642622947693, 0.6988636255264282\n",
            "Epoch 480 \t Max val Acc = 0.8129495978355408, 0.7045454382896423\n",
            "Epoch 887 \t Max val Acc = 0.8161764740943909, 0.7159090638160706\n",
            "Epoch 976 \t Max val Acc = 0.8178438544273376, 0.7215909361839294\n",
            "Epoch =  1000 \t Train loss = 0.46236 \t  Train Acc = 0.78754 \t Val loss = 0.86133 \t Acc = 0.685530.964600.80147 \t Patience = 1976\n",
            "Epoch 1498 \t Max val Acc = 0.8191881775856018, 0.7215909361839294\n",
            "Epoch 1513 \t Max val Acc = 0.8195488452911377, 0.7272727489471436\n",
            "Epoch 1535 \t Max val Acc = 0.8208954930305481, 0.7272727489471436\n",
            "Epoch 1536 \t Max val Acc = 0.8239700794219971, 0.7329545617103577\n",
            "Epoch 1537 \t Max val Acc = 0.8257575631141663, 0.7386363744735718\n",
            "Epoch 1780 \t Max val Acc = 0.8270676732063293, 0.7386363744735718\n",
            "Epoch =  2000 \t Train loss = 0.37040 \t  Train Acc = 0.84136 \t Val loss = 0.94137 \t Acc = 0.705130.973450.81784 \t Patience = 1780\n",
            "Epoch 2077 \t Max val Acc = 0.8301886916160583, 0.7443181872367859\n",
            "Epoch 2866 \t Max val Acc = 0.8314606547355652, 0.7443181872367859\n",
            "Epoch =  3000 \t Train loss = 0.34873 \t  Train Acc = 0.83994 \t Val loss = 1.04790 \t Acc = 0.715230.955750.81818 \t Patience = 1866\n",
            "Epoch 3169 \t Max val Acc = 0.8358209729194641, 0.75\n",
            "Epoch =  4000 \t Train loss = 0.36378 \t  Train Acc = 0.84561 \t Val loss = 1.07370 \t Acc = 0.705130.973450.81784 \t Patience = 1169\n",
            "Epoch =  5000 \t Train loss = 0.33695 \t  Train Acc = 0.86402 \t Val loss = 1.32716 \t Acc = 0.698720.964600.81041 \t Patience = 169\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.7820068597793579, 0.6420454382896423\n",
            "Epoch =  0 \t Train loss = 0.77890 \t  Train Acc = 0.50991 \t Val loss = 0.68198 \t Acc = 0.642051.000000.78201 \t Patience = 2000\n",
            "Epoch =  1000 \t Train loss = 0.44686 \t  Train Acc = 0.78895 \t Val loss = 0.88892 \t Acc = 0.642380.858410.73485 \t Patience = 1000\n",
            "Epoch =  2000 \t Train loss = 0.37891 \t  Train Acc = 0.84136 \t Val loss = 1.04649 \t Acc = 0.633090.778760.69841 \t Patience = 0\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.6636770963668823, 0.5738636255264282\n",
            "Epoch =  0 \t Train loss = 0.73496 \t  Train Acc = 0.56799 \t Val loss = 0.72131 \t Acc = 0.672730.654870.66368 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.7073171138763428, 0.5909090638160706\n",
            "Epoch 2 \t Max val Acc = 0.7300379872322083, 0.5965909361839294\n",
            "Epoch 3 \t Max val Acc = 0.7380074262619019, 0.5965909361839294\n",
            "Epoch 4 \t Max val Acc = 0.7553957104682922, 0.6136363744735718\n",
            "Epoch 5 \t Max val Acc = 0.7649123072624207, 0.6193181872367859\n",
            "Epoch 6 \t Max val Acc = 0.7735191583633423, 0.6306818127632141\n",
            "Epoch 7 \t Max val Acc = 0.7777777910232544, 0.6363636255264282\n",
            "Epoch 14 \t Max val Acc = 0.7820068597793579, 0.6420454382896423\n",
            "Epoch =  1000 \t Train loss = 0.41274 \t  Train Acc = 0.81445 \t Val loss = 0.96624 \t Acc = 0.658230.920350.76753 \t Patience = 1014\n",
            "Epoch =  2000 \t Train loss = 0.36515 \t  Train Acc = 0.84561 \t Val loss = 1.12748 \t Acc = 0.650680.840710.73359 \t Patience = 14\n",
            "Training complete\n",
            "Epoch =  0 \t Train loss = 0.77107 \t  Train Acc = 0.48017 \t Val loss = 0.71827 \t Acc = nan0.00000nan \t Patience = 1999\n",
            "Epoch 11 \t Max val Acc = 0.017543859779834747, 0.3636363744735718\n",
            "Epoch 26 \t Max val Acc = 0.034188032150268555, 0.3579545319080353\n",
            "Epoch 27 \t Max val Acc = 0.08196721225976944, 0.3636363744735718\n",
            "Epoch 28 \t Max val Acc = 0.21052631735801697, 0.40340909361839294\n",
            "Epoch 29 \t Max val Acc = 0.2222222238779068, 0.40340909361839294\n",
            "Epoch 30 \t Max val Acc = 0.24817517399787903, 0.41477271914482117\n",
            "Epoch 31 \t Max val Acc = 0.27142855525016785, 0.4204545319080353\n",
            "Epoch 32 \t Max val Acc = 0.2957746684551239, 0.4318181872367859\n",
            "Epoch 33 \t Max val Acc = 0.33103451132774353, 0.4488636255264282\n",
            "Epoch 34 \t Max val Acc = 0.3401360511779785, 0.4488636255264282\n",
            "Epoch 37 \t Max val Acc = 0.3513513505458832, 0.4545454680919647\n",
            "Epoch 38 \t Max val Acc = 0.3624161183834076, 0.46022728085517883\n",
            "Epoch 39 \t Max val Acc = 0.38410595059394836, 0.47159090638160706\n",
            "Epoch 40 \t Max val Acc = 0.4025973677635193, 0.47727271914482117\n",
            "Epoch 41 \t Max val Acc = 0.41290321946144104, 0.4829545319080353\n",
            "Epoch 42 \t Max val Acc = 0.430379718542099, 0.4886363744735718\n",
            "Epoch 43 \t Max val Acc = 0.45000001788139343, 0.5\n",
            "Epoch 44 \t Max val Acc = 0.45679011940956116, 0.5\n",
            "Epoch 45 \t Max val Acc = 0.47272729873657227, 0.5056818127632141\n",
            "Epoch 46 \t Max val Acc = 0.49704140424728394, 0.5170454382896423\n",
            "Epoch 53 \t Max val Acc = 0.5, 0.5113636255264282\n",
            "Epoch 56 \t Max val Acc = 0.5029240250587463, 0.5170454382896423\n",
            "Epoch 61 \t Max val Acc = 0.5116279125213623, 0.5227272510528564\n",
            "Epoch 67 \t Max val Acc = 0.5197740197181702, 0.5170454382896423\n",
            "Epoch 69 \t Max val Acc = 0.5251396894454956, 0.5170454382896423\n",
            "Epoch 74 \t Max val Acc = 0.5333333015441895, 0.5227272510528564\n",
            "Epoch 75 \t Max val Acc = 0.5494505763053894, 0.5340909361839294\n",
            "Epoch 76 \t Max val Acc = 0.5652173757553101, 0.5454545617103577\n",
            "Epoch 77 \t Max val Acc = 0.5882352590560913, 0.5625\n",
            "Epoch 78 \t Max val Acc = 0.5957447290420532, 0.5681818127632141\n",
            "Epoch 80 \t Max val Acc = 0.6105263233184814, 0.5795454382896423\n",
            "Epoch 85 \t Max val Acc = 0.6145833134651184, 0.5795454382896423\n",
            "Epoch 86 \t Max val Acc = 0.6153846383094788, 0.5738636255264282\n",
            "Epoch 92 \t Max val Acc = 0.616161584854126, 0.5681818127632141\n",
            "Epoch 94 \t Max val Acc = 0.6199999451637268, 0.5681818127632141\n",
            "Epoch 95 \t Max val Acc = 0.6372548937797546, 0.5795454382896423\n",
            "Epoch 97 \t Max val Acc = 0.6439024209976196, 0.5852272510528564\n",
            "Epoch 98 \t Max val Acc = 0.6538461446762085, 0.5909090638160706\n",
            "Epoch 101 \t Max val Acc = 0.6542055606842041, 0.5795454382896423\n",
            "Epoch 102 \t Max val Acc = 0.6604651212692261, 0.5852272510528564\n",
            "Epoch 107 \t Max val Acc = 0.6754385828971863, 0.5795454382896423\n",
            "Epoch 113 \t Max val Acc = 0.6899563074111938, 0.5965909361839294\n",
            "Epoch 114 \t Max val Acc = 0.701298713684082, 0.6079545617103577\n",
            "Epoch 117 \t Max val Acc = 0.7088608145713806, 0.6079545617103577\n",
            "Epoch 121 \t Max val Acc = 0.7112970948219299, 0.6079545617103577\n",
            "Epoch 124 \t Max val Acc = 0.7142857313156128, 0.6136363744735718\n",
            "Epoch 125 \t Max val Acc = 0.7172996401786804, 0.6193181872367859\n",
            "Epoch 126 \t Max val Acc = 0.7179487347602844, 0.625\n",
            "Epoch 135 \t Max val Acc = 0.7190082669258118, 0.6136363744735718\n",
            "Epoch 224 \t Max val Acc = 0.7265625, 0.6022727489471436\n",
            "Epoch 225 \t Max val Acc = 0.7286822199821472, 0.6022727489471436\n",
            "Epoch 238 \t Max val Acc = 0.7313432693481445, 0.5909090638160706\n",
            "Epoch 240 \t Max val Acc = 0.7320755124092102, 0.5965909361839294\n",
            "Epoch 292 \t Max val Acc = 0.7322834134101868, 0.6136363744735718\n",
            "Epoch 306 \t Max val Acc = 0.736842155456543, 0.6306818127632141\n",
            "Epoch 309 \t Max val Acc = 0.7372548580169678, 0.6193181872367859\n",
            "Epoch 335 \t Max val Acc = 0.7389557957649231, 0.6306818127632141\n",
            "Epoch 342 \t Max val Acc = 0.7401574850082397, 0.625\n",
            "Epoch 353 \t Max val Acc = 0.7441860437393188, 0.625\n",
            "Epoch 359 \t Max val Acc = 0.7557252049446106, 0.6363636255264282\n",
            "Epoch 487 \t Max val Acc = 0.7567567229270935, 0.6420454382896423\n",
            "Epoch 535 \t Max val Acc = 0.7611939907073975, 0.6363636255264282\n",
            "Epoch 894 \t Max val Acc = 0.7629629969596863, 0.6363636255264282\n",
            "Epoch 895 \t Max val Acc = 0.7765568494796753, 0.6534090638160706\n",
            "Epoch =  1000 \t Train loss = 0.44743 \t  Train Acc = 0.78329 \t Val loss = 0.93305 \t Acc = 0.635140.831860.72031 \t Patience = 1895\n",
            "Epoch =  2000 \t Train loss = 0.34736 \t  Train Acc = 0.85411 \t Val loss = 0.99240 \t Acc = 0.659420.805310.72510 \t Patience = 895\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.743682324886322, 0.5965909361839294\n",
            "Epoch =  0 \t Train loss = 0.71215 \t  Train Acc = 0.55666 \t Val loss = 0.67266 \t Acc = 0.628050.911500.74368 \t Patience = 2000\n",
            "Epoch 2 \t Max val Acc = 0.7463768720626831, 0.6022727489471436\n",
            "Epoch 3 \t Max val Acc = 0.749090850353241, 0.6079545617103577\n",
            "Epoch 11 \t Max val Acc = 0.7527676224708557, 0.6193181872367859\n",
            "Epoch 12 \t Max val Acc = 0.7545788288116455, 0.6193181872367859\n",
            "Epoch 23 \t Max val Acc = 0.7581227421760559, 0.6193181872367859\n",
            "Epoch 40 \t Max val Acc = 0.7714285850524902, 0.6363636255264282\n",
            "Epoch 490 \t Max val Acc = 0.7722008228302002, 0.6647727489471436\n",
            "Epoch 713 \t Max val Acc = 0.7744361162185669, 0.6590909361839294\n",
            "Epoch =  1000 \t Train loss = 0.42226 \t  Train Acc = 0.81586 \t Val loss = 0.87923 \t Acc = 0.669170.787610.72358 \t Patience = 1713\n",
            "Epoch 1046 \t Max val Acc = 0.7749077081680298, 0.6534090638160706\n",
            "Epoch 1047 \t Max val Acc = 0.7777777314186096, 0.6590909361839294\n",
            "Epoch =  2000 \t Train loss = 0.37991 \t  Train Acc = 0.84278 \t Val loss = 1.00035 \t Acc = 0.676260.831860.74603 \t Patience = 1047\n",
            "Epoch =  3000 \t Train loss = 0.34785 \t  Train Acc = 0.85127 \t Val loss = 1.17148 \t Acc = 0.676470.814160.73896 \t Patience = 47\n",
            "Training complete\n",
            "Epoch 0 \t Max val Acc = 0.20437955856323242, 0.3806818127632141\n",
            "Epoch =  0 \t Train loss = 0.77640 \t  Train Acc = 0.49008 \t Val loss = 0.69718 \t Acc = 0.583330.123890.20438 \t Patience = 2000\n",
            "Epoch 1 \t Max val Acc = 0.31168830394744873, 0.39772728085517883\n",
            "Epoch 2 \t Max val Acc = 0.701195240020752, 0.5738636255264282\n",
            "Epoch 3 \t Max val Acc = 0.74349445104599, 0.6079545617103577\n",
            "Epoch 4 \t Max val Acc = 0.7453874945640564, 0.6079545617103577\n",
            "Epoch 14 \t Max val Acc = 0.7625899314880371, 0.625\n",
            "Epoch 15 \t Max val Acc = 0.7714285850524902, 0.6363636255264282\n",
            "Epoch 16 \t Max val Acc = 0.7719298601150513, 0.6306818127632141\n",
            "Epoch 687 \t Max val Acc = 0.7734375, 0.6704545617103577\n",
            "Epoch 688 \t Max val Acc = 0.7751938104629517, 0.6704545617103577\n",
            "Epoch =  1000 \t Train loss = 0.40833 \t  Train Acc = 0.83003 \t Val loss = 0.82801 \t Acc = 0.671140.884960.76336 \t Patience = 1688\n",
            "Epoch 1019 \t Max val Acc = 0.7816092371940613, 0.6761363744735718\n",
            "Epoch =  2000 \t Train loss = 0.38894 \t  Train Acc = 0.82436 \t Val loss = 1.06721 \t Acc = 0.668870.893810.76515 \t Patience = 1019\n",
            "Epoch 2047 \t Max val Acc = 0.7835820317268372, 0.6704545617103577\n",
            "Epoch 2520 \t Max val Acc = 0.7851851582527161, 0.6704545617103577\n",
            "Epoch =  3000 \t Train loss = 0.33245 \t  Train Acc = 0.86261 \t Val loss = 1.13180 \t Acc = 0.678570.840710.75099 \t Patience = 1520\n",
            "Epoch =  4000 \t Train loss = 0.34621 \t  Train Acc = 0.86686 \t Val loss = 1.60314 \t Acc = 0.664180.787610.72065 \t Patience = 520\n",
            "Training complete\n",
            "Max val losses________________\n",
            "[tensor(0.8095), tensor(0.8110), tensor(0.8106), tensor(0.8016), tensor(0.8145), tensor(0.8226), tensor(0.8226), tensor(0.8199), tensor(0.8235), tensor(0.8266), tensor(0.8015), tensor(0.7985), tensor(0.7912), tensor(0.8060), tensor(0.7847), tensor(0.8162), tensor(0.8102), tensor(0.8162), tensor(0.8162), tensor(0.8358), tensor(0.7820), tensor(0.7820), tensor(0.7766), tensor(0.7778), tensor(0.7852)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9oUTo1l8aG"
      },
      "source": [
        "sch_embed2 = CustomEmbedding(78, 60)\n",
        "sch_embed2.load_state_dict(sch_embed_state_dicts[np.argmax(max_val_accs)])\n",
        "sch_cla = SimpleNN(sch_embed2,  60,2)\n",
        "sch_cla.eval()\n",
        "evaluate(sch_cla, inputs_train, labels_train, loss_func, device, calc_accuracy, pr=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWS_wqPul8aG",
        "outputId": "9da13cd3-10fe-47ed-9be2-133d6a513410"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 60 - dropout"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3WPJNsTl8aH",
        "outputId": "138a2530-fb91-4cc0-bf52-18159c3d098c"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 100 - dropout"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IDAtyB_l8aI",
        "outputId": "59395273-f174-4b55-df33-969c685714a6"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 30 - dropout"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd4MShEfl8aI",
        "outputId": "f1afca77-332e-4036-93a4-1617950a8b98"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 60 - dropout"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqEYyDRTl8aJ",
        "outputId": "dd81567e-04ea-42ff-f968-59c24f5cf02a"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input input hidden) 100 - dropout"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJlMfWdpl8aJ",
        "outputId": "0f732b2e-b503-423b-8f65-9d5c2ffe47b8"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 30 - dropout"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T41MDRmsl8aK",
        "outputId": "def640b4-58a8-4d5f-b27f-98b7ce18b4d4"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 100 - dropout"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2yJjrE6l8aK",
        "outputId": "67334fe4-0992-4c41-a283-53ff2c413e3e"
      },
      "source": [
        "np.mean(max_val_accs) # - hidden 60 - dropout"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54567784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "1dncGCctl8aL",
        "outputId": "129e9454-cb70-4c1f-a873-51cf41481d4f"
      },
      "source": [
        "plt.plot(train_losses,label=\"Training loss\")\n",
        "plt.plot(val_losses,label=\"Validation loss\")\n",
        "plt.title('Schizophrenia Training and Validation Loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(train_accuracies, label=\"Train accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation accuracy\")\n",
        "plt.title('Schizophrenia Validation Accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqlmengzvHOV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU22CnlPvHqR"
      },
      "source": [
        "# CHD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9adFlhQvHqR",
        "outputId": "dde99f5a-53d1-460a-fa33-d2d713c2cc13"
      },
      "source": [
        "chd_exp_n = np.array(chd_exp.iloc[:-1,:]).T.astype(np.float32)\n",
        "chd_risk_n = np.array(chd_risk)\n",
        "np.mean(chd_risk_n), len(chd_risk_n)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.09803921568627451, 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTIDooZ8vHqS"
      },
      "source": [
        "inputs_train, inputs_val, labels_train, labels_val = model_selection.train_test_split(chd_exp_n, chd_risk_n, test_size=0.3,\n",
        "                                                                                      stratify=chd_risk_n, random_state=0)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPYilX9VJTHF",
        "outputId": "2a9a2df5-c0fc-4da2-ffc5-9655557b96b7"
      },
      "source": [
        "len(labels_train)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eelIlAgyIyNV",
        "outputId": "29e9ab32-333f-401c-9c70-e0dede33bd2f"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "# fit and apply the transform\n",
        "inputs_train_res, labels_train_res = oversample.fit_resample(inputs_train, labels_train)\n",
        "np.mean(labels_train_res)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj2Zqf8kvHqT",
        "outputId": "5edb1897-73a6-4ea1-c5bf-4461c1e2315f"
      },
      "source": [
        "inputs_train = torch.tensor(inputs_train_res, dtype=torch.float32, requires_grad=True)\n",
        "inputs_val = torch.tensor(inputs_val, dtype=torch.float32, requires_grad=True)\n",
        "labels_train = torch.tensor(labels_train_res, dtype=torch.long)\n",
        "labels_val = torch.tensor(labels_val, dtype=torch.long)\n",
        "[inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, \n",
        " inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, \n",
        " labels_val.shape, labels_val.dtype]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([128, 10]),\n",
              " torch.float32,\n",
              " True,\n",
              " torch.Size([31, 10]),\n",
              " torch.float32,\n",
              " torch.Size([128]),\n",
              " torch.int64,\n",
              " torch.Size([31]),\n",
              " torch.int64]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZCQcwoJvHqT",
        "outputId": "06a65a5a-2f45-4047-efd2-79e6488ea082"
      },
      "source": [
        "inputs_train.shape,inputs_train.dtype,inputs_train.requires_grad, inputs_val.shape,inputs_val.dtype, labels_train.shape,labels_train.dtype, labels_val.shape, labels_val.dtype"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 10]),\n",
              " torch.float32,\n",
              " True,\n",
              " torch.Size([31, 10]),\n",
              " torch.float32,\n",
              " torch.Size([128]),\n",
              " torch.int64,\n",
              " torch.Size([31]),\n",
              " torch.int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjo5IbPmvHqT"
      },
      "source": [
        "## Init model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu3Q9u9fvHqU"
      },
      "source": [
        "chd_embed = CustomEmbedding(10, 100)\n",
        "chd_cla = SimpleNN(chd_embed, 100, 2)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(chd_cla.parameters())\n",
        "model = chd_cla\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFPmL_PsvHqU"
      },
      "source": [
        "## train for CHD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5pvvXlvHqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "8f3b1a73-81ba-4448-e14b-305e06557247"
      },
      "source": [
        "max_val_accs = []\n",
        "# for hidden_size in [30,60,100]:\n",
        "for i in range(10):\n",
        "    chd_embed = CustomEmbedding(10, 60)\n",
        "    chd_cla = SimpleNN(chd_embed, 60, 2)\n",
        "    w1 = 0.4\n",
        "    loss_func = nn.CrossEntropyLoss(torch.tensor([1-w1, w1]))\n",
        "    optimizer = torch.optim.Adam(chd_cla.parameters(), lr =0.00001)\n",
        "    model = chd_cla\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    num_epochs = 100\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    print_freq = 1000\n",
        "    patience = 5000\n",
        "    patience_2 = patience\n",
        "    max_val_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        if patience==0:\n",
        "            # print(\"Training complete\")\n",
        "            break\n",
        "        model.train()\n",
        "        train_loss, train_acc = train_one_epoch(model, inputs_train, labels_train, loss_func, optimizer, device, acc_func=calc_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # evaluate on the test dataset\n",
        "              model.eval()\n",
        "            val_loss, accuracy, precision, recall, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "            patience -= 1\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                patience =  patience_2\n",
        "                sch_embed_state_dict = sch_embed.state_dict().copy()\n",
        "                print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}, {accuracy}\")\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "        patience -= 1\n",
        "        precision, recall, f1 = val_acc\n",
        "        val_acc = f1\n",
        "\n",
        "        if precision > recall:\n",
        "          w1+=0.001\n",
        "          loss_func = nn.CrossEntropyLoss(torch.tensor([1-w1, w1 ]))\n",
        "        else:\n",
        "          w1 = min(1, w1-0.0001)\n",
        "          loss_func = nn.CrossEntropyLoss(torch.tensor([1-w1, w1]))\n",
        "\n",
        "\n",
        "        if val_acc > max_val_acc:\n",
        "            max_val_acc = val_acc\n",
        "            patience =  patience_2\n",
        "            print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}\")\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # if epoch%print_freq==0:    \n",
        "        # print(f\"Epoch =  {epoch} \\t Train loss = {train_loss:.5f} \\t  Train Acc = {train_acc:.5f} \\t\",\n",
        "        print(f\"Val loss = {val_loss:.5f} \\t Acc = {precision:.5f},{recall:.5f},{val_acc:.5f},{w1} \\t Patience = {patience}\")\n",
        "\n",
        "    max_val_accs.append(max_val_acc)\n",
        "print(\"Max val losses___________________________\")\n",
        "print(hidden_size, max_val_accs,\"\\n\", np.array(max_val_accs).mean())"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-589d21aba267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mpatience\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDiwys8ddEb9"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 30 - dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4hThUkuzgV9"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i = 0\n",
        "pre_df = pd.DataFrame(index = chddata.index)\n",
        "oddsratio_df = pd.DataFrame(index = np.arange(0.0,1.05,0.05))\n",
        "fi_df = pd.DataFrame()\n",
        "combine = chd_exp_n\n",
        "label = pd.DataFrame(chd_risk_n, columns=[\"labels\"])\n",
        "max_val_accs = []\n",
        "chd_embed_state_dicts = []\n",
        "for train, test in cv.split(chd_exp_n, label):\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "    # fit and apply the transform\n",
        "    inputs_train_res, labels_train_res = oversample.fit_resample(chd_exp_n[train], chd_risk_n[train])\n",
        "    inputs_test_res, labels_test_res = oversample.fit_resample(chd_exp_n[test], chd_risk_n[test])\n",
        "    \n",
        "    inputs_train = torch.tensor(inputs_train_res, dtype=torch.float32, requires_grad=True)\n",
        "    inputs_val = torch.tensor(inputs_test_res, dtype=torch.float32, requires_grad=True)\n",
        "    labels_train = torch.tensor(labels_train_res, dtype=torch.long)\n",
        "    labels_val = torch.tensor(labels_test_res, dtype=torch.long)\n",
        "\n",
        "    for i in range(5):\n",
        "        chd_embed = CustomEmbedding(10, 60)\n",
        "        chd_cla = SimpleNN(chd_embed, 60, 2)\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(chd_cla.parameters())\n",
        "        model = chd_cla\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "        num_epochs = 100000\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        print_freq = 1000\n",
        "        patience = 2000\n",
        "        patience_2 = patience\n",
        "        max_val_acc = 0\n",
        "        chd_embed_state_dict = None\n",
        "        for epoch in range(num_epochs):\n",
        "            # train for one epoch, printing every 10 iterations\n",
        "            if patience==0:\n",
        "                print(\"Training complete\")\n",
        "                break\n",
        "            model.train()\n",
        "            train_loss, train_acc = train_one_epoch(model, inputs_train, labels_train, loss_func, optimizer, device, acc_func=calc_accuracy)\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "\n",
        "            # evaluate on the test dataset\n",
        "            model.eval()\n",
        "            val_loss, accuracy, precision, recall, val_acc = evaluate(model, inputs_val, labels_val, loss_func, device, acc_func=calc_accuracy, pr=True)\n",
        "            patience -= 1\n",
        "            if val_acc > max_val_acc:\n",
        "                max_val_acc = val_acc\n",
        "                patience =  patience_2\n",
        "                chd_embed_state_dict = chd_embed.state_dict().copy()\n",
        "                print(f\"Epoch {epoch} \\t Max val Acc = {val_acc}, {accuracy}\")\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            if epoch%print_freq==0:    \n",
        "                print(f\"Epoch =  {epoch} \\t Train loss = {train_loss:.5f} \\t  Train Acc = {train_acc:.5f} \\t\",\n",
        "                f\"Val loss = {val_loss:.5f} \\t Acc = {precision:.5f}{recall:.5f}{val_acc:.5f} \\t Patience = {patience}\")\n",
        "        max_val_accs.append(max_val_acc)\n",
        "        chd_embed_state_dicts.append(chd_embed_state_dict)\n",
        "\n",
        "print(\"Max val losses________________\")\n",
        "print(max_val_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHcfR7nJb2fB"
      },
      "source": [
        "plt.plot(train_losses,label=\"Training loss\")\n",
        "plt.plot(val_losses,label=\"Validation loss\")\n",
        "plt.title('CHD Validation and Training Set Loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(val_accuracies, label=\"Validation accuracy\")\n",
        "plt.plot(train_accuracies, label=\"Training accuracy\") \n",
        "plt.title('CHD Validation and Training Set Accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-56xR1GvHqV"
      },
      "source": [
        "np.mean(max_val_accs) # 2 layer- (input hidden hidden) 30 - dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHycXd_PDsq9"
      },
      "source": [
        "chd_embed2 = CustomEmbedding(10, 60)\n",
        "chd_embed2.load_state_dict(chd_embed_state_dicts[np.argmax(max_val_accs)])\n",
        "chd_cla = SimpleNN(chd_embed2,  60,2)\n",
        "chd_cla.eval()\n",
        "evaluate(chd_cla, inputs_train, labels_train, loss_func, device, calc_accuracy, pr=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pl6W5L2dECJ"
      },
      "source": [
        "## Save embedding layer weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYR5_d6NdGPA"
      },
      "source": [
        "file_name = datadir+f\"chd_embedding.pt\"\n",
        "torch.save(chd_embed.state_dict(), file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peO5eoPwhL-g"
      },
      "source": [
        "aus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzcNLNEChYvn"
      },
      "source": [
        "class DeeperNN(nn.Module):\n",
        "    def __init__(self, hidden, outputs):\n",
        "        super(DeeperNN).__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(hidden)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden)\n",
        "        self.l1 = nn.Linear(hidden, hidden)\n",
        "        self.l2 = nn.Linear(hidden, outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout  = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        # x = torch.tanh(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.l2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-mmY7yiQrLC"
      },
      "source": [
        "# A-risk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97-s2SpCZt9R"
      },
      "source": [
        "aut_exp_n = np.array(aut_exp).T\n",
        "aut_risk_n = np.array(aut_risk)\n",
        "np.mean(aut_risk_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfFnCLRacaom"
      },
      "source": [
        "aut_embed2 = CustomEmbedding(95, 60)\n",
        "aut_embed2.load_state_dict(aut_embed_state_dicts[np.argmax(max_val_accs)])\n",
        "aut_cla = SimpleNN(aut_embed2,  60,2)\n",
        "aut_cla.eval()\n",
        "evaluate(aut_cla, inputs_train, labels_train, loss_func, device, calc_accuracy, pr=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBFLyr5atWO"
      },
      "source": [
        "sch_exp_n.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-LDm5iGe9NJ"
      },
      "source": [
        "data =  sch_exp.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isZX2daSQtgq"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i = 0\n",
        "pre_df = pd.DataFrame(index = data.index)\n",
        "oddsratio_df = pd.DataFrame(index = np.arange(0.0,1.05,0.05))\n",
        "fi_df = pd.DataFrame()\n",
        "combine = sch_exp_n\n",
        "label = pd.DataFrame(sch_risk_n, columns=[\"labels\"])\n",
        "for train, test in cv.split(sch_exp_n, label):\n",
        "    \n",
        "    inputs_train = torch.tensor(sch_exp_n[train], dtype=torch.float32, requires_grad=True)\n",
        "    inputs_val = torch.tensor(sch_exp_n[test], dtype=torch.float32, requires_grad=True)\n",
        "    labels_train = torch.tensor(sch_risk_n[train], dtype=torch.long)\n",
        "    labels_val = torch.tensor(sch_risk_n[test], dtype=torch.long)\n",
        "\n",
        "    sch_embed2.eval()\n",
        "    with torch.no_grad():\n",
        "        train_embeddings = sch_embed2(inputs_train).data.numpy()\n",
        "        test_embeddings = sch_embed2(inputs_val).data.numpy()\n",
        "    \n",
        "    params = {'n_estimators': 300,'learning_rate': 0.05, 'max_depth':1, 'random_state':2019}\n",
        "    clf = GradientBoostingClassifier(**params).fit(train_embeddings, label.iloc[train])\n",
        "    #print(combine.ix[test])\n",
        "    \n",
        "    probas_ = clf.predict_proba(test_embeddings)\n",
        "    #print probas_\n",
        "    \n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = roc_curve(label.iloc[test], probas_[:,[1]])\n",
        "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "    \n",
        "    #calculate odds ratio\n",
        "    ortable = label.iloc[test]\n",
        "    ortable['probas_'] = probas_[:,[1]]\n",
        "    oddsratios = []\n",
        "    for j in np.arange(0.0,1.05,0.05):\n",
        "        upthreshold = ortable[ortable['probas_'] >= j]\n",
        "        x1 = float(sum(upthreshold['labels'] == 1)+1)\n",
        "        x0 = float(sum(upthreshold['labels'] == 0)+1)\n",
        "        \n",
        "        downthreshold = ortable[ortable['probas_'] < j]\n",
        "        y1 = float(sum(downthreshold['labels'] == 1)+1)\n",
        "        y0 = float(sum(downthreshold['labels'] == 0)+1)\n",
        "        \n",
        "        oddsratio = (x1/y1)/(x0/y0)\n",
        "        oddsratios.append(oddsratio)\n",
        "    oddsratio_df.insert(value = oddsratios, column = i, loc = 0)\n",
        "    \n",
        "    data_embeds = sch_embed2(torch.tensor(sch_exp_n, dtype=torch.float32, requires_grad=True))\n",
        "    prediction = clf.predict_proba(data_embeds.data.numpy())\n",
        "    pre_df.insert(value= prediction[:,[1]], column= i, loc=0)\n",
        "    \n",
        "    fi = clf.feature_importances_\n",
        "    fi_df.insert(value= fi, column = i, loc = 0)\n",
        "\n",
        "    i += 1\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve of 5-fold cross validation')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "#plt.savefig('../plot/ROC_filteredCellNumber_LMZhong_fracE_classifier_average_08292019sfari.pdf', bbox_inches = 'tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saMR6sWUaV3R"
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pwnjHTKh1OU"
      },
      "source": [
        "## Header ffs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SIjLOrCY-3u"
      },
      "source": [
        "#plot feature importance that are not zero\n",
        "feature_importance = fi_df.mean(axis=1)\n",
        "# make importances relative to max importance\n",
        "#feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "\n",
        "sorted_idx = np.argsort(feature_importance) #Returns the indices that would sort an array\n",
        "sorted_idx = sorted_idx[np.array(feature_importance[sorted_idx] != 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_4xNu50tp6S"
      },
      "source": [
        "pos = np.arange(0,sorted_idx.shape[0]*3, 3) + .5\n",
        "pos.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oE0grfXY-3v"
      },
      "source": [
        "plt.figure(figsize=(8,20))\n",
        "plt.tick_params(axis = \"y\", labelsize = 3.5)\n",
        "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "plt.yticks(pos, combine.columns[sorted_idx])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.title('Variable Importance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQGNAwGvY-3w"
      },
      "source": [
        "feature_importance_sele = pd.DataFrame({'celltypes': data.columns[sorted_idx], 'importance': feature_importance[sorted_idx]})\n",
        "feature_importance_sele"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38xicXeEY-3x"
      },
      "source": [
        "feature_importance_sele.to_csv('../data/feature_importance_filteredCellNumber_LMZhong_fracE_filtergene_zhong_classifier_average_noUnk_08292019sfari.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSqrcH9YuFzL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lASdTOwquGKW"
      },
      "source": [
        "feature_importance_sele = pd.DataFrame({'celltypes': combine.columns[sorted_idx], 'importance': feature_importance[sorted_idx]})\n",
        "feature_importance_sele"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPXPLAuquGKb"
      },
      "source": [
        "feature_importance_sele.to_csv('noUnk_08292019sfari.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FdqJXcFY-3x"
      },
      "source": [
        "pre_prob = pd.DataFrame(pre_df.mean(axis=1))\n",
        "\n",
        "pre_prob['Batch'] = 'testing'\n",
        "pre_prob.loc[aut_exp.T.index,'Batch'] = 'autism'\n",
        "# pre_prob.loc[control.T.index,'Batch'] = 'control'\n",
        "\n",
        "pre_prob.rename(columns={0 : 'Frisk'}, inplace=True)\n",
        "pre_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4E7-RqiY-3y"
      },
      "source": [
        "pre_prob.to_csv(datadir+'autism_gene_preds.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XErmpX9VeNH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}